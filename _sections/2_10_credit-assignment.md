> The reason we are (once again) having a fight about whether the producers of publicly available/published data should be authors on any work using said data is that we have a completely dysfunctional system for crediting the generation of useful data. {% cite eisenReasonWeAre2021 %}
>
> The same is true for people who generate useful reagents, resources and software. {% cite eisenSameTruePeople2021 %}
>
> And like everything, the real answer lies on how we assess candidates for jobs, grants, etc… So long as people treat authorship as the most/only valuable currency, this debate will fester. But it’s in our power to change it. - Michael Eisen, EIC eLife {% cite eisenEverythingRealAnswer2021 %}

The critical anchor for changes to the scientific infrastructural systems is the system of professional incentives that structure it. As long as the only thing that has professional value is authorship in journal papers, the system stays: Blog posts, analysis pathways, wikis, and forums are nice and all, but they don't count as *science.* !! All the acrimony around the use of the h-index is rooted in authorship and citation count as the sole means of credit assignment.

Imagining different systems of credit assignment is easy: just make a new DOI-like identifier for my datasets that I can put on my CV. Integrating systems of credit assignment into commonly-held beliefs about what is valuable is harder. One way to frame solutions to the credit assignment problem is as a collective action problem: everyone/funding agencies/hiring committees just need to *decide* that publishing data, reviewing, criticism et al. is valuable without any serious changes to broader scientific infrastructure. Another is to *displace* the system of credit assignment by aligning the interests of the broad array of researchers, technicians, and students that it directly impacts to build an alternative.

The sheer quantity of work that is currently uncredited in science is a structural advantage to any more expansive system of credit assignment. The strategic question is how to design a system that aligns the self-interest of everyone with uncredited work to build it. 

That's what I've tried to do here. Everything that exists in this system is attributable to one or many equal peers. Rather than attempting to be an abstract body of knowledge, clean and tidy, that conceals its social underpinnings, we embrace its messy and pluralistic personality. We have *not* been focused on some techno-utopian dream of automatically computing over a system of universally linked data, but on representing and negotiating over a globally discontinuous body of work and ideas linked to people and groups. We have *not* been imagining new platforms and services to suit a limited set of needs, but on a set of tools and frameworks to let people work together to cumulatively build what they need. 

Credit is woven throughout this system: the means of using someone else's work are tied to crediting it. While credit is currently meted out by proprietary journal aggregators like google scholar, citeseer, or web of science; downloading a dataset, using an analysis tool, and so on should be directly attributed to a digital identity that you control. 

The first-order effects for the usual suspects in need of credit are straightforward: counting the number of analyses and papers our datasets are cited in, seeing the type of experiments our software was used to perform. Control over the means of credit assignment also opens the possibility of surfacing the work that happens invisibly but is nonetheless essential for the normal operation of research. Why shouldn't the animal care technician receive credit for caring for the animals that were involved with a study, its results, and its impact on science more broadly?

Contextual technical knowledge is an example that warrants special consideration. Why would anyone spend the time to describe the fine technical details of how to use a type of motor, or which solenoids last the longest, or how to solder this particular type of circuit board? 

---
<div id="draftmarker"><h1># draftmarker</h1><br>~ everything past here is purely draft placeholder text ~  </div>
---

Let's say we're using a semantic wiki for our lab or institute, and use it to coordinate work locally and document lab practices. Let's also say that we 

- integration with experimental tools makes it easier to do experiment (eg. solenoid polarity, autoconfiguring hardware)
	- say we were using a lab wiki, that could then be transcluded into a broader set of wikis. So just by doing your normal work and documenting basic practices for your lab, you can choose to federate them with maybe a methods wiki.
- then network effects of contribution
	- someone references our documentation in their plugin which is linked to their paper. Your work helped them do their experiment! you should get credit for that. 
	- derivative works are then that much easier, so someone remixes your part and then builds a plugin that references that. You had some part in the lineage.


- first-order
	- beyond shitty authorship metrics.
	- crediting new kinds of work
	- also under your control, how you want to represent yourself across multiple domains
- network-effects
	- who uses your work? who uses your work who uses your work?
	- what impact does your work have on a broad range of scientific questions? eg. how did your forum discussion of how to use a solenoid contribute to someone's paper that then changed the state of the field?
- critical for getting the system on its feet and maintaining it
	- the labor for maintaining the system can't be invisible!
	- positive feedback loop hopefully!


!! make it easy for someone else to use your work and then by using it you have some verifiable record that other people like and use your stuff! !! this appeals to a much broader base of people not traditionally in the scientific value system, so they might be interested. !! also lets us value different kinds of scientific labor, like mentorship, advice, debugging, etc. without necessarily needing to gamify it. !! Deep linking and long provenance lets us see our impact on the broader scientific world, which is a much more valuable and informative than shitty journal rankings. !! people always talk about how shitty journal metrics are, and so that's an opening! !!






the work of maintaining the system can't be invisible, read & cite {% cite classeDistributedInfrastructureSupport2017 bowkerInformationInfrastructureStudies2010 %}
