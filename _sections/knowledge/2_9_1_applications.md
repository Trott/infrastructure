
Continuing the example of the Autopilot wiki, we could make an array of **technical knowledge wikis.** Wikis organized around individual projects could federate together to share information, and broader wikis could organize the state of our art which currently exists hollowed out in supplemental methods sections. The endless stream of posts asking around for whoever knows how to do some technique that should be basic knowledge for a given discipline illustrate the need. Across disciplines, we are drenched in widely-used instrumentation and techniques without coherent means of discussing how we use them. Organizing the technical knowledge that is mostly hard-won by early career researchers without robust training mechanisms would dramatically change their experience in science, whittling away at inequities in access to expertise. Their use only multiplies with tools that are capable of using the semantically organized information to design interface or simplify their operation as described in [experimental frameworks](#experimental-frameworks).

Technical wikis could change the character of technical work. By giving a venue for technical workers to describe their work, they would be welcomed into and broaden the base of credit currently reserved only for paper authors. Even without active contribution, they would be a way of describing the unseen iceberg of labor that science rests on. Institutional affiliations are currently just badges of prestige, but they could also represent the dependence of scientific output on the workers of that institution. If I do animal research at a university, and someone has linked to the people responsible for maintaining the animal facility, then they should be linked to all of my work. Making technical knowledge broadly available might also be a means of inverting the patronizing approach to "crowdsourcing" "citizen science" by putting it directly in the hands of nonscientists, rather than at the whim of some gamified platform (see {% cite delangeShortTimeBig2022 %}).

Technical wikis blend smoothly into **methods wikis** for cataloguing best practices in experimental design and analysis. It is a damning indictment of our systems of training or review (or, more likely, both) that it is possible to publish a paper with a misused t-test, yet the scientific literature is flooded with errors {% cite strasakStatisticalErrorsMedical2007 brownIssuesDataAnalyses2018 leekStatisticsValuesAre2015 %}. The cause of analytical errors is not just a matter of lack of education, with a complex network of incentives and disciplinary subcultures. Combined with the ability to survey and contextualize the array of analytical techniques for a given body of data as described in [analytical frameworks](#analytical-frameworks), methods wikis could at least identify and bring to light the state of a discipline's art. 

**Analysis wikis** would also be a natural means of organizing the previously mentioned Folding@Home-style distributed computing grids. Groups of researchers could organize computational resources and govern and document their use. Since the kind of wikis we are describing combine free text with computer-readable data structures, policies for use could be directly implemented in the wiki in the same place they were discussed. This too is a means of collectivizing support for open-source initiatives that support basic infrastructure by donation and the mercy of cloud providers by integrating them in the basic social practices of science {% cite dupreAdvertisingNewInfrastructures2022 %}.

**Review wikis** could replace journals almost as an afterthought. Though an adequate infrastructure of scientific communication immediately antiquates traditional peer review, review wikis could facilitate it without recourse to an extractive information industry. In response to the almost unique profitability of publishing, some researchers have reacted, perhaps justifiably, by demanding payment for their reviews (eg {% cite heathers450Movement2020 %}). An alternative might be to organize review *ourselves.* Like the ratio requirements of private bittorrent trackers, we might establish a review ratio system, where for every review your work receives you need to review n other works. This would effectively function as a **reviewer co-op** that can make the implicit labor of reviewing explicit, and tie the reviews required for frequent publication with explicit norms around reciprocal reviewing. 

**Library wikis** focused on curation, contextualization, and organization of information could be one modality of resisting the neoliberal drive to reduce librarians to stewards of subscriptions and surveillance data {% cite lamdanLibrarianshipCrossroadsICE2019 quinnResistingNeoliberalismChallenge2017 %}. Knowledge organization is hard practical and theoretical work, and reimagining the space of scientific communication as one that we actively *create* instead of one that we merely *suffer through* is a wide-open invitation for the comradeship and leadership of librarians. Linked data has been a mixed blessing for librarians, its promise obscured by intellectual property oligopolies and the complexity of linked data standards (see {% cite librariaStoningGoliath2022 %}). Given fresh tooling and a path away from structuring influence of for-profit publishers, the rest of us should be prepared to learn from those that have already been doing the work of curating our archives: 

> [M]ake it easy to rely on linked data, easier than it is to rely on MARC, and the library world will shift, from the smallest and poorest libraries upwardâ€¦ and David will at last stone Goliath to death with his linked-data slingshot. 
>
> [*Stoning Goliath*](https://gavialib.com/2022/06/stoning-goliath/) (2022) The Library Loon {% cite librariaStoningGoliath2022 %}










  - Theory wiki
    - mix of multivalent documents, wiki pages, discussions, public wiki pages from eg. "publisher-like" things that are more communally edited and be linked to the same underlying body of information and argument without making assumptions about the form of the discussion.




---



For example, we briefly mentioned a Folding@Home-like system of donated computing resources, and separately described embedding analyses in a forum by calling our own compute resources. Together, a tracker could implement a compute ratio where to use shared computing resources you need to contribute a certain amount of your own. The bounty system where peers would donate their excess upload in exchange for uploading a rare album on what.cd could translate to one where someone who has donated a lot of excess compute time could donate it for someone uploading or collecting a particular dataset. 



Forum and feedlike media are good for organizing continuous conversation, but wikis serve as a more durable knowledge store for cumulative reference information. We don't need to imagine wikis as being text-only, with wiki formatting used just to change the appearance of text, but as a means of declaring and manipulating semantic links. For example, [Semantic MediaWiki](https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki) is an extension to Wikipedia's wiki system that extends `[[Wikilinks]]` to be able to declare semantic links like `[[linkType::Target]]`. For example, if our project had a wiki page like `[[My Project]]` we could say it `[[hasType::@analysis:project]]` and `[[usesDataset::@jonny:mydata1]]` etc. These wikis have the capability to not only organize knowledge, but also serve as a flexible means of declaring new programming interfaces and assigning credit. 


The same combination of trackers, forums, and wikis has a natural application to analysis pipelines. Ideally, to move beyond fragile code reduplicated in every lab, we need some means of reaching consensus on a few canonical implementations of fundamental analysis operations. Given a system where analysis chains are linked to the formats and subdisciplines they are used with, we can map a semantically dense map of the analysis paths used in a research domain. In neurophysiology: "What are the different ways spikes are extracted and analyzed from extracellular electrophysiology recordings?" Having the ability to discuss and contextualize different analytical methods elevates all the exasperated methods critiques and exhortations to "not use this statistically unsound technique" into something *structurally expressed in the practice of science.* See all the `@neurotheory` threads about this specific analysis chain, or the `@methodswiki` page that summarizes this general category of techniques.

We're now in a place where we can address the problem of a cumulative knowledge system for science directly. In many (most?) scientific epistemologies, scientific results do not directly reflect some truth about reality, but instead instead are embedded in a system of meaning through a process of active interpretation (eg. {% cite meehlTheoreticalRisksTabular1978 %}). The interpretation of every scientific result is left as the responsibility of the authors to recreate and a few reviewers to evaluate, which would be a monumental amount of labor given the velocity of papers, so researchers do the best they can engaging with a small amount of research. Since the space of argumentation is built from scratch each time from incomplete information, there's no guarantee of making cumulative progress on a shared set of theories, and most fall far from the supposed ideal of hard refutation and can have long lives as "zombie theories." van Rooij and Baggio describe the "collecting seashells" approach of gathering many results and leaving the theory for later with an analogy:

> "In a sense, trying to build theories on collections of effects is much like trying to write novels by collecting sentences from randomly generated letter strings. Indeed, each novel ultimately consists of strings of letters, and theories should ultimately be compatible with effects. Still, the majority of the (infinitely possible) effects are irrelevant for the aims of theory building, just as the majority of (infinitely possible) sentences are irrelevant for writing a novel." {% cite vanrooijTheoryTestHow2021 %}

They and others (eg. {% cite guestHowComputationalModeling2021 %}) have argued for an iterative process of experiments informed by theory and modeling that confirm or constrain future models. Their articulation of the need for multiple registers of formality and rigidity is particularly resonant here. van Rooij and Baggio again:

> "The first sketch of an f need not be the final one; what matters is how the initial f is constrained and refined and how the rectification process can actually drive the theory forward. Theory building is a creative process involving a dialectic of divergent and convergent thinking, informal and formal thinking." {% cite vanrooijTheoryTestHow2021 %}

Let's turn our provenance chain into a circle: a means of linking theories to analytical results and interpretation as well as experimental design and tooling. Say the theorists have a wiki. They start making some loose schematic descriptions of their theories and linking them to different experimental results that constrain, affirm, refute, or otherwise interact with them. These could be forward or backlinks: declared by the original author or by someone else describing their results. 

In the most optimistic case, where we have a full provenance chain from analytical results back through experimental practice, we have a means of formally evaluating the empirical contingencies that serve as the evidence for scientific theories. For a given body of experimental data bearing on a theoretical question, what kinds of evidence exist? As the state of the art in analytical tooling changes, how are the interpretations of prior results changed by different analyses? How do different experimental methodologies influence the form of our theories? The points of conflicting evidence and unevaluated predictions of theory are then a means of distributed coordination of future experiments: guided by a distributed body of evidence and interpretation, rather than the amount of the literature base individual researchers are able to hold in mind, what are the most informative experiments to do?

The pessimistic case where we only have scientific papers in their current form to evaluate is not that much worse --- it requires the normal reading and evaluation of experimental results of a review paper, but the process of annotating the paper to describe its experimental and analytical methods as a shared body of links makes that work cumulative. Even more pessimistic, where for some reason we aren't able to formulate theories even as rough schematics but just link experimental results to rough topic domains is still vastly better than the current state of disorganization and proprietary indices. 

For both researchers and the public at large a meta-organization of experimental results changes the way we interact with scientific literature. It currently takes many years of implicit knowledge to understand any scientific subfield: finding canonical papers, knowing which researchers to follow, which keywords to search in table of contents alerts. Being able to find a collection of papers about an object of research, as well as the conversations at all levels of formality that contextualize them --- to say nothing of building a world without paywalls --- would profoundly lower barriers to access to primary scientific knowledge for *everyone.* 

It is worth pausing to compare a world where we boisterously and fluidly organize knowledge explicitly as a collective project of understanding with one where knowledge organization is weaponized into a product that lets us get ahead of our competitors without necessarily improving our understanding of the body of scientific literature. One sounds like science, the other sounds like industry capture.

All the technological-social tools described here are not a definitive set of tools needed for scientific communications infrastructure, but *examples of interfaces to a linked data system.* Using JSON-LD notebooks to enable us to embed links in our writing to be mentioned or transcluded elsewhere. Using a forum as a means of creating linked discussions about experimental results and analyses. Using linked microblogging tools for a rapid, informal means of organizing and discussing knowledge. Using all of the above to represent the many expressions of a work across multiple linked namespaces. Using annotation tools to create anchors and links for referencing links in other communication media. Using tracker-like and wiki-like systems to interact with, negotiate about, and govern a wily body of autonomously declared links.

Each is intended to be mutable, easy to iterate on, uncontrolling, mutually coordinated. Each interacts with and augments the previously described systems for shared data, analytical, and experimental tools. The purpose of this section is not to advocate a specific set of technologies, but to describe a base layer of familiar technologies for an indefinite future of possible interfaces for representing and interacting with a body of shared knowledge. 

What we've described is a nonutopian, fully realizable path to making a scientific system that is fully negotiable through the entire theoretical-empirical loop with minor development of existing tools and minimal adjustment of scientific practices. No clouds, no journals, a little rough around the edges but collectively owned by all scientists.

<div class="draft-text">Final system summary</div>