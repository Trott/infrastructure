> I also think one of the big obstacles to freeing up scientific information remains the way in which we continue to pay allegiance to the idea that the most important work is published in so-called ‘high-impact’ journals [...]. These journals continue to thrive, despite a kind of anti-social policy, because **so many academic scientists evaluate each other’s work and measure abilities and accomplishments based on where people have published.**
> 
> **The only way by which we’ll eventually get out of the current situation is by changing the formula dramatically.** That means that we’ll probably have to move to a world where the authors have full control – their work will be presented online together with expert reviews and perhaps accompanied by a new evaluation system in which members of the scientific community will provide qualitative and perhaps quantitative measures of the value of the paper. The current world of high- and low-impact journals will eventually dissolve, it's just taking a lot longer than I thought.
>
> Harold Varmus, former director of the NIH (2019) *Of Oncogenes and Open Science* {% cite varmusOncogenesOpenScience2019 %}

> The reason we are (once again) having a fight about whether the producers of publicly available/published data should be authors on any work using said data is that we have a completely dysfunctional system for crediting the generation of useful data. {% cite eisenReasonWeAre2021 %} The same is true for people who generate useful reagents, resources and software. {% cite eisenSameTruePeople2021 %} And like everything, the real answer lies on how we assess candidates for jobs, grants, etc… **So long as people treat authorship as the most/only valuable currency, this debate will fester. But it’s in our power to change it.** {% cite eisenEverythingRealAnswer2021 %}
>
> Michael Eisen, EIC eLife (2021) 


The critical anchor for changes to the scientific infrastructure is the system of professional incentives that structure it. As long as the only way we operationalize scientific value is paper authorship and the prestige of the journals they are placed in, the system stays: Blog posts, software, analysis pipelines, wikis, forums, reviews, are nice, but they don't count as *science.*

Imagining different systems of credit assignment is easy: just make a new DOI-like identifier for my datasets that I can put on my CV. Integrating systems of credit assignment into commonly-held beliefs about what is valuable is harder. One way to frame solutions to the credit assignment problem is as a collective action problem: everyone/funding agencies/hiring committees just need to *decide* that publishing data, reviewing, criticism et al. is valuable without any serious changes to broader scientific infrastructure. As is hopefully obvious, the approach favored here is to *displace* the system of credit assignment by aligning the interests of the broad array of researchers, technicians, and students that it directly impacts to build an alternative that makes it *irrelevant.*

The sheer quantity of work that is currently uncredited in science is a structural advantage to any more expansive system of credit assignment. The strategic question is how to design a system that aligns the interests of enough people excluded by the current system. Belief, as always, is a tricky circular process: how would the people being evaluated come to believe in its value enough to contribute to it, and how would the people doing the evaluation believe in its value enough to ignore the analytics products by deeply embedded industries?

Everything that exists in this system is attributable to one or many equal peers. Rather than attempting to be an abstract body of knowledge, clean and tidy, that conceals its social underpinnings, we embrace its messy and pluralistic personality. We have *not* been focused on some techno-utopian dream of automatically computing over a system of universally linked data, but on representing and negotiating over a globally discontinuous body of work and ideas linked to people and groups. We have *not* been imagining new platforms and services to suit a limited set of needs, but on a set of tools and frameworks to let people work together to cumulatively build what they need. What is different about this set of ideas is that it is not a new metric, journal, or platform intended to be the [new standard](https://xkcd.com/927/) that replaces some small element of the system, leaving the rest unchanged. We are taking a broad view on the infrastructural deficits that define scientific work, learning from the broad histories of attempts to remedy them, and trying to chart a course to building systems that fill basic needs. The hope is to seed a critical mass of solidarity by organizing the work to fill the unmet needs that structure the current system of evaluation, in the process building a real alternative that makes the existing system look as ridiculous as it is.

Credit is woven through the heart of this system: the basic operations of interacting with someone else's work are tied to crediting it. While credit is currently meted out by proprietary scientometric tools like altmetric or Plum; downloading a dataset, using an analysis tool, and so on should be directly attributable to one or several digital identities that you control in the manner that you want. 

The first-order effects for the usual suspects in need of credit are straightforward: counting the number of analyses and papers our datasets are cited in, seeing the type of experiments our software was used to perform. Control over the means of credit assignment also opens the possibility of surfacing the work that happens invisibly but is nonetheless essential for the normal operation of research. Why shouldn't the animal care technician receive credit for caring for the animals that were involved with a study, its results, and its impact on science more broadly?

A name prominently displayed on a wiki page and a permalink for a CV is ok, but clearly not enough. Foundational work like technical, communicative, and organizational work is useful in itself, but its impact is mostly felt *downstream* in the work it enables. Beyond first-order credit, a linked credit assignment system lets us evaluate *higher-order* effects of work that *more closely resemble* its impact. Say we find someone else's [3D Model](https://wiki.auto-pi-lot.com/index.php/3D_CAD), modify it for our use, and then use it to collect a dataset and publish a paper. Someone else sees it and links a colleague to it, and they too use it in their work. Over time someone else updates the design and puts it in some derivative component. Most of the linking is automatic, built into the interfaces of the relevant tools, and soon the network of links is dense and deep.

The incentive to "freeload" by making the use of the system without credit is changed by breaking apart the notion of unitary credit where one or a few people are responsible for "all" of a work. Our current obsession with utter novelty and closed credit removes incentives to extend someone else's work: why would I help patch their code? I won't be added as an author on their paper. For us, instead of just getting professional credit for our paper, we also get credit for extending someone else's work, for documenting it, and for the potentially large number of nth-order derivative uses. Our credit extends multimodally, including papers that cite papers that use our tool, and the "amount" of credit can be contextualized because the type of link between them is explicit -- as opposed to the non-semantic links of citation. Our colleague that recommended our part gets credit as well, as they should since helpful communication is presumably something we want to reward. Rather than the scarcity mindset of authorship, a link-based system can push us towards abundance: "good" work is work that engages with and extends a broad array of techniques, technologies, and expertise.

From the perspective of the worker, their extended contribution graph will always be a superset of the things they would otherwise be credited for. The goal should make it be something we *prefer* to share because it's more reflective of our work. Unlike proprietary metrics that will be increasingly based on surveillance data, our system gives us control over which information we want to be part of our evaluative profile, and it's something that we own to do what we will with rather than the product of some platform.

It's easy to imagine extended credit scenarios for a broad array of workers: A grad student rotating in a lab might not get enough data to make a paper, but they might make some tangible improvement to lab infrastructure, which they can document and receive credit for. Open source software developers might get some credit from a code paper, but will be systematically undervalued from failure to cite it and undercounted in derivative packages. The many groups of workers whose work is formally excluded from scientific valuation are those with the most to gain by reimagining credit systems, and an infrastructural plan that actively involves them and elevates their work has a much broader base of labor, expertise, and potential for buy-in.

From the perspective of the evaluator, our contribution graph provides a much richer space of evaluation while also eroding the notion of a scalar-valued ranking. Some of my more communitarian colleagues might share my distaste for metricizing knowledge work --- but hiring committees and granting agencies are going to use *some* metric, the question is whether it's a good reflection of our work and who controls it. Our problems with the h-index (eg. {% cite teixeiradasilvaMultipleVersionsHindex2018 costasReflectionsCautionaryUse2018 %}) are problems with paper citations being a bad basis for evaluating scientific "value", and their primacy is in turn a consequence of the monopoly over scientific communication and organization by publishers and aggregators. Their successors, black box algorithmic tools like SciVal with valuation criteria that are bad for science (but good for administrators) like 'trendiness' are here whether we like it or not. A transparent graph of scientific credit at least gives the *possibility* for reimagining the more fundamental questions of scientific valuation: assigning credit for communication, maintenance, mentorship, and so on. So some misguided reductions of the complexity of scientific labor to a single number are inevitable, but at least we'll be able to *see what they're based on* and *propose alternatives.* The presence of many simultaneous metrics on the same underlying graph would be itself a demonstration of the inability of any single metric to capture the value of our work. Conversely, spamming the graph to increase your "high score" with a large number of trivial contributions would be straightforward to detect because of the likely shallowness of the graph, so microcommodification of labor is less likely. The incentives are aligned to do work that is useful to others and positively affect the state of our understanding.

It's true that some of these extended metrics are already possible to compute. One could crawl package dependencies for code, or download the [100GB Crossref database](https://academictorrents.com/details/e4287cb7619999709f6e9db5c359dda17e93d515) {% cite crossrefJanuary2021Public2021 %} and manually crunch our statistics, but being *able* to compute some means of credit is very different than making it a *normal part* of doing and evaluating research. The multimodality of credit assignment that's possible with a linked data system is part of its power: our work *actually does* have impacts across modalities, and we should be able to represent that as part of our contribution to science. 

Reaching a critical mass of linked tools and peers is not altogether necessary for them to be useful, but critical mass may trigger a positive feedback loop for the development of the system itself. Even in isolation, a semantic wiki is a better means of assigning credit than a handful of google docs, experimental tools that automatically annotate data are better than a pile of `.csv` files, etc. Bridging two tools to share credit is better than one tool in isolation, and more people using them are better than fewer for any given user of the system. Lessons learned from STS, Computer-Supported Cooperative Work (CSCW), pirates, wikis, forums, et al. make it clear that *the labor of maintaining and building the system can't be invisible.* 
