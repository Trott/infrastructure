The remaining set of problems implied by the infrastructural system sketched in the preceding sections is the *communication* and *organization* systems that make up the interfaces to maintain and use it. We can finally return to some of the breadcrumbs laid before: the need for negotiating over distributed and conflicting data schema, for incentivizing and organizing collective labor, and ultimately for communicating scientific results. 

The communication systems that are needed double as *knowledge organization* systems. Knowledge organization has the rosy hue of something that might be uncontroversial and apolitical --- surely everyone involved in scientific communication wants knowledge to be organized, right? The reality of scientific practice might give a hint at our naivete. Despite being, in some sense, itself an effort to organize knowledge, *scientific results effectively have no system of explicit organization.* There is no means of, say, "finding all the papers about a research question." The problem is so fundamental it seems natural: the usual methods of using search engines, asking around on Twitter, and chasing citation trees are flex tape slapped over the central absence of a system for formally relating our work as a shared body of knowledge. 

Information capitalism, in its terrifying splendor, here too pits private profit against public good. Analogously to the necessary functional limitations of SaaS platforms, artificially limiting knowledge organization opens space for new products and profit opportunities. In their 2020 shareholder report, RELX, the parent of Elsevier, lists increasing the number of journals and papers as a primary means of increasing revenue {% cite RELXAnnualReport2020 %}. This represents a shift in their business model from subscriptions to deals like open access, which according to RELX CEO Erik Nils EngstrÃ¶m "is where revenue is priced per article on a more explicit basis" {% cite relx2020ResultsPresentation2021 %}.

In the next breath, they describe how "in databases & tools and electronic reference, representing over a third of divisional[^whatdivision] revenue, we continued to drive good growth through content development and enhanced machine learning [ML] and natural language processing [NLP] based functionality." 

What ML and NLP systems are they referring to? The 2019 report is a bit more revealing (emphases mine): 

> Elsevier looks to enhance quality by building on its premium brands and **grow article volume** through **new journal launches,** the expansion of open access journals and growth from emerging markets; and add value to core platforms by implementing capabilities such as **advanced recommendations on ScienceDirect and social collaboration through reference manager and collaboration tool Mendeley.**
>
> **In every market, Elsevier is applying advanced ML and NLP techniques** to help researchers, engineers and clinicians perform their work better. For example, in research, ScienceDirect Topics, a free layer of content that enhances the user experience, uses **ML and NLP techniques to classify scientific content and organise it thematically,** enabling users to get faster access to relevant results and related scientific topics. The feature, launched in 2017, is proving popular, generating 15% of monthly unique visitors to ScienceDirect via a topic page. **Elsevier also applies advanced ML techniques that detect trending topics per domain,** helping researchers make more informed decisions about their research. **Coupled with the automated profiling and extraction of funding body information from scientific articles,** this process supports the whole researcher journey; from planning, to execution and funding. {% cite RELXAnnualReport2019 %}

[^whatdivision]: RELX is a huge information conglomerate, and scientific publication is just one division.

Reading between the lines, it's clear that the difficulty of finding research is a feature, not a bug of their system. Their explicit business model is to increase the number of publications and sell organization back to us with recommendation services. The recommendation system might be free[^notfree], but the business is to develop dependence to sell ad placement --- which they proudly describe as looking very similar to their research content {% cite springernatureBrandedContent elsevier360AdvertisingSolutions %}. 

[^notfree]: "free"

It gets more sinister: Elsevier sells multiple products to recommend 'trending' research areas likely to win grants, rank scientists, etc., algorithmically filling a need created by knowledge disorganization. The branding varies by audience, but the products are the same. For pharmaceutical companies ["scientific opportunity analysis"](https://www.elsevier.com/solutions/professional-services/drug-design-optimization#opportunity) promises custom reports that answer questions like "Which targets are currently being studied?" "Which experts are not collaborating with a competitor?" and "How much funding is dedicated to a particular area of research, and how much progress has been made?" {% cite elsevierDrugDesignOptimization %}. For academics, ["Topic Prominence in Science"](https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science#how) offers university administrators tools to "enrich strategic research planning with portfolio overviews of their own and peer institutions." Researchers get tools to "identify experts and potential cross-sector collaborators in specific Topics to strengthen their project teams and funding bids and identify Topics which are likely to be well funded." {% cite elsevierTopicProminenceScienceb %} 

These tools are, of course, designed for a race to the bottom --- if my colleague is getting an algorithmic leg up, how can I afford not to? Naturally only those labs that *can* afford them and the costs of rapidly pivoting research topics will benefit from them, making yet another mechanism that reentrenches scientific inequity for profit. Knowledge disorganization, coupled with a little surveillance capitalism that monitors the activity of colleagues and rivals {% cite brembsReplacingAcademicJournals2021 %}, has given publishers powerful control over the course of science, and they are more than happy to ride algorithmically amplified scientific hype cycles in fragmented research bubbles all the way to the bank.

The consequences are hard to overstate. In addition to literature search being an unnecessarily huge sink of time and labor,  science operates as a wash of tail-chasing results that only rarely seem to cumulatively build on one another. The need to constantly reinforce the norm that purposeful failure to cite prior work is research misconduct is itself a symptom of how engaging with a larger body of work is both extremely labor intensive and *strictly optional* in the communication regime of journal publication. Despite the profusion of papers, by some measures progress in science has slowed to a crawl as the long tail of papers with very few citations grows ever longer {% cite chuSlowedCanonicalProgress2021 %}. 

While Chu and Evans correctly diagnose *symptoms* of knowledge disorganization like the need to "resort to heuristics to make continued sense of the field" and reliance on canonical papers, by treating the journal model as a natural phenomenon and citation as the only means of ordering research, they misattribute root *causes.* The problem is not people publishing *too many papers,* or a *breakdown of traditional publication hierarchies,* but the *profitability of knowledge disorganization.* Their prescription for "a clearer hierarchy of journals" misses the role of organizing scientific work in journals ranked by prestige, rather than by the content of the work, as a potentially major driver of extremely skewed citation distributions. It also misses the publisher's stated goals of, well *publishing more papers,* and pushing algorithmic paper recommendations, as there is nothing recommendation algorithms love recommending more than things that are alreaady popular. Without diagnosing knowledge disorganization as a core part of the business model of scientific publishers, we can be led to prescriptions that would make the problem worse.

!! Another impact of the arcanity of scientific knowledge organization is that it is effectively impenetrable to people that aren't domain experts. Why is trust in science so low right now? one contributor is that they have no idea what the hell we do or how different domains of knowledge have evolved. (cite cold war peer review and journals paper)

!! Practically, this makes the quality of scientific literature constantly in question. Each paper effectively exists as an island, and engagement with prior literature is effectively optional (outside the minimum bar set by the 3-5 additional private peer reviewers, each with their own limited scope and conflicting interests). Forensic peer-reviewers have been ringing the alarm bell, saying that there is "no net" to bad research {% cite heathersRealScandalIvermectin2021 %}, and brave and highly-skilled investigators like [Elisabeth Bik](https://scienceintegritydigest.com/) have found thousands of papers with evidence of purposeful manipulation {% cite shenMeetThisSuperspotter2020 bikPrevalenceInappropriateImage2016 %}. !! So our existing systems of communication and organization are woefully inadequate for our needs, and don't serve the role of guaranteeing consistency or reliability in research that they claim to. 

It's hard to imagine an alternative to journals that doesn't look like, well, journals. While a full treatment of the journal system is outside the scope of this paper, the system we describe here renders them *effectively irrelevant* by making papers as we know them *unnecessary.* Rather than facing the massive collective action problem of asking everyone to change their publication practices head on, by reconsidering the way we organize the surrounding infrastructure of science we can flank journals and replace them "from below" with something qualitatively more useful. 

Beyond journals, the other technologies of communication that have been adopted out of need, though not necessarily design, serve as [desire paths](https://en.wikipedia.org/wiki/Desire_path) that trace other needs for scientific communication. As a rough sample: Researchers often prepare their manuscripts using platforms like Google Drive, indicating a need for collaborative tools in perparation of an idea. When working in teams, we often use tools like Slack to plan our work. Scientific conferences reflect the need for federated communication within subdisciplines, and we have adopted Twitter as a de facto platform for socializing and sharing our work to a broader audience. We use a handful of blogs and other sites like [OpenBehavior](https://edspace.american.edu/openbehavior/) {% cite whiteFutureOpenOpenSource2019 %}, [Open Neuroscience](https://open-neuroscience.com/), and many others to index technical knowledge and tools. Finally we use sites like [PubPeer](https://pubpeer.com) and ResearchGate for comment and criticism.

!! these tools are don't really suit our needs at all, and constrain rather than support some basic things that we want to do. For example, there is really no venue where we can have sustained, longform, multiparty discussions about difficult topics in our field. Sure, it is possible to publish series of commentary pieces back and forth, or to write blog posts against one another, or to squabble on twitter, but there's nothing that truly supports a cumulative body of public understanding well. 

These technologies point to a few overlapping and not altogether binary axes of communication systems. 
!! make this a table? with technological examples for each.

- **Durable vs Ephemeral** - journals seek to represent information as permanent, archival-grade material, but scientific communication also necessarily exists as contextual, temporally specific snapshots.
- **Structured vs Chronological** - scientific communication both needs to present itself as a structured basis of information with formal semantic linking, but also needs the chronological structure that ties ideas to their context. This axis is a gradient from formal ontologies, through intermediate systems like forums with hierarchical topic structure that embeds a feed, to the purely chronological feed-based social media systems.
- **Messaging vs Indexing** - Communication can be person-to-person or person-to-group messaging with defined senders and recipients, or intended as a generalizable category of objects. This ranges from entirely-specific DMs through domain-specific tool indexes like OpenBehavior through the uniform indexing of Wikipedia.
- **Public vs. Private** - Who gets to read, who gets to contribute? Communication can be composed of entirely private notes to self, through communication in a lab, collaboration group, discipline, and landing in the entirely public realm of global communication. 
- **Formal vs. Informal** - Journal articles and encyclopedia-bound writing that conforms to a particular modality of expression vs. a vernacular style intended to communicate with people outside the jargon culture.
- **Push vs. Pull** - Do you go to get information from a reference location, or does information come to you as an alert or message?

Clearly a variety of different types of communication tools are needed, but there is no reason that each of them should be isolated and inoperable with the others. We have already seen several of the ideas that help bring an alternative into focus. Piracy communities demonstrate ways to build social systems that can sustain infrastructure. Federated and protocol-based systems show us that we don't need to choose between a single monolithic system or many disconnected ones, but can have a heterogeneous space of tools linked by a basic protocol. The semantic web and linked data people showed us the power of triplet links as a very general means of linking disparate systems. We can bridge these lessons with some from the early wiki movement to get a more practical sense of what it takes to give people total control over the structure of their communication and knowledge systems. Together with our sketches of data, analytical, and experimental tools we can start imagining a system for coordinating them --- as well as displacing some of the more intractable systems that misstructure the practice of science.

!! This is a knotty and tangled history, so I am not attempting a full recounting, but will be selectively telling the story to motivate the kinds of tools we need. 


