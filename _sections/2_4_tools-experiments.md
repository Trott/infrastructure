Data that is to be analyzed has to be collected somehow. Tools to bridge the body of experimental practice are a different challenge than analyzing data, or at least so an anecdotal census of scientific software tools would suggest. *Everyone needs completely different things!* As practiced, we might imagine the practice of science as a cone of complexity: We can imagine the relatively few statistical outcomes from a family of tests and models. For every test statistic we can imagine a thousand analysis scripts, for every analysis script we might expect a hundred thousand data formats, and so the french-horn-bell convexity of complexity of experimental tools used to collect the data feels ... different. 

Beyond a narrow focus of the software for performing experiments itself, the contextual knowledge work that surrounds it largely lacks a means of communication and organization. Scientific papers have increasingly marginalized methods sections, being pushed to the bottom, abbreviated, and relegated to supplemental material. The large body of work that is not immediately germane to experimental results, like animal care, engineering instruments, lab management, etc. have effectively no formal means of communication --- and so little formal means of credit assignment.

Extending our ecosystem to include experimental tools has a few immediate benefits: bridging the gap between collection and sharing data would resolve the need for format conversion as a prerequisite for inclusion in the linked system, allowing the expression of data to be a fluid part of the experiment itself; and serving as a concrete means of implementing and building a body of cumulative contextual knowledge in a creditable system.

I have previously written about the design of a generalizable, distributed experimental framework in section 2, and about one modular implementation in section 3 of {% cite saundersAutopilotAutomatingBehavioral2019 %}, and since many of the ideas from the section on analysis tools apply here as well, I will be relatively brief.

We don't have the luxury of a natural formalism like a DAG to structure our experimental tools. Some design constraints on experimental frameworks might help explain why: 

* They need to support a wide variety of instrumentation, from **off-the-shelf parts,** to **proprietary instruments** as are common in eg. microscopy, to **custom, idiosyncratic designs** that might make up the existing infrastructure in a lab. 
* To be supportive, rather than constraining, they need to be able to **flexibly perform many kinds of experiments** in a way that is **familiar to patterns of existing practice.** That effectively means being able to coordinate heterogeneous instruments in some "task" with a flexible syntax.
* They need to be **inexpensive to implement,** in terms of both money and labor, so it can't require buying a whole new set of hardware or dramatically restructuring existing research practices.
* They need to be **accessible and extensible,** with many different points of control with different expectations of expertise and commitment to the framework. It needs to be useful for someone who doesn't want to learn it to its depths, but also have a comprehensible codebase at multiple scales so that reasearchers can **easily extend** it when needed.
* They need to be designed to support **reproducibility and provenance,** which is a significant challenge given the heterogeneity inherent in the system. On one hand, being able to produce *data that is clean at the time of acquisition* simplifies automated provenance, but enabling experimental replication requires multiple layers of abstraction to keep the idiosyncracies of an experiment separable from its implementation: it shouldn't require building *exactly* the same apparatus with *exactly* the same parts connected in *exactly* the same way to replicate an experiment. 
* Ideally, they need to support **cumulative labor and knowledge organization,** so an additional concern with designing abstractions between system components is allowing work to be made portable and combinable with others. The barriers to contribution should be extremely minimal, not requiring someone to be a professional programmer to make a pull request to a central library, and contributions should come in many modes --- code is not the only form of knowing and it's far from the only thing needed to perform an experiment.

Here, as in the domains of data and analysis, the temptation to be universalizing is strong, and the parts of the problem that are emphasized influence the tools that are produced. A common design tactic for experimental tools is to design them as state machines, a system of states and transitions not unlike the analysis DAGs above. One such nascent project is [BEADL](https://archive.org/details/beadl-xml-documentation-v-0.1/mode/2up) {% cite wulfBEADLXMLDocumentation2020 %} from a Neurodata Without Borders [working group](https://archive.org/details/nwb-behavioral-task-wg). BEADL is an XML-based markup for standardizing a behavioral task as an abstraction of finite state machines called [statecharts](https://statecharts.github.io/). Experiments are fully abstract from their hardware implementation, and can be formally validated in simulations. The working group also describes creating a standardized ontology and metadata schema for declaring all the many variable parameters for experiments, like reward sizes, stimuli, and responses {% cite nwbbehavioraltaskwgNWBBehavioralTask2020 %}. This group, largely composed of members from the Neurodata Without Borders team, understandably emphasize systematic description and uniform metadata as a primary design principle.

This means of standardization has many attractive qualities and is being led by very capable researchers, but I think the project is illustrative of how the differing constraints of different systems and differing goals of different approaches influence the possible space of tooling. Analysis tasks are often asynchronous, where the precise timing of each node's completion is less important than the path dependencies between different nodes be clearly specified.  Analysis tasks often have a clearly defined set of start, end, and intermediate cache points, rather than branching or cyclical decision paths that change over multiple timescales. Statecharts are a hierarchical abstraction of finite state machines, the primary advantage of which is that they are better able to incorporate continuous and history-dependent behavior, which causes state explosion in traditional finite-state machines. 

The syntax of statecharts is actually nice, and I am personally a fan. The problem is that it's not necessarily natural to express things as statecharts as you would want to, or in the way that your existing, long-developed local experimental code does. There are only a few syntactical features needed to understand the following statechart: blocks are states, they can be inside each other. Arrows move between blocks depending on some condition. Entering and exiting blocks can make things happen. Short little arrows from filled spots are where you start in a block, and when you get to the end of the chart you go back to the first one. See the following example of a statechart for controlling a light, described in the [introductory documentation](https://statecharts.dev/on-off-statechart.html) and summarized in the figure caption:

![on off delayed exit statechart, see https://statecharts.dev/on-off-statechart.html for full descriptive text](/infrastructure/assets/images/on-off-delayed-exit-1.svg)
*"When you flick a lightswitch, wait 0.5 seconds before turning the light on, then once it's on wait 0.5 seconds before being able to turn it back off again. When you flick it off, wait 2 seconds before you can turn it on again.*

They have an extensive set of documents that defend the consistency and readability of statecharts on their [homepage](https://statecharts.dev/), and my point here is not to disagree with them.

---

<div id="draftmarker"><h1># draftmarker</h1><br>~ everything past here is purely draft placeholder text ~  </div>


!! complexity of system leads us to need to adopt statecharts to handle time
!! focus on format makes us want to adopt schema, and that's really why we're cluing in here: bunch of people in very similar space and aligned ideas but very different conclusions.
!! instead of focusing on the design of the system in a metastructural sense, ie. by finding a way to represent it as a DAGlike or model-based thing, here the design of the system demands a different kind of system, but one that's relatively well accomodated by a system that's based around expression rather than logical completeness.
!! but statecharts clearly demonstrate the way that different constraints on schemas constrain different possible styles of thought. Sure it's possible to reason through that, and it might even be delightful, but how does that look to a stressed grad student trying to figure out how to make the dozens of different hardware components linked together using a body of code summoned from hell itself work in some fancy new system!?  It's definitely more logical and testable and has fewer side effects, but is that the main problem we have!?
!! because we can't imagine the possible different needs of the system *in action* we can split it along different domains of the *tools employed.* Focusing on the ways people already think about the problem rather than redesigning the problem.
!! this is a matter of slicing the problem differently: a lightbulb in statecharts is represented as ... but in autopilot we would think of it as a hardware object. 
!! an attractive feature of statecharts is their abstraction from implementation, but that's possible in other ways too (see autopilot)
!! both are representable in our semantic federation naming system. but the question is how they are embedded in the current practices. This system would be a big lift to incorporate into existing practice. And so as a protocol it does enable a body of different implementations to use it, the schema itself is complicated and encodes a particular style fo thinking. 
!! and finally returning to standardization of schemas, rather than trying to define some vocabulary that evetyon agrees on using in the one means of expressing experiments, it's impossible to take a lesson from language and instead define terms depending on their use: see plugin model and flexible semantic linking. which leads into next section....

!! fit this next idea in in above section where appropriate:
!! this is not to pick on statecharts themselves or to make too big a deal out of them. Upstream a bit we have general purpose tools like psychopy, psychtoolbox, and Bonsai... !! all these are useful contributions to the domain, but contrastive with a generalizable framework.
!! really bigg empathy with statechart ppl, like basically agree with this https://statecharts.dev/faq/why-statecharts-are-not-used.html and they should be used more, but talking broad scale "what works for everyone" it's just too specific!!! !! the alternative model here really is just community control and communication, and statecharts are fully compatible with that. statechart ppl get their own namespace. !! here we're trying to illustrate not the best approach to *experimental software* but how a generalized infrastructure is capable of reflecting itself differently in different domains, and illustrate some advantages of designs that explicitly are intended to accomodate that.

!! proceed to autopilot description, through wiki stuff, to next section.
!! points to highlight are different division of labor, the choice to have schemas be resolved by wiki and continual deliberation rather than constraining a terminological space. plugins that describe different modes of using a system that can all be contributed and are focused on recombination rather than state charts which are more for discrete programming. !! incomplete, but keep brief, demonstrate reference to same idea implemented in statecharts and autopilot-like syntax being able to be linked with different namespaces  !! emphasize reuse of labor by emphasizing recombining different elements. DAGs might not work here, but if the individual nodes are different hardware objects and so on we get a bit closer. Bonsai is a good example of this too, it too has different (valid) design goals/constraints but along different axes.


