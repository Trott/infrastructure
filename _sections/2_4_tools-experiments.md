Data that is to be analyzed has to be collected somehow. Tools to bridge the body of experimental practice are a different challenge than analyzing data, or at least so an anecdotal census of scientific software tools would suggest. *Everyone needs completely different things!* As practiced, we might imagine the practice of science as a cone of complexity: We can imagine the relatively few statistical outcomes from a family of tests and models. For every test statistic we can imagine a thousand analysis scripts, for every analysis script we might expect a hundred thousand data formats, and so the french-horn-bell convexity of complexity of experimental tools used to collect the data feels ... different. 

The balance of labor is badly belied by the minimal importance of methods sections (now written off to supplements). The paper as a communication mechanism is finely tuned to precise results, but methods can often be gestural. Scientific labor is largely a practice of contextual knowledge work, but the details of much of that work are never published. 

- Scientific tools can fill two immediate roles in this ecosystem: eliminating the need for "format conversion," allowing the expression of data format to be a fluid part of the experiment itself; and serving as a concrete means of implementing and building contextual knowledge in a creditable system.
