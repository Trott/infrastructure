We can start with a few of the big and obvious limitations: people could ignore this piece entirely and it is dead on arrival. This project would be a direct threat to some of the most powerful entities in science, and they will likely actively work against it. I could be completely misinformed and missing something fundamental about the underlying technologies. The social tensions between the relevant development communities could be too great to overcome. 

Two outstanding problems on Mastodon hint at a few open challenges to development: feed organization and the fluidity of federation formation, dissolution, and interaction. 

By default, and affirmed by maybe an understandable reaction against algorithmic feed organization, Mastodon is a mostly chronological list of posts from people that you follow and that are in your host server's federated networks. While this transparency is reassuring that we aren't being microtargeted for advertising, it does make the system overwhelming to navigate, and splitting accounts multiple times to accomodate is common. A system of semantic organization is a distinct third way between algorithmic and chronological organization. Building a system that goes beyond moderator-specified category systems familiar in forums towards a sensible interface for navigating tangled concept hierarchies is an open challenge, as far as I'm aware. 

An intermediate goal might be to give finer control over groups, but groups are currently a complicated question between fediverse implementations {% cite StandardizingActivityPubGroups2021 %}. 

<div class="draft-text">
- identity!
- interaction of p2p and linked data system -- lightweight linked metadata can be reproduced more easily than massive raw data, but it needs to be possible to apply permissions and access regulation with more verifiability than just being able to access a unique tracker ID or being pointed to a UUID.
- some might say we will have a hard time indexing across a bunch of namespaces that people hold individually -- this is actually a good thing. We *want* the system to be difficult to make full scrapes to capture and repackage. We *want* connections to be purposeful and transparent, rather than having arbitrary crawlers sucking up all scientific data.
- ppl might lie! ppl already lie! and we handle ambiguity all the time. the real dangerous thing is a system that *presents* itself as infallible/neutral/true.
</div>