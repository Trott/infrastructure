
The shallowest onramp towards a generalized data infrastructure is to make use of existing discipline-specific standardized data formats. As will be discussed later, a truly universal pandisciplinary format is effectively impossible, but to arrive at the alternative we should first congeal the wild west of unstandardized data into a smaller number of established formats. 

Data formats consist of some combination of an abstract specification, an implementation in a particular storage medium, and an API for interacting with the format. I won't dwell on the particular qualities that a particular format needs, assuming that most that would be adopted would abide by FAIR principles. For now we assume that the particular constellation of these properties that make up a particular format will remain mostly intact with an eye towards semantically linking specifications and unifying their implementation.

There are a dizzying number of scientific data formats {% cite teamScientificDataFormats %}, so a comprehensive treatment is impractical here and I will use the Neurodata Without Borders:N (NWB){% cite rubelNWBAccessibleData2019a %} as an example. NWB is the de facto standard for systems neuroscience, adopted by many institutes and labs, though far from uniformly. NWB [consists of](https://www.nwb.org/nwb-software/) a [specification language](https://schema-language.readthedocs.io/en/stable/), a [schema written in that language](https://nwb-schema.readthedocs.io/en/stable/), a [storage implementation in hdf5](https://nwb-storage.readthedocs.io/en/stable/), and an [API for interacting with the data](https://pynwb.readthedocs.io/en/stable/). They have done an admirable job of engaging with community needs {% cite rubelNeurodataBordersEcosystem2021 %} and making a modular, extensible format ecosystem. 

The major point of improvement for NWB, and I imagine many data standards, is the ease of conversion. The conversion API requires extensive programming, knowledge of the format, and navigation of several separate tutorial documents. This means that individual labs, if they are lucky enough to have some partially standardized format for the lab, typically need to write (or hire someone to write) their own [software](https://github.com/catalystneuro/tank-lab-to-nwb) [library](https://github.com/catalystneuro/mease-lab-to-nwb) for conversion. 

Without being prescriptive about its form, substantial interface development is needed to make mass conversion possible. It's usually untrue that unstandardized data had *no structure,* and researchers are typically able to articulate it -- "the filenames have the data followed by the subject id," and so on. Lowering the barriers to conversion mean designing tools that match the descriptive style of folk formats, for example by prompting them to describe where each of an available set of metadata fields are located in their data. It is not an impossible goal to imagine a piece of software that can be downloaded and with minimal recourse to reference documentation allow someone to convert their lab's data within an afternoon. The barriers to conversion have to be low and the benefits of conversion have to outweigh the ease of use from ad-hoc and historical formats.

NWB also has an extension interface, which allows, for example, common data sources to be more easily described in the format. These are registered in an [extensions catalogue](https://nwb-extensions.github.io/), but at the time of writing it is relatively sparse. The preponderance of lab-specific conversion packages relative to extensions is indicative of an interface and community tools problem: presumably many people are facing similar conversion problems, but because there is not a place to share these techniques in a human-readable way, the effort is duplicated in dispersed codebases. We will return to some possible solutions for knowledge preservation and format extension when we discuss tools for [shared knowledge](#shared-knowledge). 

For the sake of the rest of the argument, let us assume that some relatively trivial conversion process exists to subdomain-specific data formats and we reach some reasonable penetrance of standardization. The interactions with the other pieces of infrastructure that may induce and incentivize conversion will come later.

### Peer-to-peer as a Backbone

We should adopt a *peer-to-peer* system for storing and sharing scientific data. There are, of course [many](https://www.dandiarchive.org/) [existing](https://openneuro.org/) [databases](https://www.brainminds.riken.jp/) [for](https://biccn.org/) scientific data, ranging from domain-general like [figshare](https://figshare.com/) and [zenodo](https://zenodo.org/) to the most laser-focused subdiscipline-specific. The notion of a database, like a data standard, is not monolithic. As a simplification, they consist of at least the hardware used for storage, the software implementation of read, write, and query operations, a formatting schema, some API for interacting with it, the rules and regulations that govern its use, and especially in scientific databases some frontend for visual interaction. For now we will focus on the storage software and read-write system, returning to the format, regulations, and interface later. 

Centralized servers are fundamentally constrained by their storage capacity and bandwidth, both of which cost money. In order to be free, database maintainers need to constantly raise money from donations or grants[^grantdb] in order to pay for both. Funding can never be infinite, and so inevitably there must be some limit on the amount of data that someone can upload and the speed at which it can serve files[^osfspeed]. In the case that a researcher never sees any of those costs, they are still being borne by some funding agency, incurring the social costs of funneling money to database maintainers. Centralized servers are also intrinsically out of the control of their users, requiring them to abide whatever terms of use the server administrators set. Even if the database is carefully backed up, it serves as a single point of infrastructural failure, where if the project lapses then at worst data will be irreversibly lost, and at best a lot of labor needs to be expended to exfiltrate, reformat, and rehost the data. The same is true of isolated, local, institutional-level servers and related database platforms, with the additional problem of skewed funding allocation making them unaffordable for many researchers. 

[^grantdb]: granting agencies seem to love funding new databases, idk.

[^osfspeed]: As I am writing this, I am getting a (very unscientific) maximum speed of 5MB/s on the [Open Science Framework](https://osf.io)

Peer-to-peer (p2p) systems solve many of these problems, and I argue are the only type of technology capable of making a database system that can handle the scale of all scientific data. There is an enormous degree of variation between p2p systems[^p2pdiscipline], but they share a set of architectural advantages. The essential quality of any p2p system is that rather than each participant in a network interacting only with a single server that hosts all the data, everyone hosts data and interacts directly with each other.

[^p2pdiscipline]: peer to peer systems are, maybe predictably, a whole academic subdiscipline. See {% cite shenHandbookPeertoPeerNetworking2010 %} for reference.

For the sake of concreteness, we can consider a (simplified) description of Bittorrent {% cite cohenBitTorrentProtocolSpecification2017 %}, arguably the most successful p2p protocol. To share a collection of files, a user creates a `.torrent` file which consists of a [cryptographic hash](https://en.wikipedia.org/wiki/Cryptographic_hash_function), or a string that is unique to the collection of files being shared; and a list of "trackers." A tracker, appropriately, keeps track of the `.torrent` files that have been uploaded to it, and connects users that have or want the content referred to by the `.torrent` file. The uploader (or seeder) then leaves a [torrent client](https://en.wikipedia.org/wiki/Glossary_of_BitTorrent_terms#Client) open waiting for incoming connections. Someone who wants to download the files (a leecher) will then open the `.torrent` file in their client, which will then ask the tracker for the IP addresses of the other peers who are seeding the file, directly connect to them, and begin downloading. So far so similar to standard client-server systems, but the magic is just getting started. Say another person wants to download the same files before the first person has finished downloading it: rather than *only* downloading from the original seeder, the new leecher downloads from *both* the original seeder and the first leecher. Leechers are incentivized to share among each other to prevent the seeders from spending time reuploading the pieces that they already have, and once they have finished downloading they become seeders themselves.

From this very simple example, a number of qualities of p2p systems become clear. 

- First, the system is extremely **inexpensive to maintain** since it takes advantage of the existing bandwidth and storage space of the computers in the swarm, rather than dedicated servers. Near the height of its popularity in 2009, The Pirate Bay, a notorious bittorrent tracker, was estimated to cost $3,000 per month to maintain while serving approximately 20 million peers {% cite roettgersPirateBayDistributing2009 %}. According to a database dump from 2013 {% cite PirateBayArchiveteam2020 %}, multiplying the size of each torrent by the number of seeders (ignoring any partial downloads from leechers), the approximate instantaneous storage size of The Pirate Bay was ~26 Petabytes. The comparison to centralized services is not straightforward, since it is hard to evaluate the distributed costs of additional storage media (as well as the costs avoided by being able to take advantage of existing storage infrastructure within labs and institutes), but for the sake of illustration: hosting 26PB would cost $546,000/month with standard AWS S3 hosting ($0.021/GB/month).
- The **speed** of a bittorrent swarm *increases,* rather than decreases, the more people are using it since it is capable of using all of the available bandwidth in the system.
- The network is extremely **resilient** since the data is shared across many independent peers in the system. If our goal is to make a resilient and robust data architecture, we would benefit by paying attention to the tools used in the broader archival community, especially the archival communities that especially need resilience because their archives are frequent targets of governments and IP-holders{% cite spiesDataIntegrityLibrarians2017 %}.  Despite more than 15 years of concerted effort by governments and intellectual property holders, the pirate bay is still alive and kicking {% cite kim15YearsPirate2019 %}[^knockin]. This is because even if the entire infrastructure of the tracker is destroyed, as it was in 2006, the files are distributed across all of its users, the actual database of `.torrent` metadata is quite small, and the tracker software is extraordinarily simple to rehost {% cite vandersarOpenBayNow2014 %} -- The Pirate Bay was back online in 2 days.  When another tracker, what.cd (which we will return to [soon](#archives-need-communities)) was shut down, a series of successors popped up using the open source tools [Gazelle](https://github.com/WhatCD/Gazelle) and [Ocelot](https://github.com/WhatCD/Ocelot) that what.cd developers built. Within two weeks, one successor site had recovered and reindexed 200,000 of its torrents resubmitted by former users {% cite vandersarWhatCdDead2016 %}. Bittorrent is also used by archival groups with little funding like [Archive Team](https://wiki.archiveteam.org/index.php/Main_Page), who struggled -- but eventually succeeded -- to disseminate their [historic preservation](https://wiki.archiveteam.org/index.php/GeoCities_Project) over a single "crappy cable modem" {% cite scottGeocitiesTorrentUpdate2010 %}. And by groups who disseminate !! return here talking about ddosevrets.
- The network is extremely **scalable** since there is no cost to connecting new peers and the users of a system expand the storage capacity of the system depending on their needs. Rather than having one extremely fast data center (or a privatized network designed to own the internet), the model of p2p systems is to leverage many approachable peer/servers.

[^knockin]: knock on wood

Peer-to-peer systems are not mutually exclusive with centralized servers: servers are peers too, after all. A properly implemented will always be *at least* as fast and have *at least* as much storage as any alternative centralized centralized server because peers can use *both* the bandwidth of the server *and* that of any peers that have the file. In the bittorrent ecosystem large-bandwidth/storage peers are known as "seedboxes"{% cite rossiPeekingBitTorrentSeedbox2014 %} when they use the bittorrent protocol, and "web seeds"{% cite hoffmanHTTPBasedSeedingSpecification %} when they use a protocol built on top of traditional HTTP. [Archive.org](https://archive.org) has been distributing all of its materials [with bittorrent](https://archive.org/details/bittorrent) by using its servers as web seeds since 2012 and makes this point explicitly: "BitTorrent is now the fastest way to download items from the Archive, because the Bittorrent client downloads simultaneously from two different Archive servers located in two different datacenters, and from other Archive users who have downloaded these Torrents already." {% cite kahle000000Torrents2012 %}

p2p systems complement centralized servers in a number of ways beyond raw download speed, increasing the efficiency and performance of the network as a whole. Spotify began as a joint client/server and p2p system {% cite kreitzSpotifyLargeScale2010b %}, where when a listener presses play the central server provides the data until peers that have the song cached are found by the p2p system to download the rest of the song from. The central server is able to respond quickly and reliably to so the song is played as quickly as possible, and is the server of last resort in the case of rare files that aren't being shared by anyone else in the network. A p2p system complements the server and makes that possible by alleviating pressure on the server for more predictable traffic.

A peer to peer system is a particularly natural fit for many of the common circumstances and practices in science, where centralized server architectures seem (and prove) awkward and inefficient. Most labs, institutes, or other organized bodies of science have some form of local or institutional storage systems. In the most frequent cases of sharing data within a lab or institute, sending it back and forth to some nationally-centralized server is like walking across the lab by going the long way around the Earth. That's the method invoked by a Dropbox or AWS link, but in the absence of a formal one you can always revert to a low-fi p2p transfer: walking a flash drive across the lab. The system makes less sense when several people in the same place need to access the same data at the same time, as is frequently the case with multi-lab collaborations, or scientific conferences and workshops. Instead of needing to wait on the 300kb/s conference wifi bandwidth as it's cheese-gratered across every machine, we instead could directly beam it between all computers in range simultaneously, full blast through the decrepit network switch that won't have seen that much excitement in years. 

!! if we take the suggestion of Andrey Andreev et al. and invest in server clusters within institutes {% cite andreevBiologistsNeedModern2021 charlesCommunityDrivenBigOpen2020 %}, their impact could be multiplied manyfold by being able to use them all fluidly and simultaneously for file transfer and storage. !! compatible and extends calls for more institutional support for storage liek andreev's paper, but satisfies the need for generalized storage systems that the NIH doesn't have to develop a whole new institute to handle. extra bonus! in that system each server would have to serve the entire file each time. WIth p2p then the load can be spread between all of them, decreasing costs for all institutions!!!! 

So far I have relied on the Extraordinarily Simplified Bittorrent™️ depiction of a peer to peer system, but there are many improvements and variants that can address different needs for scientific data infrastructure. 

One obvious need that bittorrent can't currently support is version control, but more recent p2p systems do. 
[IPFS](https://ipfs.io/) functions like "a single BitTorrent swarm, exchanging objects within one Git repository." {% cite benetIPFSContentAddressed2014 %}[^whatsgit] Dat {% cite ogdenDatDistributedDataset2017 %}, specifically designed for data synchronization and versioning, handles versioning and more. A full description of IPFS is out of scope, and it has plenty of problems {% cite patsakisHydrasIPFSDecentralised2019 %}, but for now sufficent to say p2p systems can handle version control.

[^whatsgit]:  Git, briefly, is a version control system that keeps a history of changes of files (blobs) as a Merkle DAG: files can be updated, and different versions can be branched and reconciled.

Bittorrent swarms are vulnerable to data loss if all the peers seeding a file disconnect (though the tail is longer than typically assumed, see {% cite zhangUnravelingBitTorrentEcosystem2011 %}), but this too can be addressed with updated p2p system design. A first-order solution to this problem is a variant of IPFS' notion of 'pinning.' Since backup to lab-level or institutional servers is already commonplace, one peer could be able to 'pin' another and automatically download all the data that they share. This concept could scale to institutes and national infrastructure as scientists can request the datasets they'd like to be saved permanently be pinned. 

Another could be something akin to Freenet {% cite clarkeFreenetDistributedAnonymous2001 %}. Peers could allocate a certain amount of their unused storage space to be used to automatically download, cache, and rehost shards of other datasets. Distributing chunks and encrypting them at rest so the rehoster can't inspect their contents would make it possible to maintain privacy and network availability for sensitive data (see, for example, [ERIS](https://inqlab.net/projects/eris/)). IPFS has an analogous concept -- BitSwap -- that is makes it into a barter system. Peers who seek to download will have to 'earn' it by finding some chunk of data that the other peers want, download, and share them, though it seems like an empirical question whether or not a barter system works or is necessary.

[Solid](https://solidproject.org/) is a project that almost exactly meets all these needs {% cite capadisliSolidProtocol2020 sambraSolidPlatformDecentralized2016 SolidP2PFoundation %}. Solid allows people to share data in [Pods](https://solidproject.org/about), which let them control access and distribution across storage system with a unified identity system. It is implementation-agnostic, and so can support peer-to-peer storage and transfer systems that comply with its [protocol specification](https://solidproject.org/TR/protocol).

There are a number of additional requirements for a peer to peer scientific data infrastructure, but even these seemingly very technical problems of versioning and distributed storage show the clear need to consider the structure of the surrounding social system. What control do we give to researchers over the version history of their data? Should people that aren't the originating researcher be able to issue new versions? What structure of distributed/centralized storage works? How should we incentivize sharing of excess storage and resources? 

Even before considering additional social systems, a peer to peer structure in itself implies a different relationship to a generalized data infrastructure. Scientists always unavoidably make their data available to at least one person: themselves; on at least one computer: theirs. A peer-to-peer backbone for scientific infrastructure is the unnecessarily radical notion that everyday practices like these can make up our infrastructure, rather than having it exist exogenously as something "out there." Subtly, it's the notion that our infrastructure can reflect and consist of *ourselves* instead of something out of our control that we need to buy from someone else.

Scientists don't need to reinvent the notion of distributed, community curated data archives from scratch. In addition to scholarly work on the social systems of digital infrastructure, we can learn from communities of practice, and there has been no more important and impactful decentralized archival project than internet piracy.

### Archives Need Communities

Why do hundreds of thousands of people, completely anonymously, with zero compensation, spend their time to do something that is as legally risky as curating pirated cultural archives? 

Scholarly work, particularly from Economics, tends to focus on understanding piracy in order to prevent it{% cite basamanowiczReleaseGroupsDigital2011 hindujaDeindividuationInternetSoftware2008 %}, taking the moral good of intellectual property markets as an *a priori* imperative and investigating why people behave *badly* and "rend [the] moral fabric associated with the respect of intellectual property." {% cite hindujaDeindividuationInternetSoftware2008 %}. If we put the legality of piracy aside, we may find a wealth of wisdom and insight to draw from for building scientific infrastructure.

The world of digital piracy is massive, from entirely disorganized efforts of individual people on public sites to extraordinarily organized release groups {% cite basamanowiczReleaseGroupsDigital2011 %}, and so a full consideration is out of scope, but many of the important lessons are taught by the structure of bittorrent trackers.

An underappreciated element of the BitTorrent protocol is the effect of the separation between the data transfer protocol and the 'discovery' part of the system --- or "overlay" --- on the community structure of torrent trackers (for a more complete picture of the ecosystem, see {% cite zhangUnravelingBitTorrentEcosystem2011 %}). Many peer to peer networks like KaZaA or the gnutella-based Limewire had searching for files integrated into the transfer interface. The need for torrent trackers to share .torrent files spawned a massive community of private torrent trackers that for decades have been iterating on cultures of archival, experimenting with different community structures and incentives that encourage people to share and annotate some of the world's largest, most organized libraries.

One of these private trackers was the site of one of the largest informational tragedies of the past decade: what.cd[^whatdiss], which I will use as an example to describe some of these community systems.

[^whatdiss]: for a detailed description of the site and community, see Ian Dunham's dissertation {% cite dunhamWhatCDLegacy2018 %}

What.cd was a bittorrent tracker that was arguably the largest collection of music that has ever existed. At the time of its destruction in 2016, it was host to just over one million unique releases, and approximately 3.5 million torrents[^dbsize] {% cite dunhamWhatCDLegacy2018 %}. Every torrent was organized in a meticulous system of metadata communally curated by its roughly 200,000 global users. The collection was built by people who cared deeply about music, rather than commercial collections provided by record labels notorious for ceasing distribution of recordings that are not commercially viable --- or just losing them in a fire {% cite rosenDayMusicBurned2019 %}[^lostartists]. Users would spend large amounts of money to find and digitize extremely rare recordings, many of which were unavailable anywhere else and are now unavailable anywhere, period. One former user describes one example:

> “I did sound design for a show about Ceaușescu’s Romania, and was able to pull together all of this 70s dissident prog-rock and stuff that has never been released on CD, let alone outside of Romania” {% cite sonnadEulogyWhatCd2016 %}

[^dbsize]: Though spotify now boasts its library having 50 million tracks, back of the envelope calculations relating number of releases to number of tracks are fraught, given the long tail of track numbers on albums like classical music anthologies with several hundred tracks on a single "release."

![A what.cd artist page (Kanye west) that shows each of his albums having perhaps a dozen different torrents: each time the album was released, on cd, vinyl, and web, each in multiple different audio formats.](/infrastructure/assets/images/kanye-what.png)
*The what.cd artist page for Kanye West (taken from [here](https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/) in the style of pirates, without permission). For the album "Yeezus," there are ten torrents, grouped by each time the album was released on CD and Web, and in multiple different qualities and formats (.flac, .mp3). Along the top is a list of the macro-level groups, where what is in view is the "albums" section, there are also sections for bootleg recordings, remixes, live albums, etc.*

What.cd was a "private" bittorrent tracker, where unlike public trackers that anyone can access, membership was strictly limited to those who were personally invited or to those who passed an interview (for more on public and private tracker, see {% cite meulpolderPublicPrivateBitTorrent %}). Invites were extremely rare, and the interview process was demanding to the point where [entire guides](https://opentrackers.org/whatinterviewprep.com/index.html) were written to prepare for them. When I interviewed in 2009, I had to find my way onto an obscure IRC server, wait in a lobby all day until a volunteer moderator could get to me, and was then grilled on the arcana of digital music formats, spectral analysis[^spectral], the ethics of piracy, and so on for half an hour. Getting a question wrong was an instant failure and you were banned from the server for 48 hours. A single user was only allowed one account per lifetime, so between that policy and the extremely high barriers to entries, even anonymous users were strongly incentivized to follow [the sophisticated, exacting rules for contributing](https://opentrackers.org/whatinterviewprep.com/prepare-for-the-interview/what-cd-rules/index.html). While we certainly don't want such a grueling barrier to entry for scientific data infrastructure, the problem is different and arguably simpler when the system can exist in the open. For example public reputation loss can be a reasonably strong incentive to play by the rules that may trade off with the threat of banning.

[^spectral]: The average what.cd user was, as a result, on par with many of the auditory neuroscientists I know in their ability to read a spectrogram.

The what.cd incentive system was based on a required ratio of data uploaded vs. data downloaded {% cite jiaHowSurviveThrive2013 %}. Peer to peer systems need to overcome a free-rider problem where users might download a torrent ("leeching") and turn their computer off, rather than leaving their connection open to share it to others (or, "seeding"). In order to download additional music, then, one would have to upload more. Since downloading is highly restricted, and everyone is trying to upload as much as they can, torrents had a large number of "seeders," and even rare recordings would be sustained for years, a pattern common to private trackers {% cite liuUnderstandingImprovingRatio2010 %}. 

The high seeder/leecher ratio made it so it was extremely difficult to acquire upload credit, so users were additionally incentivized to find and upload new recordings to the system. What.cd implemented a "bounty" system, where users with a large amount of excess upload credit would be able to offer some of it to whoever was able to upload the album they wanted. To "prime the pump" and keep the economy moving, highlight artists in an album of the week, or direct users to preserve rare recordings, moderators would also use a "freeleech" system, where users would be able to download a specified set of torrents without it counting against their download quantity {% cite kashEconomicsBitTorrentCommunities2012 chenImprovingSustainabilityPrivate2011a %}.

Depending on the age of your account and the amount you had contributed, what.cd users also were given *user classes* that conferred differing degrees of prestige and abilities. This is a common tactic for publicly moderates sites like [StackExchange](https://stackexchange.com) or [Genius](https://genius.com), where users need to demonstrate a certain degree of competency and good faith before they are given the keys to the castle. User classes are both *aspirational* and incentivize additional work on the site, as well as *reputational* where a user class meant you have paid your dues and were a senior contributor.

The other half of what.cd was the more explicitly social elements: its forums, comment sections, and moderation systems. The forum was home to roiling debates that lasted years about the structure of some tagging schema, whether one genre was just another with a different name, and so on. The structure of the community was an object of constant, public negotiation, and over time the metadata system evolved to be able to support a library of the entirety of human music output[^subtlety], and the rules and incentive structures were made to align with building it. To support the good operation of the site, the forums were also home to a huge amount of technical knowledge, like guides on how to make a perfect upload, that eased new users into being able to use the system.

[^subtlety]: Though music metadata might seem like a trivial problem (just look at the fields in an MP3 header), the number of edge cases are profound. How would you categorize an early Madlib casette mixtape remastered and uploaded to his website where he is mumbling to himself while recording some live show performed by multiple artists, but on the b-side is one of his Beat Konducta collections that mix together studio recordings from a collection of other artists? Who is the artist? How would you even identify the unnamed artists in the live show? Is that a compilation or a bootleg? Is it a cassette rip, a remaster, or a web release?

A critical problem in maintaining coherent databases is correcting metadata errors and departures from schemas. Finding errors was rewarded. Users were able to discuss and ask questions of the uploader in a comment section below each upload, which would allow "polite" resolution of low-level errors like typos. More serious problems could be reported to the moderation team, which caused the upload to be visibly marked as under review, and the report could then be discussed either in the comment sections or the forum. If the moderation team affirmed your report, they would usually kick back a few gigabytes of upload credit depending on the severity. Unless the problem was a repeat and malicious one, the "offender" was alerted to it, warned, and told what to do instead next time -- though, being an anonymous, gray-area community, there was plenty of power that was tripped on. Rather than being a messy hodgepodge of fake, low-quality uploads, what.cd was always teetering just shy of perfection.

These structural considerations do not capture the most elusive but indisputably important features of what.cd's community infrastructure: *the sense of commmunity*. The What.cd forums were the center of many user's relationships to music. Threads about all the finest scales of music nichery could last for years: it was a rare place people who probably cared a little bit too much about music could talk to people with the same condition. What made it more satisfying than other music forums was that no matter what music you were talking about, everyone else in the conversation would always have access to it if they wanted to hear it. Independent musicians released albums in the supportive[^mostly] Vanity House section, and people from around the world came to hold the one true album that only they knew about high aloft like a divine tablet. Beyond any structural incentives, people spent so much time building and maintaining what.cd because it became a source of community and a sink of personal investment.

[^mostly]: Mostly. You know how the internet goes...

Structural norms supported by social systems converge as a sort of *reputational* incentive. Uploading a new album to fill a bounty both makes the network more functional and complete, but it also *people respect you for it* because it's prominently displayed on your profile as well as in the bounty charts and that *feels good*. Becoming known on the forums for answering questions, writing guides, or even just having a good taste in music *feels good* and also contributes to the overall health of the system. Though there are plenty of databases, and even plenty of different communication venues for scientists, there aren't any databases (to my knowledge) with integrated community systems. 

The tracker overlay model mirrors and extends some of the recommendations made by Benedikt Fecher and colleagues in their work on the reputational economy surrounding data sharing {% cite fecherReputationEconomyHow2017 %}. They give three policy recommendations: Increasing reputational benefits, reducing transaction costs, and "increasing market transparency by making open access to research data more visible to members of the research community." The primary problem, in their eye, is that the reputational reward of data sharing is too small. In addition to increasing transparency, another way of increasing the reputational reward to sharing data is to embed it within a social system that is designed to reward communitarian behavior with reputational rewards. They continue to ideas like greater reward for data citations (which we will return to in [credit assignment](#credit-assignment)), as well as awards for good datasets. Community awards are also longstanding parts of many digital communities, like What.cd's Album of the Week, which rewarded someone who has done good work by letting them choose an album that would be freely downloadable, or Wikipedia's [Barnstars](https://en.wikipedia.org/wiki/Wikipedia:Barnstars).

Many features of what.cd's structure are undesirable for scientific infrastructure, but they demonstrate that a robust archive is not only a matter of building a database with some frontend, but by building a community {% cite brossCommunityCollaborationContribution2013 %}. Of course, we need to be careful with building the structural incentives for a data sharing system: the very last thing we want is another [coercive leaderboard](https://etiennelebel.com/cs/t-leaderboard/t-leaderboard.html). In contrast to what.cd, for infrastructure we want extremely low barriers to entry, and be agnostic to resources --- researchers with access to huge server farms should not be unduly favored. We should think carefully about using downloading as the "cost," because downloading and analyzing huge amounts of data can be *good* and exactly what we *want* in some circumstances, but a threat to privacy and data governance in others. A better system for science might be closer to [ratioless trackers](https://wiki.installgentoo.com/wiki/Private_trackers#No_economy) that allow infinite downloads as long as they remain seeded for a certain amount of time afterwards, given whatever permissions have been set on the data.

These are all solvable problems, and can be worked on iteratively. They hint at a communication medium where we can discuss our experiments in the same place that they live; linking, embedding, comparing data and techniques to have the kind of longform, cumulative scientific discourse that is for now still relegated to being a fever dream. Rather than being prescriptive about one community structure, what allowed private bittorent trackers to develop and experiment with many different types of systems is the separation from the underlying data from the community overlay. 

This model has its own problems, including the lack of interoperability between different trackers, the need to recreate a new set of accounts and database for each new tracker, among others. It's also been tried before: sharing data in specific formats (as our running example, Neurodata Without Borders) on indexing systems like bittorrent trackers amounts to something like BioTorrents {% cite langilleBioTorrentsFileSharing2010 %} or [AcademicTorrents](https://academictorrents.com/) {% cite cohenAcademicTorrentsCommunityMaintained2014 %}. Even with our extensions of version control and some model of automatic mirroring of data across the network, we still have some work to do. To address these and several other remaining needs for scientific data infrastructure, we can take inspiration from *federated systems.*


