There is no shortage of databases for scientific data, but their traditional structure chokes on the complexity of representing multi-domain data. Typical relational databases require some formal schema to structure the data they contain, which have varying reflections in the APIs used to access them and interfaces built atop them. This broadly polarizes database design into domain-specific and domain-general[^trackeranalogy]. This design pattern results in a fragmented landscape of databases with limited interoperability. How shall we link the databases? In this section we'll consider the Icaric promise of creating the great unified database of everything as a way of motivating an alternative that blends *linked data* {% cite berners-leeLinkedData2006 %} with *federated systems* in the next section.

[^trackeranalogy]: To continue the analogy to bittorrent trackers, an example domain-specific vs. domain-general dichotomy might be What.cd (with its specific formatting and aggregation tools for representing artists, albums, collections, genres, and so on) vs. ThePirateBay (with its general categories of content and otherwise search-based aggregation interface)

Domain-specific databases require data to be in one or a few specific formats, and usually provide richer tools for manipulating and querying by metadata, visualization, summarization, aggregation that are purpose-built for that type of data. For example, NIH's [Gene](https://www.ncbi.nlm.nih.gov/gene/12550) tool has several visualization tools and cross-referencing tools for finding expression pathways, genetic interactions, and related sequences (Figure xx). This pattern of database design is reflected at several different scales, through institutional databases and tools like the Allen [brain atlases](https://connectivity.brain-map.org/) or [observatory](http://observatory.brain-map.org/visualcoding/), to lab- and project-specific dashboards. This type of database is natural, expressive, and powerful --- for the researchers they are designed for. While some of these databases allow open data submission, they often require explicit moderation and approval to maintain the guaranteed consistency of the database, which can hamper mass use.

![An example specialized plot of genomic regions, transcripts and products for the CDH1 gene (linked above), showing how specific tools have been built for this specific dataset](/infrastructure/assets/images/nih_gene_cdh1.png)
*NIH's Gene tool included many specific tools for visualizing, cross-referencing, and aggregating genetic data. Shown is the "genomic regions, transcripts, and product" plot for Mouse Cdh1, which gives useful, common summary descriptions of the gene, but is not useful for, say, visualizing reading proficiency data.*

General-purpose databases like [figshare](https://figshare.com/) and [zenodo](https://zenodo.org/)[^yrcool] are useful for the mass aggregation of data, typically allowing uploads from most people with minimal barriers. Their general function limits the metadata, visualization, and other tools that are offered by domain-specific databases, however, and are essentially public, versioned, folders with a DOI. Most have fields for authorship, research groups, related publications, and a single-dimension keyword or tags system, and so don't programmatically reflect the metadata present in a given dataset.

[^yrcool]: No shade to Figshare, which, among others, paved the way for open data and are a massively useful thing to have in society. 

The dichotomy of fragmented, subdomain-specific databases and general-purpose databases makes combining information from across even extremely similar subdisciplines combinatorically complex and laborious. In the absence of a formal interoperability and indexing protocol between databases, even *finding* the correct subdomain-specific database often comes down to pure luck. It also puts researchers who want to be good data stewards in a difficult position: they can hunt down the appropriate subdomain specific database and risk general obscurity; use a domain-general database and make their work more difficult for themselves and their peers to use; or spend all the time it takes to upload to multiple databases with potentially conflicting demands on format. 

What can be done? There are a few naÃ¯ve answers from standardizing different parts of the process: If we had a universal data format, then interoperability becomes trivial. Conversely, we could make a single ur-database that supports all possible formats and tools. 

The notion of a universal database system almost immediately runs aground on the reality that organizing knowledge is intrinsically political. Every subdiscipline has conflicting *representational* needs, will develop different local terminology, allocate differing granularity and develop different groupings and hierarchies for the same phenomena. At their mildest, differences in representational systems can be incompatible, but at their worst they can reflect and reinforce prejudices and become the site of expression for intellectual and social power struggles {% cite joLessonsArchivesStrategies2020 selbstFairnessAbstractionSociotechnical2019 gebruDatasheetsDatasets2021 bowkerSortingThingsOut1999 %}. Every subdiscipline has conflicting *practical* needs, with infinite variation in privacy demands, different priorities between storage space, bandwidth, and computational power, and so on. In all cases the boundaries of our myopia are impossible to gauge: we might think we have arrived at a suitable schema for biology, chemistry, and physics... but what about the historians?

Matthew J Bietz and Charlotte P Lee articulate this tension in their ethnography of metagenomics databases:

> "Participants describe the individual sequence database systems as if they were shadows, poor representations of a widely-agreed-upon ideal. We find, however, that by looking across the landscape of databases, a different picture emerges. Instead, **each decision about the implementation of a particular database system plants a stake for a community boundary. The databases are not so much imperfect copies of an ideal as they are arguments about what the ideal Database should be.** [...]
>
> In the end, however, **the system was so tailored to a specific set of research questions that the collection of data, the set of tools, and even the social organization of the project had to be significantly changed.** New analysis tools were developed and old tools were discarded. Not only was the database ported to a different technology, the data itself was significantly restructured to fit the new tools and approaches. While the database development projects had begun by working together, in the end they were unable to collaborate. **The system that was supposed to tie these groups together could not be shielded from the controversies that formed the boundaries between the communities of practice.**" {% cite bietzCollaborationMetagenomicsSequence2009 %}

The pursuit of unified representation is an intimate part of the history of linked data, which relies on "ontologies" or controlled vocabularies that describe a set of objects (or classes) and the properties they can have. For example, [schema.org](https://schema.org) maintains a widely used set of hierarchical vocabularies to describe the fundamental things that exist in the world, in particular the unfamiliar world in which a [Person](https://schema.org/Person) has a [gender](https://schema.org/gender) and [net worth](https://schema.org/netWorth) but lacks a race. At one extreme in the world of ontology builders, the ideological nature of demarcating what is allowed to exist is as clear as a klaxon (emphasis in original):

> An exception is the Open Biomedical Ontologies (OBO) Foundry initiative, which accepts under its label only those ontologies that adhere to the principles of ontological realism. [...] Ontologies, from this perspective, are representational artifacts, comprising a taxonomy as their central backbone, whose representational units are intended to designate *universals* (such as *human being* and *patient role*) or *classes defined in terms of universals* (such as *patient,* a class encompassing *human beings* in which there inheres a *patient role*) and certain relations between them. [...]
>
> BFO is a realist ontology [15,16]. This means, most importantly, that representations faithful to BFO can acknowledge only those entities which exist in (for example, biological) reality; thus they must reject all those types of putative negative entities - lacks, absences, non-existents, possibilia, and the like {% cite ceustersFoundationsRealistOntology2010 %}

In practice, because of the difficulty of changing the representation and encompassing database systems on a dime, using these ontologies to link disparate datasets tends to follow the pattern of metadata *overlays* where the structure of individual databases are mapped onto one "unifying" ontology to allow for aggregation and translation. In the remainder of this section, we'll trace the design compromises and eventual applications forced by a unifying metadata overlay with a contemporary case study, The NIH's "Biomedical Data Translator" project, described first in its 2016 Strategic plan for Data Science:

> Through its Biomedical Data Translator program, the National Center for Advancing Translational Sciences (NCATS) is supporting research to develop ways to connect conventionally separated data types to one another to make them more useful for researchers and the public. {% cite NIHStrategicPlan2018 %}

The original [funding statement from 2016](https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator) is similarly humble, and press releases [through 2017](https://web.archive.org/web/20210709171335/https://ncats.nih.gov/pubs/features/translator) also speak mostly in terms of querying the data -- though some ambition begins to creep in. By 2019, the vision for the project had veered sharply away from anything a basic researcher might recognize as a means of translating between data types. In their piece "Toward a Universal Biomedical Translator," then in a feasibility assessment phase, the members of the Translator Consortium assert that universal translation between biomedical data is impossible[^impossibledata]{% cite consortiumUniversalBiomedicalData2019 %}. The impossibility they saw was not that of conflicting political demands on the structure of organization (as per {% cite bowkerSortingThingsOut1999 %}), but of the sheer numeracy of the data and vocabularies needed to describe them. The risk posed by a lack of a universal "language" was not being able to index all possible data, rather than inaccuracy or inequity. 

Undaunted by their stated belief in the impossibility of a universalizing ontology, the Consortium arguably created one in their [biolink](https://biolink.github.io/biolink-model/docs/) model {% cite bruskiewichBiolinkBiolinkmodel2021 %}. Biolink consists of a hierarchy of basic classes: eg. a [BiologicalEntity](https://biolink.github.io/biolink-model/docs/BiologicalEntity.html) like a [Gene](https://biolink.github.io/biolink-model/docs/Gene.html), or a [ChemicalEntity](https://biolink.github.io/biolink-model/docs/ChemicalEntity.html) like a [Drug](https://biolink.github.io/biolink-model/docs/Drug.html). Classes can then linked by any number of properties, or "Slots," like a therapeutic procedure that [treats](https://biolink.github.io/biolink-model/docs/treats.html) a disease. 

Rather than translating *between* data types, Biolink sits "on top of" a [collection of database APIs](http://www.smart-api.info/registry) that serve structured biomedical data. Individual APIs [declare](https://github.com/NCATSTranslator/ReasonerAPI) that they are able to provide data for a particular set of classes or slots, like [drugs that affect genetic expression](http://www.smart-api.info/ui/adf20dd6ff23dfe18e8e012bde686e31), and are then made browsable from the [SmartAPI Knowledge Graph](http://www.smart-api.info/portal/translator/metakg). Queries to individual APIs do not return "raw data," but return assertions of fact in the parlance of the Biolink model: this procedure treats that disease, etc.

At this stage we can already see how the choices made by the Consortium have constrained the form of the Translator. The universalizing nature of the Biolink model requires that data sources map onto it in the form of factual assertions of links between classes. That constrains the possible sources of data to commercial and medical database providers that serve structured data from a REST API, rather than being responsive to researchers or labs who might want to link their raw data splayed out across flash drives and file structures whose chaos borders on whimsy. The NIH RePORTER tool [gives an overview](https://reporter.nih.gov/search/DShVUhB_ZUq0X5UWFjy5WQ/projects?shared=true) of the means by which these data sources are being prepared: automated [text mining](https://reporter.nih.gov/project-details/10548337) tools and a series of [domain-specific data provider](https://reporter.nih.gov/project-details/10056962) projects, rather than via tools provided to researchers. 

These constraints ultimately shape the way that the Translator is intended to be used: a machine learning powered search engine that uses the graph structure of the Biolink model and SmartAPI system to infer relationships between drugs, diseases, and other entities in the model {% cite goelExplanationContainerCaseBased2021 %} with explicit intentions of a platform for both researchers and clinicians {% cite hailuNIHfundedProjectAims2019 %}. 

> With the current version of Translator, a user can enter a question, and the system will synthesize data from a variety of sources and present it to the user in a way that gives the best supported, most interesting answers first. For example, a researcher could ask the system to find all the genes involved in a certain disease and all the chemicals that have been shown to affect expression of those genes. {% cite renaissancecomputinginstituterenciBiomedicalDataTranslator2022 %}

One primary example currently featured by NCATS is using the translator to propose novel treatments for drug-induced liver injury (DILI) {% cite renaissancecomputinginstituterenciUseCasesShow2022 %} detailed in a 2021 conference paper {% cite goelExplanationContainerCaseBased2021 %}. To find a candidate drug, the researchers manually conducted three API queries: first they searched for phenotypes associated with DILI and selected "one of them"[^dilisearch] --- "red blood cell count". Then they queried for genes associated with red blood cell count to find telomerase reverse transcriptase (TERT), and then finally for drugs that affect TERT to find Zidovudine. The directionality of each of these relationships, high vs. low, increases vs. decreases, is unclear in each case. A more recent report on the Translator repeated this pattern of manual querying, arriving at a handful of different genes and drugs {% cite fechoProgressUniversalBiomedical2022 %}. 

While the current examples are highly manual, providing an array of results for each query along with links to associated papers on pubmed, some algorithmic system for ranking results is necessary to make use of the information in the extended knowledge graph. Rather than just the first-order connections, it should be possible to make use of second, third, and n-th order connections to weight potential results. Algorithmic medical recommendation systems have been thoroughly problematized elsewhere (eg. {% cite groteEthicsAlgorithmicDecisionmaking2020 obermeyerDissectingRacialBias2019 panchArtificialIntelligenceAlgorithmic2019 panchInconvenientTruthAI2019 %}). The primary ranking algorithm is developed by a defense contractor (CoVar) who has[^unironically] named it ROBOKOP {% cite mortonROBOKOPAbstractionLayer2019 %}[^fine]. Algorithmic recommendation platforms are in a regulatory gray area {% cite ordishAlgorithmsMedicalDevices2019 el-sayedMedicalAlgorithmsNeed2021 %}, but would arguably need to have interpretable results with clear provenance to pass scrutiny. The DILI example uses a language model which explained the recommendation of Zidovudine with all the clarity of "one of 'DOWNREGULATOR,' 'INHIBITOR,' 'INDIRECT DOWNREGULATOR'." 

[^unironically]: seemingly unironically

[^fine]: which seems totally fine and normal.

The arrival at an algorithmic ranking system for a knowledge graph constructed on top of 200+ data sources follows from a series of decisions made regarding the universal scope and platform-based approach. Its developing structure has a number of features that should raise concern.

First, as with any machine-learning based system, the algorithm can only reflect the structure of its input data, including its bias. The "mass of data" approach ML tools lend themselves to, in this case querying hundreds of independently operated databases, makes dissecting the provenance of every entry from every data provider effectively impossible. For example, one of the providers, [mydisease.info](https://mydisease.info) was more than happy to respond to a query for the outmoded definition of "transsexualism" as a disease {% cite ramTransphobiaEncodedExamination2021 %} along with a list of genes and variants that supposedly "cause" it - [see for yourself](http://mydisease.info/v1/query?q=%22DOID%3A10919%22). Tracing the source of that entry first leads to the disease ontology [DOID:1234](https://web.archive.org/web/20211007053446/https://www.ebi.ac.uk/ols/ontologies/doid/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FDOID_1234) which seems to trace back into an entry in a graph aggregator [Ontobee](http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234) ([Archive Link](https://web.archive.org/web/20210923110103/http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234)), which in turn lists this [github repository](https://github.com/jannahastings/mental-functioning-ontology) **maintained by a single person** as its source[^ipredit]. This is, presumably, the fragility and inconsistency in input data that the machine learning layer is intended to putty over.

[^ipredit]: I submitted a [pull request](https://github.com/jannahastings/mental-functioning-ontology/pull/8) to remove it, but it has not been merged more than 8 months later. A teardrop in the ocean. 

If the graph encodes being transgender as a disease, it is not farfetched to imagine the ranking system attempting to "cure" it. Even if it doesn't directly, the graph-based nature of the system means that any given entry will have unpredictable consequences on recommendations made from the surrounding network of objects like genes, treatment history, and so on. If the operation of the ranking algorithm is uninterpretable, as most are, or the algorithm it itself proprietary, harmful input data could have long-range influence on both the practice of medicine as well as the course of basic research *without anyone being able to tell.* The Consortium also describes a system whereby the algorithm is continuously updated based on usage of results in research or clinical practice {% cite consortiumUniversalBiomedicalData2019 %}, which stands to magnify the problem of algorithmic bias by uncritically treating harmful treatment and research practices as training data.

The approach creates a fundamental tradeoff between algorithmic interpretability and the system being useful at all. The paper cited in the 2021 DILI example as evidence that the system gives plausible results is for a specific subclass of liver injuries caused by anti-tuberculosis drugs {% cite udomsinprasertLeukocyteTelomereLength2020 %}, highlighting the danger of automated recommendations from noisy data, but also calling into question what novel contribution the Translator made if telomeres were already implicated in DILI. The 2022 report gives examples where the results were already expected by the researchers, or provided a series of papers that seems difficult to imagine being much more informative than a PubMed search. If the algorithmic recommendations are unexpected --- ie. the system provides novel information --- the process of confirming them appears to be near-identical to the usual process of reading abstracts and hopping citation trees.

Perhaps most worrisome is the eventual fate of the project in the hands of the broader ecosystem of orbiting information conglomerates. Centralized infrastructure projects can be an opportunity for for-profit companies to "dance until the music stops" and then scoop up any remaining technology when the funding dries up (so far roughly [$81.6 million](https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear&sort_order=desc) since 2016 for the Translator {% cite RePORTRePORTERBiomedical2021 %}, and [$84.7 million](https://reporter.nih.gov/search/H4LxgMGK9kGw6SeWCom85Q/projects?shared=true) for the discontinued NIH Data Commons pilot). I have little doubt that the scientists and engineers working on the Translator are doing so with the best of intentions --- the real question is what happens to it after it's finished.

Knowledge graphs in particular are promising targets for platform holders. Perhaps the most well known example is Google's 2010 acquisition of Freebase (via Metaweb) {% cite subramanianGoogleBuysFreebase2010 %}, a graph of structured data with a wealth of properties for common people, places and things. Google incorporated it into their Knowledge Graph {% cite IntroducingKnowledgeGraph2012 %} to populate its factboxes and make its search results more semantically aware in its Hummingbird upgrade in 2013, the largest overhaul of its search engine since 2001 {% cite sullivanFAQAllNew2013 %}, cementing its dominance as a search engine. Searching has a different set of cognitive expectations than browsing a database: we expect search results to be "best effort," not necessarily complete or accurate, where when browsing a database it's relatively clear when information is missing or inaccurate. For products packaged up into search platforms by for-profit companies, *it doesn't have to actually work* as long as it seems like it does.

So while an algorithmic recommendation tool may have limited use for the basic researchers it was originally intended for, it is likely to be extremely useful for the booming business of "personalized medicine." Linking biomedical and patient data in a single platform is a natural route towards a multisided market where records management apps are sold to patients, treatment recommendation systems are sold to clinicians, and research tools and advertising opportunities are sold to pharmaceutical companies, risk metrics are sold to insurance companies, and so on. The platformatization of the knowledge graph, along with carefully worded terms of service, is a clean means by which "good enough" results could be jackknifed into an expanded system of biomedical surveillance. Since the algorithm needs continual training, the translator has every incentive to suck up as much personal data as it can.

Multiple information conglomerates are posed to capitalize on the translator project. Amazon already has a broad home surveillance portfolio {% cite bridgesAmazonRingLargest2021 %}, and has been aggressively expanding into health technology {% cite AWSAnnouncesAWS2021 %} and even literally providing [health care](https://amazon.care/) {% cite lermanAmazonBuiltIts2021 %}, which could be particularly dangerous with the uploading of all scientific and medical data onto AWS with entirely unenforceable promises of data privacy {% cite quinnYouCanTrust2021 %}.

RELX, parent of Elsevier, is as always the terrifying elephant in the room. In addition to distribution rights for a large proportion of scientific knowledge and a collection of research databases, it also sells a clinical reference platform in ClinicalKey, point of service clinical products for plannint patient care with ClinicalPath, medical education tools, and pharmaceutical advertisements designed to look like scientific papers {% cite elsevier360AdvertisingSolutions %}, among others {% cite relx2021AnnualReport2021 %}. It also is explicitly expanding into "clinical decision support applications" {% cite relx2021AnnualReport2021 %} and recently embedded its medication management product into Apple's watchOS 9 {% cite appleWatchOSDeliversNew2022 %}. Subsidiaries in RELX's "Risk" market segment sell risk profiles to insurance companies based on what they claim to be highly comprehensive profiles of harvested personal data. The Translator infrastructure is a perfect keystone to unify these products: after the NIH fronts the money to develop it and lends the credibility of basic research, RELX can cheaply expand its surveillance apparatus to enhanced medical risk profiles to insurers, priority placement in candidate drug rankings to pharmaceutical companies, and augment its ranking systems for funders and employers to include some proprietary metric of "promisingness" to encourage researchers to follow its research recommendations. This isn't speculative --- it can just strap whatever clinical data Translator gains access to into its [existing biomedical knowledge graph](https://www.elsevier.com/solutions/biology-knowledge-graph).

Even assuming the Translator works perfectly and has zero unanticipated consequences, the development strategy still reflects the inequities that pervade science rather than challenge them. Biopharmaceutical research, followed by broader biomedical research, being immediately and extremely profitable, attracts an enormous quantity of resources and develops state of the art infrastructure, while no similar infrastructure is built for the rest of science, academia, and society. 

<div class="draft-text" markdown="1">
Below here is a bunch of scraps leftover from 2022-06-18

todo:

* bring narrative of structural constraints that add up to a potentially harmful translator down here.
* use that to transition into "so what could we have done instead" for the next section, emphasizing the kind of thinking that led us here in the first place.
* simplify and shorten all of the below and be less didactic and breathless

</div>

The problems here come in a few mutually reinforcing flavors, I'll group them crudely into the constraints of existing infrastructure, centralized models of development, and a misspecification of what the purpose of the infrastructure should be.

Navigating a relationship with existing technology in new development is tricky, but there is a distinction between integrating with it and embodying its implications. Since the other projects spawned from the Data Science Initiative embraced the use of cloud storage, the constraint of using centralized servers with the need for a linking overlay was baked in the project from the beginning. From this decision immediately comes the impossibility of enforcing privacy guarantees and the rigidity of database formats and tooling. Since the project started from a place of presuming that the data would be hosted "out there" where much of its existence is prespecified, building the Translator "on top" of that system is a natural conclusion. Further, since the centralized systems proposed in the other projects don't aim to provide a means of standardization or integration of scientific data that doesn't already have a form, the reliance on APIs for access to structured data follows as well.

Organizing the process as building a set of tools as a relatively large, but nonetheless centralized and demarcated group pose additional challenges. I won't speculate on the incentives and personal dynamics that led there, but I also believe this development model comes from good intention. While there is clearly a lot of delegation and distributed work, the project in its different teams takes on specific tools that *they* build and *we* use. This is broadly true of scientific tools, especially databases, and contributes to how they *feel*: they feel disconnected with our work, don't necessarily help us do it more easily or more effectively, and contributing to them is a burdensome act of charity.

These all contribute to the misdirection in the goal of the project. Linking *all* or *most* biomedical data in single mutually coherent system drifted into an API-driven knowledge-graph for pharmaceutical and clinical recommendations. Here we meet a bit of a reprise of the [#neat](#neatness-vs-scruffiness) mindset, which emphasizes global coherence as a basis for reasoning rather than providing a means of expressing the natural connections between things in their local usage. Put another way, the emphasis is on making something logically complete for some dream of algorithmically-perfect future rather than to be useful to do the things researchers at large want to do but find difficult. The press releases and papers of the Translator project echo a lot of the heady days of the semantic web[^diderot] and its attempt to link everything --- and seems ready to follow the same path of the fledgling technologies being gobbled up by technology giants to finish and privatize.

[^diderot]: not to mention a sort of enlightenment-era diderot-like quest for the encyclopedia of everything

I think the problem with the initial and eventual goals of the translator can be illustrated by problematizing the central focus on linking "all data," or at least "all biomedical data." Who is a system of "all (biomedical) data" for? Outside of metascientists and pharmaceutical companies, I think most people are interested primarily in the data of their colleagues and surrounding disciplines. Every infrastructural model is an act of balancing constraints, and prioritizing "all data" seems to imply "for some people." Who is supposed to be able to upload data? change the ontology? inspect the machine learning model? Who is in charge of what? Who is a knowledge-graph query engine useful for?

Another prioritization might be building systems for *all people* that can *embed with existing practices* and *help them do their work* which typically involves accessing *some data.* The system needs to not only be designed to allow anyone to integrate their data into it, but also to be integrated into how researchers collect and use their data. It needs to give them firm, verifiable, and fine-grained control over who has access to their data and for what purpose. It needs to be *pluralistic,* capable of representing multiple potentially conflicting representations, governable and malleable in local communities of practice. Through the normal act of making my data available to my colleague and vice versa, we build on a cumulative and negotiable understanding of the relationship between our work and its meaning. 

Without too much more prefacing, let's return to the scheduled programming.
