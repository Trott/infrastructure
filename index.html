<!DOCTYPE html>
<html lang="en" prefix="foaf: http://xmlns.com/foaf/0.1/">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Decentralized Infrastructure for (Neuro)science</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Decentralized Infrastructure for (Neuro)science" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://jon-e.net/infrastructure" />
<meta property="og:url" content="https://jon-e.net/infrastructure" />
<meta property="og:site_name" content="Decentralized Infrastructure for (Neuro)science" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Decentralized Infrastructure for (Neuro)science" />
<script type="application/ld+json">
{"sameAs":["https://twitter.com/json_dirs","https://jon-e.net"],"url":"https://jon-e.net/infrastructure","@type":"WebSite","name":"Jonny Saunders","headline":"Decentralized Infrastructure for (Neuro)science","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/infrastructure/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
<!--   <link rel="icon" type="image/png" sizes="32x32" href="/infrastructure/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/infrastructure/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/infrastructure/assets/apple-touch-icon.png">
 -->
  <!-- RSS -->


</head>


  <body vocab="https://schema.org/" typeof="ScholarlyArticle">

    

    <main>
      <div class="post">
    <div class="post-info">
        <h1 class="post-title" property="title">Decentralized Infrastructure for (Neuro)science</h1>
        
        
          <h2 class="post-subtitle">Or, Kill the Cloud in Your Mind</h2>
        

        <div class="post-line"></div>

        
          
          <span property="author" resource="#au:jonny-saunders" typeof="Person">
            <span class="post-author" property="name">Jonny Saunders</span>
            <span property="email" content="jsaunder@uoregon.edu"> 
            <a href="mailto:jsaunder@uoregon.edu">
                <i class="far fa-envelope"></i>
            </a>
            </span>
          </span>
          
        

        <br>

        

        <div class="post-date">
            <span>Rendered: </span>
            <time datetime="2022-43-11"> 2022-07-11</time> - <span style="color:#444; font-style:none">permalink:</span> <a href="/infrastructure/versions/"><code></code></a>  - <a href="/infrastructure/tex/decentralized_infrastructure_render.pdf"><code>PDF VERSION</code></a> 
        </div>

        <div class="acknowledgements">
            <h3>With gratitude to...</h3>
            
            <div class="group">
                <span class="prefix">Labmates</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Lucas Ott, the steadfast">
                            Lucas Ott, the steadfast,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Tillie Morris">
                            Tillie Morris,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Nick Sattler">
                            Nick Sattler,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Sam Mehan">
                            Sam Mehan,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Molly Shallow">
                            Molly Shallow,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Mike and as always ty for letting me always go rogue">
                            Mike and as always ty for letting me always go rogue,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">Committee</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Matt Smear">
                            Matt Smear,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Santiago Jaramillo">
                            Santiago Jaramillo,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">linked data</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Gabriele Hayden">
                            Gabriele Hayden,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Joel Chan">
                            Joel Chan,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Tomasz Pluskiewicz">
                            Tomasz Pluskiewicz,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="James Meickle">
                            James Meickle,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Christine Lemmer-Webber">
                            Christine Lemmer-Webber,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Arnold Schrijver">
                            Arnold Schrijver,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Aad Versteden">
                            Aad Versteden,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Leslie Harka">
                            Leslie Harka,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="nwb & dandi teams">
                            nwb & dandi teams,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">STS</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Os Keyes">
                            Os Keyes,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Avery Everhart">
                            Avery Everhart,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Eartha Mae Guthman">
                            Eartha Mae Guthman,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Olivia Guest">
                            Olivia Guest,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">Neuroscientists</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Lauren E. Wool">
                            Lauren E. Wool,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Kris Chauvin">
                            Kris Chauvin,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Phil Parker">
                            Phil Parker,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Ceci Herbert">
                            Ceci Herbert,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Chris Rogers">
                            Chris Rogers,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Petar Todorov">
                            Petar Todorov,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Jeremy Delahanty">
                            Jeremy Delahanty,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Andrey Andreev">
                            Andrey Andreev,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Ralph Emilio Peterson">
                            Ralph Emilio Peterson,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Manuel Schottdorf">
                            Manuel Schottdorf,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">Open Source Ppl</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="jakob voigts for participating in the glue wiki">
                            jakob voigts for participating in the glue wiki,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Gonçalo Lopes">
                            Gonçalo Lopes,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Mackenzie Mathis">
                            Mackenzie Mathis,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Mark Laubach & Open Behavior Team">
                            Mark Laubach & Open Behavior Team,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">Free Internet and Information Liberationists</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Irene Knapp">
                            Irene Knapp,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Nire Bryce">
                            Nire Bryce,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Danny Mclanahan">
                            Danny Mclanahan,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Björn Brembs">
                            Björn Brembs,
                        </span>
                    </span>
                
            </div>
            
            <div class="group">
                <span class="prefix">And all the other wisdom givers</span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Sanjay Srivastava & Metascience Class">
                            Sanjay Srivastava & Metascience Class,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Joon An">
                            Joon An,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="The Emerging ONICE team">
                            The Emerging ONICE team,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="The Janet Smith House">
                            The Janet Smith House,
                        </span>
                    </span>
                
                    <span class="name" property="contributor" typeof="Person">
                        <span property="name" content="Rumbly Tumbly Lawnmower">
                            Rumbly Tumbly Lawnmower,
                        </span>
                    </span>
                
            </div>
            
        </div>

    </div>
    


    <div property="articleBody">
    <div class="annotation-container">
	<h2>This document is in-progress, feedback welcome!</h2>
	<p>
		If you are reading this, at this point <strong>please do not redistribute this link!</strong> As you'll see it is still *very* drafty &lt;3
	</p>
	<p>
		This page embeds <a href="https://hypothes.is">hypothes.is</a>, which allows you to annotate the text!
        Existing annotations should show up highlighted like this,
        and you can make a new annotation by highlighting an area of the text and clicking the 'annotate' button that appears! You can see all annotations by opening the sidebar on the right.
	</p>
	<p>
		Unfortunately, hypothes.is causes the page to freeze on load currently (especially on mobile). I've submitted an issue: <a href="https://github.com/hypothesis/client/issues/3919">https://github.com/hypothesis/client/issues/3919</a>
	</p>
	<p>
		If you do work that you think is relevant here but I am not citing it, it's 99% likely that's because I haven't read it, not that I'm deliberately ignoring you! Feel free to annotate any missing citations, odds are I'd love to read &amp; cite your work, and if you're working in the same space try and join efforts! 
	</p>
	<h2>Current Document Status (21-12-06):</h2>
	<p>
		We've made it to the end of the document! There are plenty of places with placeholder text and extra plenty editing to do, but we're almost there!
	</p>
</div>
<div class="trimlink">
<a href="trims">Trimmings</a> <span>from the main document for future pieces</span>
</div>
<div class="trimlink">
<a href="todo">Todo</a> <span>what's left to be done?</span>
</div>

<div id="toc-drawer"></div>
<nav class="toc" id="toc">
<ol id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#the-state-of-things" id="markdown-toc-the-state-of-things">The State of Things</a>    <ol>
      <li><a href="#the-costs-of-infrastructure-deficits" id="markdown-toc-the-costs-of-infrastructure-deficits">The Costs of Infrastructure Deficits</a></li>
      <li><a href="#misincentives-in-scientific-software" id="markdown-toc-misincentives-in-scientific-software">(Mis)incentives in Scientific Software</a>        <ol>
          <li><a href="#incentivized-fragmentation" id="markdown-toc-incentivized-fragmentation">Incentivized Fragmentation</a></li>
          <li><a href="#domain-specific-silos" id="markdown-toc-domain-specific-silos">Domain-Specific Silos</a></li>
          <li><a href="#the-long-now-of-immediacy-vs-idealism" id="markdown-toc-the-long-now-of-immediacy-vs-idealism">“The Long Now” of Immediacy vs. Idealism</a></li>
          <li><a href="#neatness-vs-scruffiness" id="markdown-toc-neatness-vs-scruffiness">“Neatness” vs “Scruffiness”</a></li>
          <li><a href="#taped-on-interfaces-open-loop-user-testing" id="markdown-toc-taped-on-interfaces-open-loop-user-testing">Taped-on Interfaces: Open-Loop User Testing</a></li>
          <li><a href="#platforms-industry-capture-and-the-profit-motive" id="markdown-toc-platforms-industry-capture-and-the-profit-motive">Platforms, Industry Capture, and the Profit Motive</a></li>
          <li><a href="#protection-of-institutional-and-economic-power" id="markdown-toc-protection-of-institutional-and-economic-power">Protection of Institutional and Economic Power</a></li>
        </ol>
      </li>
      <li><a href="#the-ivies-institutes-and-the-rest-of-us" id="markdown-toc-the-ivies-institutes-and-the-rest-of-us">The Ivies, Institutes, and “The Rest of Us”</a>        <ol>
          <li><a href="#institutional-core-facilities" id="markdown-toc-institutional-core-facilities">Institutional Core Facilities</a></li>
          <li><a href="#centralized-institutes" id="markdown-toc-centralized-institutes">Centralized Institutes</a></li>
          <li><a href="#meso-scale-collaborations" id="markdown-toc-meso-scale-collaborations">Meso-scale collaborations</a></li>
          <li><a href="#the-rest-of-us" id="markdown-toc-the-rest-of-us">The rest of us…</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#a-draft-of-decentralized-scientific-infrastructure" id="markdown-toc-a-draft-of-decentralized-scientific-infrastructure">A Draft of Decentralized Scientific Infrastructure</a>    <ol>
      <li><a href="#design-principles" id="markdown-toc-design-principles">Design Principles</a>        <ol>
          <li><a href="#protocols-not-platforms" id="markdown-toc-protocols-not-platforms">Protocols, not Platforms</a></li>
          <li><a href="#integration-not-invention" id="markdown-toc-integration-not-invention">Integration, not Invention</a></li>
          <li><a href="#embrace-heterogeneity-be-uncoercive" id="markdown-toc-embrace-heterogeneity-be-uncoercive">Embrace Heterogeneity, Be Uncoercive</a></li>
          <li><a href="#empower-people-not-systems" id="markdown-toc-empower-people-not-systems">Empower People, not Systems</a></li>
          <li><a href="#infrastructure-is-social" id="markdown-toc-infrastructure-is-social">Infrastructure is Social</a></li>
          <li><a href="#usability-matters" id="markdown-toc-usability-matters">Usability Matters</a></li>
        </ol>
      </li>
      <li><a href="#shared-data" id="markdown-toc-shared-data">Shared Data</a>        <ol>
          <li><a href="#formats-as-onramps" id="markdown-toc-formats-as-onramps">Formats as Onramps</a></li>
          <li><a href="#peer-to-peer-as-a-backbone" id="markdown-toc-peer-to-peer-as-a-backbone">Peer-to-peer as a Backbone</a></li>
          <li><a href="#archives-need-communities" id="markdown-toc-archives-need-communities">Archives Need Communities</a></li>
          <li><a href="#linked-data-or-surveillance-capitalism" id="markdown-toc-linked-data-or-surveillance-capitalism">Linked Data or Surveillance Capitalism?</a></li>
          <li><a href="#folk-federation" id="markdown-toc-folk-federation">Folk Federation</a></li>
        </ol>
      </li>
      <li><a href="#shared-tools" id="markdown-toc-shared-tools">Shared Tools</a>        <ol>
          <li><a href="#analytical-frameworks" id="markdown-toc-analytical-frameworks">Analytical Frameworks</a></li>
          <li><a href="#experimental-frameworks" id="markdown-toc-experimental-frameworks">Experimental Frameworks</a></li>
        </ol>
      </li>
      <li><a href="#shared-knowledge" id="markdown-toc-shared-knowledge">Shared Knowledge</a>        <ol>
          <li><a href="#the-wiki-way" id="markdown-toc-the-wiki-way">The Wiki Way</a></li>
          <li><a href="#rebuilding-scientific-communication" id="markdown-toc-rebuilding-scientific-communication">Rebuilding Scientific Communication</a>            <ol>
              <li><a href="#documents--notebooks" id="markdown-toc-documents--notebooks">Documents &amp; Notebooks</a></li>
              <li><a href="#forums--feeds" id="markdown-toc-forums--feeds">Forums &amp; Feeds</a></li>
              <li><a href="#overlays--adversarial-interoperability" id="markdown-toc-overlays--adversarial-interoperability">Overlays &amp; Adversarial Interoperability</a></li>
              <li><a href="#trackers-clients--wikis" id="markdown-toc-trackers-clients--wikis">Trackers, Clients, &amp; Wikis</a></li>
            </ol>
          </li>
          <li><a href="#credit-assignment" id="markdown-toc-credit-assignment">Credit Assignment</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ol>
      <li><a href="#tactics--strategy" id="markdown-toc-tactics--strategy">Tactics &amp; Strategy</a>        <ol>
          <li><a href="#gestural-roadmap" id="markdown-toc-gestural-roadmap">(Gestural) Roadmap</a></li>
          <li><a href="#to-whom-it-may-concern" id="markdown-toc-to-whom-it-may-concern">To Whom It May Concern…</a>            <ol>
              <li><a href="#rank-and-file-researchers" id="markdown-toc-rank-and-file-researchers">Rank and File Researchers</a></li>
              <li><a href="#open-source-developers" id="markdown-toc-open-source-developers">Open Source Developers</a></li>
              <li><a href="#funding-agencies" id="markdown-toc-funding-agencies">Funding Agencies</a></li>
              <li><a href="#university-administrators" id="markdown-toc-university-administrators">University Administrators</a></li>
              <li><a href="#librarians" id="markdown-toc-librarians">Librarians!</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#limitations" id="markdown-toc-limitations">Limitations</a></li>
      <li><a href="#in-closing" id="markdown-toc-in-closing">In Closing</a></li>
      <li><a href="#contrasting-visions-of-science" id="markdown-toc-contrasting-visions-of-science">Contrasting Visions of Science</a>        <ol>
          <li><a href="#what-if-we-do-nothing" id="markdown-toc-what-if-we-do-nothing">What if we do nothing?</a></li>
          <li><a href="#what-we-could-build" id="markdown-toc-what-we-could-build">What we could build</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#footnotes" id="markdown-toc-footnotes">Footnotes</a></li>
</ol>
  <svg class="toc-marker" id="toc-marker" width="200" height="200" xmlns="http://www.w3.org/2000/svg">
  </svg>
  <div id="rdfa-component"></div>
</nav>

<blockquote>
  <p>If we can make something decentralised, out of control, and of great simplicity, we must be prepared to be astonished at whatever might grow out of that new medium.</p>

  <p><a href="https://www.w3.org/1998/02/Potential.html">Tim Berners-Lee (1998): Realising the Full Potential of the Web</a></p>
</blockquote>

<blockquote>
  <p>A good analogy for the development of the Internet is that of
constantly renewing the individual streets and buildings of a city,
rather than razing the city and rebuilding it. The architectural
principles therefore aim to provide a framework for creating
cooperation and standards, as a small “spanning set” of rules that
generates a large, varied and evolving space of technology.</p>

  <p><a href="https://datatracker.ietf.org/doc/html/rfc1958">RFC 1958: Architectural Principles of the Internet</a></p>
</blockquote>

<blockquote>
  <p>In building cyberinfrastructure, the key question is not whether a problem is a “social” problem or a “technical” one. That is putting it the wrong way around. The question is whether we choose, for any given problem, a primarily social or a technical solution</p>

  <p><a href="https://doi.org/10.1007/978-1-4020-9789-8_5">Bowker, Baker, Millerand, and Ribes (2010): Toward Information Infrastructure Studies</a> <a class="citation" href="#bowkerInformationInfrastructureStudies2010">[1]</a></p>
</blockquote>

<blockquote>
  <p>The critical issue is, how do actors establish generative platforms by instituting a set of control points acceptable to others in a nascent ecosystem? <a class="citation" href="#tilsonDigitalInfrastructuresMissing2010">[2]</a></p>
</blockquote>

<blockquote>
  <p>Billionaires have squatted on the Magna Cum Lauded / […]
Methodically they plotted against those who fought it / […] / 
Now the scientific process got hijacked for profits /
It flows in the direction that a silver spoon prodded / 
We’ll get science for the people when we run the economics.</p>

  <p>The Coup (2012) <a href="https://youtu.be/lW59xoilGnw">The Gods of Science</a></p>
</blockquote>

<hr />

<h1 id="introduction">Introduction</h1>

<p>We work in an archipelago of technical islands: researchers, labs, consortia, and a few well-funded institutions reinventing the wheel in parallel. Our knowledge dissemination systems are as nimble as static pdfs<sup id="fnref:socialmediaflirting" role="doc-noteref"><a href="#fn:socialmediaflirting" class="footnote" rel="footnote">1</a></sup> served by an extractive publishing turned surveillance industry we can’t seem to quit. Experimental instrumentation except for that at the polar extremes of technological complexity or simplicity is designed and built custom, locally, and on-demand<sup id="fnref:disciplinecaveat" role="doc-noteref"><a href="#fn:disciplinecaveat" class="footnote" rel="footnote">2</a></sup>. Software for performing experiments is a patchwork of libraries that satisfy some of the requirements of the experiment, sewn together by some uncommented script written years ago by a grad student who left the lab long-since. The technical knowledge to build both instrumentation and software is fragmented and unavailable as it sifts through the funnels of word-limited methods sections and never-finished documentation. Our data is born into this world without coherent form to speak of, indexable only by passively-encrypted notes in a paper lab notebook, and analyzed once before being mothballed in ignominy on some unlabeled external drive.</p>

<p>These problems are typically treated in isolation, but all are symptomatic of a broader deficit in <strong>digital infrastructure</strong> for science. Every routine need that requires heavy technical development, an appeal to a hostile publishing system, or yet another platform subscription is an indicator that infrastructural deficits <em>define the daily reality of science.</em> We <em>should</em> be able to easily store, share, and search for data; be able to organize and communicate with each other; be able to write and review our work, but we are hemmed in on all sides by looming tech profiteers and chasms of underdevelopment.</p>

<p>If the term infrastructure conjures images of highways and plumbing, then surely digital infrastructure would be flattered at the association. Roughly following Star and Ruhleder’s (1996) dimensions <a class="citation" href="#starStepsEcologyInfrastructure1996">[3]</a>, by analogy they illustrate many of its promises and challenges: when designed to, it can make practically impossible things trivial, allowing the development of cities by catching water where it lives and snaking it through tubes and tunnels sometimes directly into your kitchen. Its absence or failure is visible and impactful, as in the case of power outages. There is no guarantee that it “optimally” satisfies some set of needs for the benefit of the greatest number of people, as in the case of the commercial broadband duopolies. It exists not only as its technical reality, but also as an embodied and shared set of social practices, and so even when it does exist its form is not inevitable or final; as in the case of bottled water producers competing with municipal tap water on a behavioral basis despite being dramatically less efficient and more costly. Finally it is not socially or ethically neutral, and the impact of failure to build or maintain it is not equally shared, as in the expression of institutional racism that was the Flint, Michigan water crisis <a class="citation" href="#michicancivilrightscommissionFlintWaterCrisis2017">[4]</a>.</p>

<p>Infrastructural deficits are not our inevitable and eternal fate, but the course of infrastructuring is far from certain. It is not the case that “scientific digital infrastructure” will rise from the sea monolithically as a natural result of more development time and funding, but instead has many possible futures<a class="citation" href="#mirowskiFutureOpenScience2018">[5]</a>, each with their own advocates and beneficiaries. Without concerted and strategic development based on a shared and liberatory ethical framework, science will continue to follow the same path as other domains of digital technology down the dark road of platform capitalism. The prize of owning the infrastructure that the practice of science is built on is too great, and it is not hard to imagine tech behemoths buying out the emerging landscape of small scientific-software-as-a-service startups and selling subscriptions to Science Prime.</p>

<p>The possibility of future capture of nascent infrastructure is still too naive a framing: operating as obligate brokers of (usually surveillance) data<a class="citation" href="#pooleySurveillancePublishing2021">[6, 7, 8]</a>, prestige, and computational resources naturally relies on <em>displacing</em> the possibility of alternative infrastructure. Our predicament is doubly difficult: we both have digital infrastructural deficits, but are also being actively <em>deinfrastructured.</em> The harms of deinfrastructuring are bidirectional, comprising both the missed opportunities from decades of free knowledge exchange, and the impacts of the informational regime that exists in its place. One can only imagine what the state of science and medicine might be if NIH’s 1999 push to displace for-profit journals<a class="citation" href="#robertsBuildingGenBankPublished2001">[9, 10, 11, 12]</a> had succeeded and we had more than 20 years of infrastructural development built atop a fundamentally free system of scientific knowledge. Instead, our failure to seize the digital infrastructure of science has led to a system where what should be our shared intellectual heritage is yoked to the profit engine of surveillance conglomerates (formerly known as publishers) <a class="citation" href="#pooleySurveillancePublishing2021">[6, 13]</a> that repackage it along with a deluge of mined personal data in a circular economy of control <a class="citation" href="#brembsAlgorithmicEmploymentDecisions2021">[14, 15, 16]</a> that makes us directly complicit in the worst abuses of informational capitalism <a class="citation" href="#biddleICESearchedLexisNexis2022">[17, 18, 19, 20, 21]</a>.</p>

<p>We need to move beyond conceptualizing the problems of scientific infrastructure as being unique to science, a sighing hope for some future that “might be nice” to have (built by an always-anonymous “<em>someone else</em>”), but one to be pursued gradually after staid and cautious scholars are convinced no risk will come to our precious systems of prestige and peer review. We need to start seeing ours as one of many stories in the digital enclosure movement where adversarial economic entities take ownership of basic digital infrastructure and wipe out a domain of knowledge work, reducing it to a captive market and content farm <a class="citation" href="#warkCapitalDeadThis2021">[8, 22]</a>. We need to see taking control of our digital infrastructure as <em>essential</em> to the continued existence of science as we know it.</p>

<p>This paper is an argument that <strong>decentralized</strong> digital infrastructure<sup id="fnref:nocrypto" role="doc-noteref"><a href="#fn:nocrypto" class="footnote" rel="footnote">3</a></sup> is the best means of alleviating the harms of infrastructural deficits and building a digital landscape that supports, rather than extracts from science. I will draw from several disciplines and knowledge communities, across and outside academia to articulate a vision of an infrastructure in three parts: <strong>shared data, shared tools, and shared knowledge.</strong> These domains reflect three of the dominant modes of digital enclosure prerequisite for platform capture: <strong>storage, computation, and communication.</strong> The systems we will describe are in conversation with and a continuation of a long history of reimagining the relationship between these domains for a healthier web (see eg. <a class="citation" href="#berners-leeSociallyAwareCloud2009">[23, 24]</a>). We depart from it to describe a system of fluid, peer-to-peer social affiliation and <a href="#federated-systems-of-language">folksonomic</a> linked data with lessons primarily from <a href="#the-wiki-way">early wikis and Wikipedia</a>, the fissures of the <a href="#neatness-vs-scruffiness">semantic web and linked data</a> communities, the social structure of <a href="#archives-need-communities">private bittorrent trackers</a>, and the federation system of <a href="#forums--feeds">ActivityPub and the Fediverse</a>. Approaching this problem from science has its constraints — like the structuring need to rebuild systems of <a href="#credit-assignment">credit assignment</a> — as well as the powerful opportunity of one of the last systems of labor largely not driven by profit developing technology and seeding communities that could begin to directly address the dire, societywide need for digital freedom.</p>

<p>The problems we face are different than they were at the dawn of the internet, but we can learn from its history: we shouldn’t be waiting for a new journal-like <strong>platform,</strong> software package, or subscription to save us. We need to build <strong>protocols</strong> for communication, interoperability, and self-governance (see, recently <a class="citation" href="#brembsReplacingAcademicJournals2021">[25]</a>).</p>

<p>I will start with a brief description of what I understand to be the state of our digital infrastructure and the structural barriers and incentives that constrain its development. I will then propose a set of design principles for decentralized infrastructure and possible means of implementing it informed by prior successes and failures at building mass digital infrastructure. I will close with contrasting visions of what science could be like depending on the course of our infrastructuring, and my thoughts on how different actors in the scientific system can contribute to and benefit from decentralization.</p>

<p>I insist that what I will describe is <em>not utopian</em> but is eminently practical — the truly impractical choice is to do nothing and continue to rest the practice of science on a pyramid scheme <a class="citation" href="#ponziSciencePyramidScheme2020">[26]</a> of underpaid labor. With a bit of development to integrate and improve the tools, <strong>every class of technology I propose here already exists and is widely used.</strong> A central principle of decentralized systems is embracing heterogeneity: harnessing the power of the diverse ways we do science instead of constraining them. Rather than a patronizing argument that everyone needs to fundamentally alter the way they do science, the systems that I describe are specifically designed to be easily incorporated into existing practices and adapted to variable needs. In this way I argue decentralized systems are <em>more practical</em> than the dream that any one system will be capable of expanding to the scale of all science — and as will hopefully become clear, inarguably <em>more powerful</em> than a disconnected sea of centralized platforms and services.</p>

<p>An easy and common misstep is to categorize this as solely a <em>technical</em> challenge. Instead the challenge of infrastructure is also <em>social</em> and <em>cultural</em> — it involves embedding any technology in a set of social practices, a shared belief that such technology should exist, that its form is not neutral, and a sense of communal valuation and purpose that sustains it <a class="citation" href="#bietzSustainingDevelopmentCyberinfrastructure2012">[27]</a>.</p>

<p>The social and technical perspectives are both essential, but make some conflicting demands on the construction of the piece: Infrastructuring requires considering the interrelatedness and mutual reinforcement of the problems to be addressed, rather than treating them as isolated problems that can be addressed piecemeal with a new package or by founding a new journal alternative. Such a broad scope trades off with a detailed description of the relevant technology and systems, but a myopic techno-zealotry that does not examine the social and ethical nature of scientific practice risks reproducing or creating new sources of harm. That, and techno-solutionism never <em>works</em> anyway. As a balance I will not be proposing a complete technical specification or protocol, but describing the general form of the tools and some existing examples that satisfy them; I will not attempt a full history or treatment of the problem of infrastructuring, but provide enough to motivate the form of the proposed implementations.</p>

<p>My understanding of this problem is, of course, uncorrectably structured by my training largely centered in systems neuroscience and my position as an early career researcher (ECR). While the core of my argument is intended to be a sketch compatible with sciences and knowledge systems generally, my examples will sample from, and my focus will skew to my experience. In many cases, my use of “science” or “scientist” could be “neuroscience” or “neuroscientist,” but I will mostly use the former to avoid the constant context switches. This document is also an experiment in public collaboration on a living scientific document: to try and ease our way out of disciplinary tunnelvision, we invite annotation and contribution with no lower bound — if you’d like to add or correct a sentence or two (or a page or ten), you’re welcome as coauthor. I ask the reader for a measure of patience for the many ways this argument requires elaboration and modification for distant fields.</p>

<h1 id="the-state-of-things">The State of Things</h1>

<h2 id="the-costs-of-infrastructure-deficits">The Costs of Infrastructure Deficits</h2>

<p>A diagnosis of digital infrastructure deficits gives a common framework to consider many technical and social harms in scientific work that are typically treated separately, and allows us to problematize other symptoms have become embedded as norms.</p>

<p>I will list some of the present costs to give a sense of the scale of need, as well as scope for the problems we intend to address here. These lists are grouped into rough and overlapping categories, but make no pretense at completeness.</p>

<p>Impacts on the <strong>daily experience</strong> of researchers include:</p>

<ul>
  <li>A prodigious duplication and dead-weight loss of labor as each lab, and sometimes each person within each lab, will reinvent basic code, tools, and practices from scratch. Literally it is the inefficiency of the <a href="https://en.wikipedia.org/wiki/Deadweight_loss#Harberger's_triangle">Harberger’s triangle</a> in the supply and demand system for scientific infrastructure caused by inadequate supply. Labs with enough resources are forced to pay from other parts of their grants to hire professional programmers and engineers to build the infrastructure for their lab<sup id="fnref:usuallyjusttheirs" role="doc-noteref"><a href="#fn:usuallyjusttheirs" class="footnote" rel="footnote">4</a></sup>, but most just operate on a purely amateur basis. Many PhD students will spend the first several years of their degree re-solving already-solved problems, chasing the tails of the wrong half-readable engineering whitepapers, in their 6th year finally discovering the technique that they actually needed all along. That’s not an educational or training model, it’s the effect of displacing the undone labor of unbuilt infrastructure on vulnerable graduate workers almost always paid poverty wages.</li>
  <li>At least the partial cause of the phenomenon where “every scientist needs to be a programmer now” as people who aren’t particularly interested in being programmers — which is <em>fine</em> and <em>normal</em> — need to either suffer through code written by some other unlucky amateur or learn several additional disciplines in order to do the work of the one they chose.</li>
  <li>A great deal of pain and alienation for early- career researchers not previously trained in programming before being thrown in the deep end. Learning data hygeine practices like backup, annotation, etc. “the hard way” through some catastrophic loss is accepted myth in much of science. At some scale all the very real and widespread pain, guilt, and shame felt by people who had little choice but to reinvent their own data management system must be recognized as an infrastructural, rather than a personal problem.</li>
  <li>The high cost of “openness” and the dearth of mass-scale collaboration. It is still rare to publish full, raw data and analysis code, often because the labor of cleaning it is too great. We can’t expect openness from everyone while it is still so <em>hard.</em> The “Open science” movement, roughly construed, has reached a few hard limits from present infrastructure that have forced its energy to leak from the sides as bullying leaderboards or sets of symbols that are mere signifiers of cultural affiliation to openness. “Openness” is not a uniform or universal goal for all science, and even the framing of openness as inspection of results collected and analyzed in private isolation illustrates how infrastructural deficts bound our imagination. Our dreams can be bigger than being able to police each other’s data, towards a more continuously collaborative process that renders the need for post-hoc openness irrelevant with mutually beneficial information sharing baked into every stage.</li>
</ul>

<p>Impacts on the <strong>system of scientific inquiry</strong> include:</p>

<ul>
  <li>A profoundly leaky knowledge acquisition system where entire PhDs worth of data can be lost and rendered useless when a student leaves a lab and no one remembers how to access the data or how it’s formatted.</li>
  <li>The inevitability of continual replication crises because it is often literally impossible to replicate an experiment that is done on a rig that was built one time, used entirely in-lab code, and was never documented</li>
  <li>Reliance on communication platforms and knowledge systems that aren’t designed to, and don’t come close to satisfying the needs of scientific communication. In the absence of some generalized means of knowledge organization, scientists ask the void<sup id="fnref:whichvoid" role="doc-noteref"><a href="#fn:whichvoid" class="footnote" rel="footnote">5</a></sup> for advice or guidance from anyone that algorithmically stumbles by. Often our best recourse is to make a Slack about it, which is incapable of producing a public, durable, and cumulative resource: and so the same questions will be asked again… and again…</li>
  <li>A perhaps doomed intellectual endeavor<sup id="fnref:solaris" role="doc-noteref"><a href="#fn:solaris" class="footnote" rel="footnote">6</a></sup> as we attempt to understand the staggering complexity of the brain by peering at it through the camera obscura of just the most recent data you or your lab have collected rather than being able to index across the many measurements of the same phenomena. The unnecessary reduplication of experiments becomes not just a methodological limitation, but an ethical catastrophe as researchers have little choice but to abandon the elemental principle of sacrificing as few animals as possible.</li>
  <li>A near-absence of semantic or topical organization of research that makes cumulative progress in science proabilistic at best, and subject to the malformed incentives of publication and prestige gathering at worst. Since engaging with prior literature is a matter of manually reconstructing a caricature of a field of work in every introduction, continuing lines of inquiry or responding to conflicting results is <em>strictly optional.</em></li>
  <li>A hierarchy of prestige that devalues the labor of many groups of technicians, animal care workers, and so on. Authorship is the coin of the realm, but many workers that are fundamental to the operation of science only receive the credit of an acknowledgement. We need a system to value and assign credit for the immense amount of technical and practical knowledge and labor they contribute.</li>
</ul>

<p>Impacts on the relationship between <strong>science and society</strong>:</p>

<ul>
  <li>An insular system where the inaccessibility of all the “contextual” knowledge <a class="citation" href="#woolKnowledgeNetworksHow2020">[28, 29]</a> that doesn’t have a venue for sharing but is necessary to perform experiments, like “how to build this apparatus,” “what kind of motor would work here,” etc. is a force that favors established and well-funded labs who can rely on local knowledege and hiring engineers/etc. and excludes new, lesser-funded labs at non-ivy institutions. The concentration of technical knowledge magnifies the inequity of strongly skewed funding distributions such that the most well-funded labs can do a completely different kind of science than the rest of us, turning the positive-feedback loop of funding begetting funding ever faster.</li>
  <li>An absconscion with the public resources we are privileged enough to receive, where rather than returning the fruits of the many technical challenges we are tasked with solving to the public in the form of data, tools, collected practical knowledge, etc. we largely return papers. Since those papers are often impenetrable outside of their discipline or paywalled outside of academia, we multiply the above impacts of labor duplication and knowledge inaccessibility by the scale of society.</li>
  <li>The complicity of scientists in rendering our collective intellectual heritage nothing more than another regiment in the ever-advancing armies of <a href="#platforms-industry-capture-and-the-profit-motive">platform capitalism</a>. If our highest aspirations are to shunt all our experiments, data, and analysis tools onto Amazon Web Services, our failure of imagination will be responsible for yet another obligate funnel of wealth into the system of extractive platforms that dominate the flow of global information. For ourselves, we stand to have the practice of science filleted at the seams into a series of mutually incompatible subscription services. For society, we squander the chance for one of the very few domains of non-economic labor to build systems to recollectivize the basic infrastrucutre of the internet: rather than providing an alternative to the information overlords and their digital enclosure movement, we will be run right into their arms.</li>
</ul>

<p>Considered separately, these are serious problems, but together they are a damning indictment of our role as stewards of our corner of the human knowledge project.</p>

<p>We arrive at this situation not because scientists are lazy and incompetent, but because we are embedded in a system of mutually reinforcing disincentives to cumulative infrastructure development. Our incentive systems are, in turn, coproductive with a raft of economically powerful entities that would really prefer owning it all themselves, thanks. Put bluntly, “we are dealing with a massively entrenched set of institutions, built around the last information age and fighting for its life” <a class="citation" href="#bowkerInformationInfrastructureStudies2010">[1]</a></p>

<p>There is, of course, an enormous amount of work being done by researchers and engineers on all of these problems, and a huge amount of progress has been made on them. My intention is not to shame or devalue anyone’s work, but to try and describe a path towards integrating it and making it mutually reinforcing.</p>

<p>Before proposing a potential solution to some of the above problems, it is important to motivate why they haven’t already been solved, or why their solution is not necessarily imminent. To do that, we need a sense of the social and technical challenges that structure the development of our tools.</p>

<h2 id="misincentives-in-scientific-software">(Mis)incentives in Scientific Software</h2>

<p>The incentive systems in science are complex, subject to infinite variation everywhere, so these are intended as general tendencies rather than statements of irrevocable and uniform truth.</p>

<h3 id="incentivized-fragmentation">Incentivized Fragmentation</h3>

<p>Scientific software development favors the production of many isolated, single-purpose software packages rather than cumulative work on shared infrastructure. The primary means of evaluation for a scientist is academic reputation, primarily operationalized by publications, but a software project will yield a single paper (if any). Traditional publications are static units of work that are “finished” and frozen in time, but software is never finished: the thousands of commits needed to maintain and extend the software are formally not a part of the system of academic reputation.</p>

<p>Howison &amp; Herbsleb described this dynamic in the context of BLAST<sup id="fnref:whatisblast" role="doc-noteref"><a href="#fn:whatisblast" class="footnote" rel="footnote">7</a></sup></p>

<blockquote>
  <p>In essence we found that BLAST innovations from those motivated to improve BLAST by academic reputation are motivated to develop and to reveal, but not to integrate their contributions. Either integration is actively avoided to maintain a separate academic reputation or it is highly conditioned on whether or not publications on which they are authors will receive visibility and citation. <a class="citation" href="#howisonIncentivesIntegrationScientific2013">[30]</a></p>
</blockquote>

<p>For an example in Neuroscience, one can browse the papers that cite the DeepLabCut paper <a class="citation" href="#mathisDeepLabCutMarkerlessPose2018">[31]</a> to find hundreds of downstream projects that make various extensions and improvements that are not integrated into the main library. While the alternative extreme of a single monolithic ur-library is also undesirable, working in fragmented islands makes infrastructure a random walk instead of a cumulative effort.</p>

<p>After publication, scientists have little incentive to <strong>maintain</strong> software outside of the domains in which the primary contributors use it, so outside of the most-used libraries most scientific software is brittle and difficult to use <a class="citation" href="#carverSurveyStatePractice2022">[32, 33, 34]</a>.</p>

<p>Since the reputational value of a publication depends on its placement within a journal and number of citations (among other metrics), citation practices for scientific software are far from uniform and universal, and relatively few “prestige” journals publish software papers at all, the incentive to write scientific software in the first place is low compared to its near-universal use <a class="citation" href="#howisonSoftwareScientificLiterature2016">[35]</a>.</p>

<h3 id="domain-specific-silos">Domain-Specific Silos</h3>

<p>When funding exists for scientific infrastructure development, it typically comes in the form of side effects from, or administrative supplements to research grants. The NIH describes as much in their Strategic Plan for Data Science <a class="citation" href="#NIHStrategicPlan2018">[36]</a>:</p>

<blockquote>
  <p>from 2007 to 2016, NIH ICs used dozens of different funding strategies to support data 
resources, most of them linked to research-grant mechanisms that prioritized innovation and hypothesis testing over user service, utility, access, or efficiency. In addition, although the need for open and efficient data sharing is clear, where to store and access datasets generated by individual laboratories—and how to make them compliant with FAIR principles—is not yet straightforward. Overall, it is critical that the data-resource ecosystem become seamlessly integrated such that different data types and information about different organisms or diseases can be used easily together rather than existing in separate data “silos” with only local utility.</p>
</blockquote>

<p>The National Library of Medicine within the NIH currently lists 122 separate databases in its <a href="https://eresources.nlm.nih.gov/nlm_eresources/">search tool</a>, each serving a specific type of data for a specific research community. Though their current funding priorities signal a shift away from domain-specific tools, the rest of the scientific software system consists primarily of tools and data formats purpose-built for a relatively circumscribed group of scientists. Every field has its own challenges and needs for software tools, but there is little incentive to build tools that serve as generalized frameworks to integrate them.</p>

<h3 id="the-long-now-of-immediacy-vs-idealism">“The Long Now” of Immediacy vs. Idealism</h3>

<p>Digital infrastructure development takes place at multiple timescales simultaneously — from the momentary work of implementing it; through longer timescales of planning, organization, and documenting; to the imagined indefinite future of its use — what Ribes and Finholt call “The Long Now. <a class="citation" href="#ribesLongNowTechnology2009">[37]</a>” Infrastructural projects constitutively need to contend with the need for immediately useful results vs. general and robust systems; the need to involve the effort of skilled workers vs. the uncertainty of future support; the  balance between stability and mutability; and so on. The tension between hacking something together vs. building something sustainable for future use is well-trod territory in the hot-glue and exposed wiring of systems neuroscience rigs.</p>

<p>Deinfrastructuring divides the incentives and interests of junior and senior researchers. ECRs might be interested in developing tools they’ll use throughout their careers, but given the pressure to establish their reputation with publications rarely have the time to develop something fully. The time pressure never ends, and established researchers also need to push enough publications through the door to be able to secure the next round of funding. The time preference of scientific software development is thus very short: hack it together, get the paper out, we’ll fix it later.</p>

<p>The constant need to produce software that <em>does something</em> in the context of scientific programming which largely lacks the institutional systems and expert mentorship needed for well-architected software means that most programmers <em>never</em> have a chance to learn best practices commonly accepted in software engineering. As a consequence, a lot of software tools are developed by near-amateurs with no formal software training, contributing to their brittleness <a class="citation" href="#altschulAnatomySuccessfulComputational2013">[38]</a>.</p>

<p>The problem of time horizon in development is not purely a product of inexperience, and a longer time horizon is not uniformly better. We can look to the history of the semantic web, a project that was intended to bridge human and computer-readable content on the web, for cautionary tales. In the semantic web era, thousands of some of the most gifted programmers and some of the original architects of the internet worked with an eye to the indefinite future, but the raw idealism and neglect of the pragmatic reality of the need for software to <em>do something</em> drove many to abandon the effort (bold is mine, italics in original):</p>

<blockquote>
  <p><strong>But there was no <em>use</em> of it.</strong> I wasn’t using any of the technologies for anything, except for things related to the technology itself. The Semantic Web is utterly inbred in that respect. The problem is in the model, that we create this metaformat, RDF, and <em>then</em> the use cases will come. But they haven’t, and they won’t. Even the genealogy use case turned out to be based on a fallacy. The very few use cases that there are, such as Dan Connolly’s hAudio export process, don’t justify hundreds of eminent computer scientists cranking out specification after specification and API after API.</p>

  <p>When we discussed this on the Semantic Web Interest Group, the conversation kept turning to how the formats could be fixed to make the use cases that I outlined happen. “Yeah, Sean’s right, let’s fix our languages!” But <strong>it’s not the languages which are broken,</strong> except in as much as they are entirely broken: because <strong>it’s the <em>mentality</em> of their design which is broken.</strong> You can’t, it has turned out, make a metalanguage like RDF and then go looking for use cases. We thought you could, but you can’t. It’s taken eight years to realise.
<a class="citation" href="#palmerDitchingSemanticWeb2008">[39]</a></p>
</blockquote>

<p>Developing digital infrastructure must be both bound to fulfilling immediate, incremental needs as well as guided by a long-range vision. The technical and social lessons run in parallel: We need software that solves problems people actually have, but can flexibly support an eventual form that allows new possibilities. We need a long-range vision to know what kind of tools we should build and which we shouldn’t, and we need to keep it in a tight loop with the always-changing needs of the people it supports.</p>

<p>In short, to develop digital infrastructure we need to be <em>strategic.</em> To be strategic we need a <em>plan.</em> To have a plan we need to value planning as <em>work.</em> On the valuation of this kind of work, Ribes and Finholt are instructive:</p>

<blockquote>
  <p>“On the one hand, I know we have to keep it all running, but on the other, LTER is about long-term data archiving. If we want to do that, we have to have the time to test and enact new approaches. But if we’re working on the to-do lists, we aren’t working on the tomorrow-list” (LTER workgroup discussion 10/05).</p>

  <p>The tension described here involves not only time management, but also the differing valuations placed on these kinds of work. The implicit hierarchy places scientific research first, followed by deployment of new analytic tools and resources, and trailed by maintenance work. […] While in an ideal situation development could be tied to everyday maintenance, in practice, maintenance work is often invisible and undervalued. As Star notes, infrastructure becomes visible upon breakdown, and only then is attention directed at its everyday workings (1999). Scientists are said to be rewarded for producing new knowledge, developers for successfully implementing a novel technology, but the work of maintenance (while crucial) is often thankless, of low status, and difficult to track. <em>How can projects support the distribution of work across research, development, and maintenance?</em> <a class="citation" href="#ribesLongNowTechnology2009">[37]</a></p>
</blockquote>

<h3 id="neatness-vs-scruffiness">“Neatness” vs “Scruffiness”</h3>

<p>Closely related to the tension between “Now” and “Later” is the tension between “Neatness” and “Scruffiness.” Lindsay Poirier traces its reflection in the semantic web community as the way that differences in “thought styles” result in different “design logics”  <a class="citation" href="#poirierTurnScruffyEthnographic2017">[40]</a>. On the question of how to develop technology for representing the ontology of the web – the system of terminology and structures with which everything should be named – there were (very roughly) two camps. The “neats” prioritized consistency, predictability, uniformity, and coherence – a logically complete and formally valid System of Everything. The “scruffies” prioritized local systems of knowledge, expressivity, “believing that ontologies will evolve organically as everyday webmasters figure out what schemas they need to describe and link their data. <a class="citation" href="#poirierTurnScruffyEthnographic2017">[40]</a>”</p>

<p>This tension is as old as the internet, where amidst the <a href="https://en.wikipedia.org/wiki/Dot-com_bubble">dot-com bubble</a> a telecom spokesperson lamented that the internet wasn’t controllable enough to be profitable because “it was devised by a bunch of hippie anarchists.” <a class="citation" href="#hiltzikTamingWildWild2001">[41]</a> The hippie anarchists probably agreed, famously rejecting “kings, presidents and voting” in favor of “rough consensus and running code” during an attempted ISO coup to replace TCP/IP with a <a href="https://www.iso.org/standard/35872.html">proprietary protocol</a>. Clearly, the difference in thought styles has an unsubtle relationship with beliefs about who should be able to exercise power and what ends a system should serve <a class="citation" href="#larsenPoliticalNatureTCP2012">[42]</a>.</p>

<p><img src="/infrastructure/assets/images/clark-slide.png" alt="Black and white slide with title &quot;The last force on us - us,&quot; page 551 in https://www.ietf.org/proceedings/24.pdf full text: &quot;The standards elephant  of  yesterday  - OSi. The standards elephant of today - its right here. As the Internet and its community grows, how do wemanage the process of change and  growth? ̄Open process - let all voices  be heard. ̄Closed  process - make progress. ̄Quick process - keep up with reality. Slow process - leave  time to think Market driven process - future is commercial. Scaling driven  process - the  future is the internet. We reject: kings, presidents and voting.We believe in: rough consensus and running code.&quot;" />
<em>A slide from David Clark’s 1992 “Views of the Future”<a class="citation" href="#clarkCloudyCrystalBall1992">[43]</a> that contrasts differing visions for the development process of the future of the internet. The struggle between engineered order and wild untamedness is summarized forcefully as “We reject: kings, presidents and voting. We believe in: rough consensus and running code”</em></p>

<p>Practically, the differences between these thought communities impact the tools they build. Aaron Swartz put the approach of the “neat” semantic web architects the way he did:</p>

<blockquote>
  <p>Instead of the “let’s just build something that works” attitude that made the Web (and the Internet) such a roaring success, they brought the formalizing mindset of mathematicians and the institutional structures of academics and defense contractors. They formed committees to form working groups to write drafts of ontologies that carefully listed (in 100-page Word documents) all possible things in the universe and the various properties they could have, and they spent hours in Talmudic debates over whether a washing machine was a kitchen appliance or a household cleaning device.</p>

  <p>With them has come academic research and government grants and corporate R&amp;D and the whole apparatus of people and institutions that scream “pipedream.” And instead of spending time building things, they’ve convinced people interested in these ideas that the first thing we need to do is write standards. (To engineers, this is absurd from the start—standards are things you write after you’ve got something working, not before!) <a class="citation" href="#swartzAaronSwartzProgrammable2013">[44]</a></p>
</blockquote>

<p>The outcomes of this cultural rift are subtle, but the broad strokes are clear: the “scruffies” largely diverged into the linked data community, which has taken some of the core semantic web technology like RDF, OWL, and the like, and developed a broad range of downstream technologies that have found purchase across information sciences, library sciences, and other applied domains<sup id="fnref:andgoogle" role="doc-noteref"><a href="#fn:andgoogle" class="footnote" rel="footnote">8</a></sup>. The linked data developers, starting by acknowledging that no one system can possibly capture everything, build tools that allow expression of local systems of meaning with the expectation and affordances for linking data between these systems as an ongoing social process.</p>

<p>The vision of a totalizing and logically consistent semantic web, however, has largely faded into obscurity. One developer involved with semantic web technologies (who requested not be named), captured the present situation in their description of a still-active developer mailing list:</p>

<blockquote>
  <p>I think that some people are completely detached from practical applications of what they propose. […] I could not follow half of the messages. these guys seem completely removed from our plane of existence and I have no clue what they are trying to solve.</p>
</blockquote>

<p>This division in thought styles generalizes across domains of infrastructure, though outside of the linked data and similar worlds the dichotomy is more frequently between “neatness” and “people doing whatever” – with integration and interoperability becoming nearly synonymous with standardization. Calls for standardization without careful consideration and incorporation of existing practice have a familiar cycle: devise a standard that will solve everything, implement it, wonder why people aren’t using it, funding and energy dissipiates, rinse, repeat<sup id="fnref:xkcd" role="doc-noteref"><a href="#fn:xkcd" class="footnote" rel="footnote">9</a></sup>. The difficulty of scaling an exacting vision of how data should be formatted, the tools researchers should use for their experiments, and so on is that they require dramatic and sometimes total changes to the way people do science. The alternative is not between standardization and chaos, but a potential third way is designing infrastructures that allow the diversity of approaches, tools, and techniques to be expressed in a common framework or protocol along with the community infrastructure to allow the continual negotiation of their relationship.</p>

<h3 id="taped-on-interfaces-open-loop-user-testing">Taped-on Interfaces: Open-Loop User Testing</h3>

<p>The point of most active competition in many domains of commercial software is the user interface and experience (UI/UX). To compete, software companies will exhaustively user-test and refine them with pixel precision to avoid any potential customer feeling even a thimbleful of frustration. Scientific software development is largely disconnected from usability testing, as what little support exists is rarely tied to it. This, combined with the preponderance of semi-amateurs and above incentives for developing new packages – and thus reduplicating the work of interface development – make it perhaps unsurprising that most scientific software is hard to use!</p>

<p>I intend the notion of “interface” in an expansive way: In addition to a graphical user interface (GUI) or set of functions and calling conventions exposed to the end-user, I am referring generally to all points of contact with users, developers, and other software. Interfaces are intrinsically social, and include the surrounding documentation and experience of use — part of using software is being able to figure out how to use it! The favored design idiom of scientific software is the black box: I implemented an algorithm of some kind, here are the two or three functions needed to use it, but beneath the surface there be dragons.</p>

<p>Ideally, software would be designed with developer interfaces and documentation at multiple scales of complexity to enable clean entrypoints for developers with differing levels of skill and investment to contribute. When this kind of design and documentation is underdeveloped, even widely used projects with excellent top-level interfaces like <a href="https://python-poetry.org/">poetry</a> struggle to respond to the pile of issues <a href="https://github.com/python-poetry/poetry/issues">thousands deep</a> as even users who have spent time reading the source have difficulty understanding what exactly needs to be fixed and maintainers have to spend their time triaging them and manually re-explaining the software hundreds of times<sup id="fnref:poetryexample" role="doc-noteref"><a href="#fn:poetryexample" class="footnote" rel="footnote">10</a></sup>.</p>

<p>Additionally, it would include interfaces for use and integration with other software — or APIs. While the term “API” most commonly refers to <a href="https://en.wikipedia.org/wiki/Web_API">web APIs</a>, the term generally refers to the means by which other programs can interact with a given program. All programs have some limit to their function, the question is how other programs are expected to handle them. One particularly successful approach to program interface design is the Unix philosophy as articulated by Doug McIlroy and colleagues <a class="citation" href="#mcilroyUNIXTimeSharingSystem1978">[45]</a> — which was originally designed to help build research software. Its first “make each program do one thing well” and second “expect the output of every program to become the input to another, as yet unknown, program” principles inspired a set of simple tools that can be composed together for complex tasks. When a program is monolithic and isn’t designed to provide access to its component parts, it becomes difficult to reuse in downstream projects, potentially reskin with a more friendly user interface, and ultimately more likely to be a dead-end in a system of shared infrastructure.</p>

<p>Without care given to any of these types of interfaces, the barrier to use is likely to remain high, the community of co-developers is likely to remain small, and the labor they expend is less likely to be useful outside that single project. This, in turn, closes the loop with incentives to develop new packages and makes another vicious cycle reinforcing fragmentation<sup id="fnref:uxloop" role="doc-noteref"><a href="#fn:uxloop" class="footnote" rel="footnote">11</a></sup>.</p>

<h3 id="platforms-industry-capture-and-the-profit-motive">Platforms, Industry Capture, and the Profit Motive</h3>

<p>Publicly funded science is an always-irresistable golden goose for private industry. The fragmented interests of scientists and the historically light touch of funding agencies on encroaching privatization means that if some company manages to capture and privatize a corner of scientific practice they are likely to keep it. Industry capture has been thoroughly criticized in the context of the journal system (eg. recently, <a class="citation" href="#brembsReplacingAcademicJournals2021">[25]</a>), and that criticism should extend to the rest of our infrastructure as information companies seek to build a for-profit platform system that spans the scientific workflow (eg. <a class="citation" href="#ElsevierSevenBridges2017">[46]</a>). The mode of privatization of scientific infrastructure follows the broader software market as a proliferation of software as a service (SaaS), from startups to international megacorporations, that rent access to some, typically proprietary software without selling the software itself.</p>

<p>While in isolation SaaS can make individual components of the infrastructural landscape easier to access — and even free!!* — the business model is fundamentally incompatible with integrated and accessible infrastructure. The SaaS model derives revenue from subscription or use costs, often operating as “freemium” models that make some subset of its services available for free. Even in freemium models, though, the business model requires that some functionality of the platform is enclosed and proprietary. To keep the particular domain of enclosure viable as a profit stream, the proprietor needs to actively defend against competitors as well as any technology that might fill the need for the proprietary technology<sup id="fnref:googleantitrust" role="doc-noteref"><a href="#fn:googleantitrust" class="footnote" rel="footnote">12</a></sup> (See a more thorough treatment of platform capitalism in science in <a class="citation" href="#mirowskiFutureOpenScience2018">[5]</a>)</p>

<p>As isolated services, one can imagine the practice of science devolving along a similar path as the increasingly-fragmented streaming video market: to do my work I need to subscribe to a data storage service, a cloud computing service, a platform to host my experiments, etc. For larger software platforms, however, vertical integration of multiple complementary services makes their impact on infrastructure more insidious. Locking users into more and more services makes for more and more revenue, which encourages platforms to be as mutually incompatible as they can get away with <a class="citation" href="#macinnesCompatibilityStandardsMonopoly2005">[47]</a>. To encourage adoption, platforms that can offer multiple services may offer one of the services – say, data storage – for free, forcing the user to use the adjoining services – say, a cloud computing platform.</p>

<p>Since these platforms are often subsidiaries of information industry monopolists, scientists become complicit in their often profoundly unethical behavior of by funneling millions of dollars into them. Longterm, unconditional funding of wildly profitable journals has allowed conglomerates like Elsevier to become sprawling surveillance companies <a class="citation" href="#RELXAnnualReport2020">[48, 6]</a> that are sucking as much data up as they can to market derivative products like algorithmic ranking of scientific productivity <a class="citation" href="#brembsAlgorithmicEmploymentDecisions2021">[14]</a> and making data sharing agreements with ICE <a class="citation" href="#biddleLexisNexisProvideGiant2021">[18]</a>. Or our reliance on AWS and the laundry list of human rights abuses by Amazon <a class="citation" href="#CriticismAmazon2021">[49]</a>. In addition to lock-in, dependence on a constellation of SaaS allows the opportunity for platform-holders to take advantage of their limitations and <em>sell us additional services to make up for what the other ones purposely lack</em> — for example Elsevier has taken advantage of our dependence on the journal system and its strategic disorganization to sell a tool for summarizing trending research areas for tailoring maximally-fundable grants <a class="citation" href="#elsevierTopicProminenceSciencea">[50]</a>.</p>

<p>Funding models and incentive structures in science are uniformly aligned towards the platformatization of scientific infrastructure. Aside from the corporate doublespeak “technology transfer” rhetoric that pervades the neoliberal university, the relative absence of major funding opportunities for scientific software developers competitive with the profit potential from “industry” often leaves it as the only viable career path. The preceding structural constraints on local infrastructural development strongly incentivize labs and researchers to rely on SaaS that provides a readymade solution to specific problems. Distressingly, rather than supporting infrastructural development that would avoid obligate payments to platform-holders, funding agencies seem all too happy to lean into them (emphases mine):</p>

<blockquote>
  <p>NIH will <strong>leverage what is available in the private sector,</strong> either through strategic partnerships or procurement, to create a workable <strong>Platform as a Service (PaaS)</strong> environment. […] NIH will partner with cloud-service providers for cloud storage, computational, and related infrastructure services needed to facilitate the deposit, storage, and access to large, high-value NIH datasets. […]</p>

  <p>NIH’s cloud-marketplace initiative will be the first step in a phased operational framework that <strong>establishes a SaaS paradigm for NIH and its stakeholders.</strong> (-NIH Strategic Plan for Data Science, 2018 <a class="citation" href="#NIHStrategicPlan2018">[36]</a>)</p>
</blockquote>

<p>The articulated plan being to pay platform holders to house data while also paying for the labor to maintain those databases veers into parody, haplessly building another triple-pay industry <a class="citation" href="#buranyiStaggeringlyProfitableBusiness2017">[51]</a> into the economic system of science — one can hardly wait until they have the opportunity to rent their own data back with a monthly subscription. This isn’t a metaphor: the STRIDES program, with the official subdomain <a href="https://web.archive.org/web/20210729131920/https://cloud.nih.gov/">cloud.nih.gov</a>, has been authorized to pay $85 million to cloud providers since 2018. In exchange, NIH hasn’t received any sort of new technology, but <a href="https://web.archive.org/web/20211006003547/https://cloud.nih.gov/enrollment/account-type/">“extramural”</a> scientists receive a maximum discount of 25% on cloud storage and “data egress” fees as well as plenty of training on how to give control of the scientific process to platform giants <a class="citation" href="#reillyNIHSTRIDESInitiative2021">[52]</a><sup id="fnref:STRIDESsuccess" role="doc-noteref"><a href="#fn:STRIDESsuccess" class="footnote" rel="footnote">13</a></sup>. Without exaggeration, we are paying them to let us pay for something that makes it so we need to pay them more later.</p>

<p>It is unclear to me whether this is the result of the cultural hegemony of platform capitalism narrowing the space of imaginable infrastructures, industry capture of the decision-making process, or both, but the effect is the same in any case.</p>

<h3 id="protection-of-institutional-and-economic-power">Protection of Institutional and Economic Power</h3>

<p>Aside from information industries, infrastructural deficits are certainly not without beneficiaries within science — those that have already accrued power and status.</p>

<p>Structurally, the adoption of SaaS on a wide scale necessarily sacrifices the goals of an integrated mass infrastructure as the practice of research is carved into small, marketable chunks within vertically integrated technology platforms. Worse, it stands to amplify, rather than reduce, inequities in science, as the labs and institutes that are able to afford the tolls between each of the weigh stations of infrastructure are able to operate more efficiently — one of many positive feedback loops of inequity.</p>

<p>More generally, incentives across infrastructures are often misaligned across strata of power and wealth. Those at the top of a power hierarchy have every incentive to maintain the fragmentation that prevents people from competing — hopefully mostly unconsciously via uncritically participating in the system rather than maliciously reinforcing it.</p>

<p>This poses an organizational problem: the kind of infrastructure that unwinds platform ownership is not only unprofitable, it’s <strong>anti-profitable</strong> – making it impossible to profit from its domain of use. That makes it difficult to rally the kind of development and <a href="https://www.snsi.info/">lobbying</a> resources that profitable technology can, requiring organization based on ethical principles and a commitment to sacrifice control in order to serve a practical need.</p>

<p>The problem is not insurmountable, and there are strategic advantages to decentralized infrastructure and its development within science. Centralized technologies and companies might have more concerted power, but we have <em>numbers</em> and can make tools that let us combine small amounts of labor from many people. We often start (and end) our dreams of infrastructure with the belief that they will necessarily cost a lot of <em>money,</em> but that’s propaganda. Of course development isn’t <em>free,</em> but the cost of decentralized technologies is far smaller than the vast sums of money funnelled into industry profits, labor hours spent compensating for the designed inefficiencies of the platform model, and the development of a fragmented tool ecosystem built around them.</p>

<p>Science, as one of few domains of non-economic labor, has the opportunity to be a seed for decentralized technologies that could broadly improve not only the health of scientific practice, but the broader information ecosystem. We can develop a plan and mobilize to make use of our collective expertise to build tools that have no business model and no means of development in commercial domains — we just need to realize what’s at stake and agree that the health of science is more important than the convenience of the cloud<sup id="fnref:awsdown" role="doc-noteref"><a href="#fn:awsdown" class="footnote" rel="footnote">14</a></sup> or which journal our papers go into.</p>

<h2 id="the-ivies-institutes-and-the-rest-of-us">The Ivies, Institutes, and “The Rest of Us”</h2>

<p>Given these constraints, who can build new digital infrastructure? Constraints, motivations, and strategies all depend on the circumstance of those doing the development. The undone work of infrastructure is being nibbled at around the edges<sup id="fnref:actuallywork" role="doc-noteref"><a href="#fn:actuallywork" class="footnote" rel="footnote">15</a></sup> by several different kinds of organization already ranging in scale and structure. A short survey to give us some notion of how we should seek to organize infrastructure building:</p>

<h3 id="institutional-core-facilities">Institutional Core Facilities</h3>

<p>Centralized “core” facilities are maybe the most typical form of infrastructure development and resource sharing at the level of departments and institutions. These facilities can range from minimal to baroque extravagance depending on institutional resources and whatever complex web of local history brought them about.</p>

<p>A <a href="https://reporter.nih.gov/project-details/9444124#sub-Projects">subproject</a> within a <a href="https://projectreporter.nih.gov/project_info_details.cfm?aid=9444124">PNI Systems Core</a> grant echoes a lot of the thoughts here, particularly regarding effort duplication<sup id="fnref:tymae" role="doc-noteref"><a href="#fn:tymae" class="footnote" rel="footnote">16</a></sup>:</p>

<blockquote>
  <p>Creating an Optical Instrumentation Core will  address the problem that much of the technical work required to innovate and maintain these instruments has  shifted to students and postdocs, because it has exceeded the capacity of existing staff. This division of  labor is a problem for four reasons: (1) lab personnel often do not have sufficient time or expertise to produce  the best possible results, (2) the diffusion of responsibility leads people to duplicate one another’s efforts, (3)  researchers spend their time on technical work at the expense of doing science, and (4) expertise can be lost  as students and postdocs move on. For all these reasons, we propose to standardize this function across  projects to improve quality control and efficiency. Centralizing the design, construction, maintenance, and  support of these instruments will increase the efficiency and rigor of our microscopy experiments, while  freeing lab personnel to focus on designing experiments and collecting data.</p>
</blockquote>

<p>While core facilities are an excellent way of expanding access, reducing redundancy, and standardizing tools <em>within</em> an instutition, as commonly structured they can displace work spent on efforts that would be portable <em>outside</em> of the institution. Elite institutions can attract the researchers with the technical knowledge to develop the instrumentation of the core and infrastructure for maintaining it, but this development is only occasionally made usable by the broader public. The Princeton data science core is an excellent example of a core facility that does makes its software infrastructure development <a href="https://github.com/BrainCOGS">public</a><sup id="fnref:pnidatascience" role="doc-noteref"><a href="#fn:pnidatascience" class="footnote" rel="footnote">17</a></sup>, which they should be applauded for, but also illustrative of the problems with a core-focused infrastructure project. For an external user, the documentation and tutorials are incomplete – it’s not clear to me how one would set this up for my institute, lab, or data, and there are several places of hard-coded princeton-specific values that I am unsure how exactly to adapt<sup id="fnref:pnicaveat" role="doc-noteref"><a href="#fn:pnicaveat" class="footnote" rel="footnote">18</a></sup>. I would consider this example a high-water mark, and the median openness of core infrastructure falls far below it. I was unable to find an example of a core facility that maintained publicly-accessible documentation on the construction and operation of its experimental infrastructure or the management of its facility.</p>

<p>This might be unsurprising given the economic structure of most core facilities: an institution pays for a core to benefit the institution, and downstream public benefits are a nice plus but not high up in the list of concerns (if present at all). Core facilities are thus unlikely to serve as the source of mass infrastructure, but they do serve as a point of local coordination within institutions, and so given some larger means of coordination may still be useful.</p>

<h3 id="centralized-institutes">Centralized Institutes</h3>

<p>Outside of universities, the Allen Brain Institute is perhaps the most impactful reflection of centralization in neuroscience. The Allen Institute has, in an impressively short period of time, created several transformative tools and datasets, including its well-known atlases <a class="citation" href="#leinGenomewideAtlasGene2007">[54]</a> and the first iteration of its <a href="http://observatory.brain-map.org/">Observatory</a> project which makes a massive, high-quality calcium imaging dataset of visual cortical activity available for public use. They also develop and maintain software tools like their <a href="https://allensdk.readthedocs.io/en/latest/">SDK</a> and Brain Modeling Toolkit <a href="https://alleninstitute.github.io/bmtk/">(BMTK)</a>, as well as a collection of <a href="https://portal.brain-map.org/explore/toolkit/hardware">hardware schematics</a> used in their experiments. The contribution of the Allen Institute to basic neuroscientific infrastructure is so great that, anecdotally, when talking about scientific infrastructure it’s not uncommon for me to hear something along the lines of “I thought the Allen was doing that.”</p>

<p>Though the Allen Institute is an excellent model for scale at the level of a single organization, its centralized, hierarchical structure cannot (and does not attempt to) serve as the backbone for all neuroscientific infrastructure. Performing single (or a small number of, as in its also-admirable <a href="https://alleninstitute.org/what-we-do/brain-science/news-press/articles/three-collaborative-studies-launch-openscope-shared-observatory-neuroscience">OpenScope Project</a>) carefully controlled experiments a huge number of times is an important means of studying constrained problems, but is complementary with the diversity of research questions, model organisms, and methods present in the broader neuroscientific community.</p>

<p>Christof Koch, its director, describes the challenge of centrally organizing a large number of researchers:</p>

<blockquote>
  <p>Our biggest institutional challenge is organizational: assembling, managing, enabling and motivating large teams of diverse scientists, engineers and technicians to operate in a highly synergistic manner in pursuit of a few basic science goals <a class="citation" href="#grillnerWorldwideInitiativesAdvance2016">[55]</a></p>

  <p>These challenges grow as the size of the team grows. Our anecdotal evidence suggests that above a hundred members, group cohesion appears to become weaker with the appearance of semi-autonomous cliques and sub-groups. This may relate to the postulated limit on the number of meaningful social interactions humans can sustain given the size of their brain <a class="citation" href="#kochBigScienceTeam2016">[56]</a></p>
</blockquote>

<p>These institutes too are certainly helpful in building core technologies for the field, but they aren’t necessarily organized for developing mass-scale infrastructure. They reflect the capabilities and needs of the institute itself, which are likely to be radically different than a small lab. They can build technologies on a background of expensive cloud storage and computation and rely on a team of engineers to implement and maintain them. So while the tools they make are certainly <em>useful</em> we shouldn’t count on them to build the systems we need for scientists at large.</p>

<h3 id="meso-scale-collaborations">Meso-scale collaborations</h3>

<p>Given the diminishing returns to scale for centralized organizations, many have called for smaller, “meso-scale” collaborations and consortia that combine the efforts of multiple labs <a class="citation" href="#mainenBetterWayCrack2016">[57]</a>. The most successful consortium of this kind has been the International Brain Laboratory <a class="citation" href="#abbottInternationalLaboratorySystems2017">[58, 28]</a>, a group of 22 labs spread across six countries. They have been able to realize the promise of big team neuroscience, setting a new standard for performing reproducible experiments across many labs <a class="citation" href="#laboratoryStandardizedReproducibleMeasurement2020">[59]</a> and developing data management infrastructure to match <a class="citation" href="#laboratoryDataArchitectureLargescale2020">[60]</a><sup id="fnref:dataportal" role="doc-noteref"><a href="#fn:dataportal" class="footnote" rel="footnote">19</a></sup>. Their project thus serves as the benchmark for large-scale collaboration and a model from which all similar efforts should learn from.</p>

<p>Critical to the IBL’s success was its adoption of a flat, non-hierarchical organizational structure, as described by Lauren E. Wool:</p>

<blockquote>
  <p>IBL’s virtual environment has grown to accommodate a diversity of scientific activity, and is supported by a flexible, ‘flattened’ hierarchy that emphasizes horizontal relationships over vertical management. […] Small teams of IBL members collaborate on projects in Working Groups (WGs), which are defined around particular specializations and milestones and coordinated jointly by a chair and associate chair (typically a PI and researcher, respectively). All WG chairs sit on the Executive Board to propagate decisions across WGs, facilitate operational and financial support, and prepare proposals for voting by the General Assembly, which represents all PIs. <a class="citation" href="#woolKnowledgeNetworksHow2020">[28]</a></p>
</blockquote>

<p>They should also be credited with their adoption of a form of consensus decision-making, <a href="https://sociocracy.info">sociocracy</a>, rather than a majority-vote or top-down decisionmaking structure. Consensus decision-making systems are derived from those developed by <a href="https://rhizomenetwork.wordpress.com/2011/06/18/a-brief-history-of-consenus-decision-making/">Quakers and some Native American nations</a>, and emphasize collective consent rather than the will of the majority.</p>

<p>The infrastructure developed by the IBL is impressive, but its focus on a single experiment makes it difficult to expand and translate to widescale use. The hardware for the IBL experimental apparatus is exceptionally well-documented, with a <a href="https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732">complete and detailed build guide</a> and <a href="https://figshare.com/articles/online_resource/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_CAD_files_for_behavior_rig/11639973">library of CAD parts</a>, but the documentation is not modularized such that it might facilitate use in other projects, remixed, or repurposed. The <a href="https://github.com/int-brain-lab/iblrig">experimental software</a> is similarly single-purpose, a chimeric combination of Bonsai <a class="citation" href="#lopesBonsaiEventbasedFramework2015">[61]</a> and <a href="https://github.com/pybpod/pybpod">PyBpod</a> <a href="https://github.com/int-brain-lab/iblrig/tree/master/tasks/_iblrig_tasks_ephysChoiceWorld">scripts</a>. It unfortunately <a href="https://iblrig.readthedocs.io/en/latest/index.html">lacks</a> the API-level documentation that would facilitate use and modification by other developers, so it is unclear to me, for example, how I would use the experimental apparatus in a different task with perhaps slightly different hardware, or how I would then contribute that back to the library. The experimental software, according to the <a href="https://figshare.com/articles/preprint/A_standardized_and_reproducible_method_to_measure_decision-making_in_mice_Appendix_3_IBL_protocol_for_setting_up_the_behavioral_training_rig/11634732">PDF documentation</a>, will also not work without a connection to an <a href="https://github.com/cortex-lab/alyx">alyx</a> database. While alyx was intended for use outside the IBL, it still has <a href="https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/management/commands/files.py#L188">IBL-specific</a> and <a href="https://github.com/cortex-lab/alyx/blob/07f481f6bbde668b81ad2634f4c42df4d6a74e44/alyx/data/fixtures/data.datasettype.json#L29">task-specific</a> values in its source-code, and makes community development difficult with a similar <a href="https://alyx.readthedocs.io/en/latest/">lack</a> of API-level documentation and requirement that users edit the library itself, rather than temporary user files, in order to use it outside the IBL.</p>

<p>My intention is not to denigrate the excellent tools built by the IBL, nor their inspiring realization of meso-scale collaboration, but to illustrate a problem that I see as an extension of that discussed in the context of core facilities — designing infrastructure for one task, or one group in particular makes it much less likely to be portable to other tasks and groups. This argument is much more contingent on the specific circumstances of the consortium than the prior arguments about core facilities and institutes: when organized with mass-infrastructure in mind, collaborations between semi-autonomous groups across institutions could be a powerful mode of tool development.</p>

<p>It is also unclear how replicable these consortia are, and whether they challenge, rather than reinforce technical inequity in science. Participating in consortia systems like the IBL requires that labs have additional funding for labor hours spent on work for the consortium, and in the case of graduate students and postdocs, that time can conflict with work on their degrees or personal research which are still far more potent instruments of “remaining employed in science” than collaboration. In the case that only the most well-funded labs and institutions realize the benefits of big team science without explicit consideration given to scientific equity, mesoscale collaborations could have the unintended consequence of magnifying the skewed distribution of access to technical expertise and instrumentation.</p>

<p>The central lesson of the IBL, in my opinion, is that governance matters. Even if a consortium of labs were to form explicitly to build mass-scale digital infrastructure, without a formal system to ensure contributors felt heard and empowered to shape the project it would soon become unfocused or unsustainable. Even if this system is not perfect, with some labor still falling unequally on some researchers, it is a promising model for future collaborative consortia.</p>

<h3 id="the-rest-of-us">The rest of us…</h3>

<p>Outside of ivies with rich core facilities, institutes like the Allen, or nascent multi-lab consortia, the rest of us are largely on our own, piecing together what we can from proprietary and open source technology. The world of open source scientific software has plenty of energy and lots of excellent work is always being done, though constrained by the circumstances of its development described briefly above. Anything else comes down to whatever we can afford with remaining grant money, scrape together from local knowledge, methods sections, begging, borrowing, and (hopefully not too much) stealing from neighboring labs.</p>

<p>The state of broader scientific deinfrastructuring is perhaps to be expected given our relationship to informational monopolies that in some part depend on it, but unlike many other industries or professions there is reason for hope in science. Science is packed with people with an enormous diversity of skills, resources, and perspectives. Publicly funded science is relatively unique as a labor system that does not strictly depend on profit. There is widespread discontent with the systems of scientific practice, and so the question becomes how we can organize our skill, labor, and energy to rebuild the systems that constrain us.</p>

<p>A third option from the standardization offered by centralization and the blooming, buzzing, beautiful chaos of disconnected open-source development is that of decentralized systems, and with them we might build the means by which the “rest of us” can mutually benefit by organizing our knowledge and labor.</p>

<p>We don’t need to wait for permission from a memo from a funding body or the founding of some new organization. We do have to recognize that while we might have very different roles to play, we are all responsible for the state of digital scientific infrastructure. We should take courage and purpose in knowing that we are not alone, and that our problems are just one reflection of the model of digital enclosure and surveillance that defines the information economy. There is no need for distance or animosity with the other modes of organization described above, as if what we intend to build is truly useful to <em>everyone</em> except those that profit from its absence, then that certainly includes them.  Shunting the vision of a better future onto some as-yet formed effort is precisely the trap we should avoid: our existing organizations <em>should</em> be a part of the work of rebuilding our infrastructure precisely because we should be reconsidering the ways that <em>we, ourselves</em> work. Seeing a subscription to this platform monopolist’s cloud, or that knowledge baron’s prestige hierarchy as not being a value-neutral decision begs an alternative from people, labs, and institutions alike. The diversity in what that means for different groups is a <em>strength,</em> not a weakness, but it does require some shared vision and notion of how to get there. The rest of the paper is an attempt to draft one.</p>

<h1 id="a-draft-of-decentralized-scientific-infrastructure">A Draft of Decentralized Scientific Infrastructure</h1>

<p>What should we build?</p>

<p>The infrastructural systems I will describe here are similar to previous notions of “grass-roots” science articulated within systems neuroscience <a class="citation" href="#mainenBetterWayCrack2016">[57]</a>, “small tech” <a class="citation" href="#balkanSmallTechnologyFoundation">[62]</a> or the anti software software club’s manifesto <a class="citation" href="#kaplanPartAntisoftwareAction2020">[63]</a> in the web development world , and shares some of the motivations of the <a href="https://solidproject.org/">Solid project</a> <a class="citation" href="#sambraSolidPlatformDecentralized2016">[64]</a>, but ultimately draws from a set of ideas with broad and deep history in many domains of computing. My intention is to provide a more prescriptive scaffolding for their design and implementation as a way of painting a picture of what science could be like. This sketch is not intended to be final, but a starting point for further negotiation and refinement.</p>

<p>Throughout this section, when I am referring to any particular piece of software I want to be clear that I don’t intend to be dogmatically advocating that software <em>in particular</em>, but software <em>like it</em> that <em>shares its qualities</em> — no snake oil is sold in this document. Similarly, when I describe limitations of existing tools, without exception I am describing a tool or platform I love, have learned from, and think is valuable — learning from something can mean drawing respectful contrast! Many of these technologies have long and torrid social histories, and so when invoked as examples I don’t necessarily mean to import along with them all the unmentioned baggage that might accompany them<sup id="fnref:permauri" role="doc-noteref"><a href="#fn:permauri" class="footnote" rel="footnote">20</a></sup>.</p>

<h2 id="design-principles">Design Principles</h2>

<p>I won’t attempt to derive a definition of decentralized systems from first principles here, but from the constraints described above, some design principles that illustrate the idea emerge naturally. For the sake of concreteness, in some of these I will draw from the architectural principles of the internet protocols (specifically TCP/IP): the most successful decentralized digital technology project to date.</p>

<h3 id="protocols-not-platforms">Protocols, not Platforms</h3>

<p>Much of the basic technology of the internet was developed as protocols that describe the basic attributes and operations of a process. A simple and common example is email over SMTP (Simple Mail Transfer Protocol) <a class="citation" href="#Rfc5321SimpleMail">[65]</a>. SMTP describes a series of steps that email servers must follow to send a message: the sender initiates a connection to the recipient server, the recipient server acknowledges the connection, a few more handshake steps ensue to describe the senders and receivers of the message, and then the data of the message is transferred. Any software that implements the protocol can send emails to and from any other. The protocol basis of email is the reason why it is possible to send an email from a gmail account to a hotmail account (or any other hacky homebrew SMTP client) despite being wholly different pieces of software.</p>

<p>In contrast, <em>platforms</em> provide some service with a specific body of code usually without any pretense of generality. In contrast to email over SMTP, we have grown accustomed to not being able to send a message to someone using Telegram from WhatsApp, switching between multiple mutually incompatible apps that serve nearly identical purposes. Platforms, despite being <em>theoretically</em> more limited than associated protocols, are attractive for many reasons: they provide funding and administrative agencies a single point of contracting and liability, they typically provide a much more polished user interface, and so on. These benefits are short-lived, however, as the inevitable toll of lock-in and shadowy business models is realized.</p>

<p>By virtue of being intended for use by many independent organizations rather than under the sole control of a platform-holder, protocols are a complicated political effort that embed and facilitate systems of belief and power (see re: TCP/IP <a class="citation" href="#larsenPoliticalNatureTCP2012">[42]</a>, ActivityPub <a class="citation" href="#lemmer-webberStandardsDivisionsCollaboration2018">[66]</a>). For example, in order to arrive at a version of TCP/IP that kept the intermediate relays relatively simple at the expense of reliability, the manufacturer of the “smart” relays had to be excluded from the group. TCP/IP’s success was not inevitable: it was one of several protocols, becoming the default over proprietary competitors from telecommunication and network hardware companies because of some combination of timing, its relative absence of bureaucracy, and institutional adoption (depending on who does the accounting)<a class="citation" href="#larsenPoliticalNatureTCP2012">[42]</a>.</p>

<p>Seemingly prosocial protocols can be used by industries to pre-empt an alternative that would undermine their profit model — a notable example for academics being the DOI system, created in order for publishers to preserve control over their intellectual property <a class="citation" href="#rosenblattDigitalObjectIdentifier1997">[67]</a>. The STM association<sup id="fnref:stm" role="doc-noteref"><a href="#fn:stm" class="footnote" rel="footnote">21</a></sup> hastily<sup id="fnref:hastydoi" role="doc-noteref"><a href="#fn:hastydoi" class="footnote" rel="footnote">22</a></sup> threw its weight behind the DOI-X initiative at its 1999 meeting. The impending creation of PubMed Central by the National Library of Medicine (and see then-NIH Director Harold Varmus’ and others self-described “radical” departure from publishers with what became PLoS <a class="citation" href="#varmusArtPoliticsScience2009">[10, 9]</a>) posed an existential threat to for-profit publishing. At the time there was no unified means of linking to scholarly work<sup id="fnref:urisnew" role="doc-noteref"><a href="#fn:urisnew" class="footnote" rel="footnote">23</a></sup>, and bilateral publisher-publisher linking deals threatened the smooth operation of business, so an NIH-owned platform might have made journals might lose their status as the obligate dissemination platform. According to Bob Campbell, STM chair at the time: “our consensus was that publishers should be the ones doing the linking.” Unlike the anarchic URI/URL, The DOI system requires a registrar (denoted by the prefix before the slash, <code class="language-plaintext highlighter-rouge">doi:10.xxxx/yyyyy</code>) to create DOI names (missing reference). In the US, that means being an institution with an approved <a href="https://www.crossref.org/services/content-registration/">CrossRef membership</a>, which <a href="https://www.crossref.org/membership/terms/">requires</a> members not to link to intellectual property infringing content, and to use DOIs as their default reference links to other works. Effectively, though it is an “open<sup id="fnref:isocostsmoney" role="doc-noteref"><a href="#fn:isocostsmoney" class="footnote" rel="footnote">24</a></sup>” standard, the DOI system ensures that publishers remain in control of what counts as scholarly work <a class="citation" href="#crossrefFormationCrossRefShort2009">[68]</a>.</p>

<p>When approaching protocols, we should do so with humility and caution: work in smaller teams with shared visions with the intention of rough consensus around multiple instances of working code. We should refuse participation by the wide range of industries and interest groups circling each domain of infrastructure, their protocols and standards are siren songs.</p>

<h3 id="integration-not-invention">Integration, not Invention</h3>

<p>At the advent of the internet protocols, several different institutions and universities had already developed existing network infrastructures, and so the “top level goal” of IP was to “develop an effective technique for multiplex utilization of existing interconnected networks,” and “come to grips with the problem of integrating a number of separately administered entities into a common utility” <a class="citation" href="#clarkDesignPhilosophyDARPA1988">[69]</a>. As a result, IP was developed as a ‘common language’ that could be implemented on any hardware, and upon which other, more complex tools could be built. This is also a cultural practice: when the system doesn’t meet some need, one should try to extend it rather than building a new, separate system — and if a new system is needed, it should be interoperable with those that exist.</p>

<p>This point is practical as well as tactical: to compete, an emerging protocol should integrate or be capable of bridging with the technologies that currently fill its role. A new database protocol should be capable of reading and writing existing databases, a new format should be able to ingest and export to existing formats, and so on. The degree to which switching is seamless is the degree to which people will be willing to switch.</p>

<p>This principle runs directly contrary to the current incentives for novelty and fragmentation and the dominant economic model of software platforms, which must be counterbalanced by design choices elsewhere.</p>

<h3 id="embrace-heterogeneity-be-uncoercive">Embrace Heterogeneity, Be Uncoercive</h3>

<p>In addition to integrating with existing systems, it must be straightforward for unanticipated future development to be integrated to accomodate unanticipated needs and practices. This idea is related to “the test of independent invention”, summarized with the question “if someone else had already invented your system, would theirs work with yours?” <a class="citation" href="#berners-leePrinciplesDesign1998">[70]</a>. Rather than attempting to <em>a priori</em> divine a single perfect universal protocol, we should design multiple with extensibility in mind (see this discussion of the extensibility models of ActivityPub to XMPP <a class="citation" href="#schubertActivityPubFinalThoughts2019">[71]</a> and Christopher Yoo’s description of the tradeoffs of the internet’s layered protocols <a class="citation" href="#yooProtocolLayeringInternet2013">[72]</a>) to leave open the opportunity for porting functionality between them.</p>

<p>This principle also has tactical elements. An uncoercive system allows users to gradually adopt it rather than needing to adopt all of its components in order for any one of them to be useful. We shouldn’t rely on potential users making dramatic changes to their existing practices. For example, an experimental framework should not insist on a prescribed set of supported hardware and rigid formulation for describing experiments. Instead it should provide affordances that give a clear way for users to extend the system to fit their needs <a class="citation" href="#carpenterRFC1958Architectural1996">[73]</a>.There always needs to be a <em>benefit</em> to adopting further components of the system to encourage <em>voluntary</em> adoption, but it should never be <em>compulsory.</em> For example, again from experimental frameworks, it should be possible to use it to control experimental hardware without needing to use the rest of the experimental design, data storage, and interface system. To some degree this is accomplished with a modular system design where designers are mindful of keeping the individual modules independently useful.</p>

<p>A noncoercive architecture also prioritizes the ease of leaving. Though this is somewhat tautological to protocol-driven design, specific care must be taken to enable export and migration to new systems. Multiplicity of design and making leaving easy help ensure that early missteps in development of the system are not fatal, preventing lock-in to a component that becomes fixed and stagnant.</p>

<h3 id="empower-people-not-systems">Empower People, not Systems</h3>

<p>Because IP was initially developed as a military technology by DARPA, a primary design constraint was survivability in the face of failure. The model adopted by internet architects was to move as much functionality as possible from the network itself to the end-users of the network — rather than the network itself guaranteeing a packet is transmitted, the sending computer will do so by requiring a response from the recipient <a class="citation" href="#clarkDesignPhilosophyDARPA1988">[69]</a>.</p>

<p>For infrastructure, we should make tools that don’t require a central team of developers to maintain, a central server-farm to host data, or a small group of people to govern. Whenever possible, data, software, and hardware should be self-describing<sup id="fnref:selfdescribing" role="doc-noteref"><a href="#fn:selfdescribing" class="footnote" rel="footnote">25</a></sup>, so one needs minimal additional tools or resources to understand and use it. It should never be the case that funding drying up for one node in the system causes the entire system to fail.</p>

<p>Practically, this means that the tools of digital infrastructure should be deployable by individual people and be capable of recapitulating the function of the system without reference to any central authority. Researchers need to be given control over the function of infrastructure: from controlling sharing permissions for eg. clinically sensitive data to assurance that their tools aren’t spying on them. Formats and standards must be negotiable by the users of a system rather than regulated by a central governance body.</p>

<h3 id="infrastructure-is-social">Infrastructure is Social</h3>

<p>The alternative to centralized governing and development bodies is to build the tools for community control over infrastructural components. This is perhaps the largest missing piece in current scientific tooling. On one side, decentralized governance is the means by which an infrastructure can be maintained to serve the ever-evolving needs of its users. On the other, a sense of community ownership is what drives people to not only adopt but contribute to the development of an infrastructure. In addition to being a source of all the warm fuzzies of socially affiliative “community-ness,” any collaborative system needs a way of ensuring that the practice of maintaining, building, and using it is designed to <em>visibly and tangibly benefit</em> those that do, rather than be relegated to a cabal of invisible developers and maintainers <a class="citation" href="#grudinGroupwareSocialDynamics1994">[74, 75]</a>.</p>

<p>Governance and communication tools also make it possible to realize the infinite variation in application that infrastructures need while keeping them coherent: tools must be built with means of bringing the endless local conversations and modifications of use into a common space where they can become a cumulative sense of shared memory.</p>

<p>I will return to this idea in <a href="#archives-need-communities">Archives Need Communities</a> in the context of social dynamics of private bittorrent trackers, as well as propose a set of basic communication and governance tools in <a href="#rebuilding-scientific-communication">Rebuilding Scientific Communication</a>.</p>

<h3 id="usability-matters">Usability Matters</h3>

<p>It is not enough to build a technically correct technology and assume it will be adopted or even useful, it must be developed embedded within communities of practice and <em>be useful for solving problems that people actually have.</em> We should learn from the struggles of the semantic web project. Rather than building a fully prescriptive and complete system first and deploying it later, we should develop tools whose usability is continuously improved <em>en route</em> to a (flexible) completed vision.</p>

<p>The adage from RFC 1958<sup id="fnref:rfc1958" role="doc-noteref"><a href="#fn:rfc1958" class="footnote" rel="footnote">26</a></sup> “nothing gets standardized until there are multiple instances of running code” <a class="citation" href="#carpenterRFC1958Architectural1996">[73]</a> captures the dual nature of the constraint well. Workable standards don’t emerge until they have been extensively tested in the field, but development without an eye to an eventual protocol won’t make one.</p>

<p>We should read the <a href="https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish">gobbling up</a> of open protocols into proprietary platforms that defined “Web 2.0” as instructive<sup id="fnref:moneyofc" role="doc-noteref"><a href="#fn:moneyofc" class="footnote" rel="footnote">27</a></sup><a class="citation" href="#markoffTomorrowWorldWide1996">[76]</a>. <em>Why</em> did Slack outcompete IRC?<sup id="fnref:slackirc" role="doc-noteref"><a href="#fn:slackirc" class="footnote" rel="footnote">28</a></sup> The answer is relatively simple: it was relatively simple to use. Using a contemporary example, to <a href="https://matrix-org.github.io/synapse/latest/setup/installation.html">set up a Synapse server</a> to communicate over <a href="https://matrix.org/docs/spec/">Matrix</a> one has to wade through dozens of shell commands, system-specific instructions, potential conflicts between dependent packages, set up an SQL server… and that’s just the backend, we don’t even have a frontend client yet! In contrast, to use Slack you download the app, give it your email, and you’re off and running.</p>

<p>The control exerted by centralized systems over their system design does give certain structural advantages to their usability, and their for-profit model gives certain advantages to their development process. There is no reason, however, that decentralized systems <em>must</em> be intrinsically harder to use, we just need to focus on user experience to a degree comparable to centralized platforms: if it takes a college degree to turn the water on, that ain’t infrastructure.</p>

<p>People are smart, they just get frustrated easily and have other things to do on a deadline. We have to raise our standards of design such that we don’t expect users to have even a passing familiarity with programming, attempting to build tools that are truly general use. We can’t just design a peer-to-peer system, we need to make the data ingestion and annotation process automatic, effortless, and expressive. We can’t just build a system for credit assignment, it needs to happen as an automatic byproduct of using the system. We can’t just make tools that <em>work,</em> they need to <em>feel good to use.</em></p>

<p>Centralized systems also have intrinsic limitations that provide openings for decentralized systems, like cost, incompatibility with other systems, restrictions on independent extension, and opacity of function. The potential for decentralized systems to capture the independent development labor of all of its users, rather than just that of a core development team, is one means of competition. If a system is sufficiently easy to adopt, at least comparable to prior tooling, and gives people a satisfying means of having their work accepted and valued, the social and technical joy might be enough to outweigh the inertia of change and the convenience of centralized systems.</p>

<p>With these principles in mind, and drawing from other knowledge communities solving similar problems: internet infrastructure, library/information science, peer-to-peer networks, and radical organizing, I conceptualize a system of distributed infrastructure for (neuro)science as three objectives: <a href="#shared-data"><strong>shared data</strong></a>, <a href="#shared-tools"><strong>shared tools</strong></a>, and <a href="#shared-knowledge"><strong>shared knowledge</strong></a>.</p>

<h2 id="shared-data">Shared Data</h2>

<h3 id="formats-as-onramps">Formats as Onramps</h3>

<p>The shallowest onramp towards a generalized data infrastructure is to make use of existing discipline-specific standardized data formats. As will be discussed later, a truly universal pandisciplinary format is impossible and undesirable, but to arrive at the alternative we should first congeal the wild west of unstandardized data into a smaller number of established formats.</p>

<p>Data formats consist of some combination of an abstract specification, an implementation in a particular storage medium, and an API for interacting with the format. I won’t dwell on the particular qualities that a particular format needs, assuming that most that would be adopted would abide by FAIR principles.</p>

<p>There are a dizzying number of scientific data formats <a class="citation" href="#teamScientificDataFormats">[77]</a>, so a comprehensive treatment is impractical here and I will use Neurodata Without Borders:N (NWB)<a class="citation" href="#rubelNWBAccessibleData2019a">[78]</a> as an example. NWB is the de facto standard for systems neuroscience, adopted by many institutes and labs, though far from universally. NWB <a href="https://www.nwb.org/nwb-software/">consists of</a> a <a href="https://schema-language.readthedocs.io/en/stable/">specification language</a>, a <a href="https://nwb-schema.readthedocs.io/en/stable/">schema written in that language</a>, a <a href="https://nwb-storage.readthedocs.io/en/stable/">storage implementation in hdf5</a>, and an <a href="https://pynwb.readthedocs.io/en/stable/">API for interacting with the data</a>. They have done an admirable job of engaging with community needs <a class="citation" href="#rubelNeurodataBordersEcosystem2021">[79]</a> and making a modular, extensible format ecosystem.</p>

<p>The major point of improvement for NWB, and I imagine many data standards, is the ease of conversion and use. The conversion API requires extensive programming, knowledge of the format, and navigation of several separate tutorial documents. This means that individual labs, if they are lucky enough to have some partially standardized format for the lab, typically need to write (or hire someone to write) their <a href="https://github.com/catalystneuro/tank-lab-to-nwb">own</a> software <a href="https://github.com/catalystneuro/mease-lab-to-nwb">library</a> for conversion.</p>

<p>Without being prescriptive about its form, substantial interface development is needed to make mass conversion possible. It’s usually untrue that unstandardized data had <em>no structure,</em> and researchers are typically able to articulate it – “the filenames have the collection date followed by the subject id,” and so on. Lowering the barriers to conversion mean designing tools that match the descriptive style of folk formats, for example by prompting them to describe where each of an available set of metadata fields are located in their data. It is not an impossible goal to imagine a piece of software that can be downloaded and with minimal recourse to reference documentation allow someone to convert their lab’s data within an afternoon.</p>

<p>NWB also has an extension interface, which allows, for example, data from common hardware and software tools to be more easily described in the format. These are registered in an <a href="https://nwb-extensions.github.io/">extensions catalogue</a>, but at the time of writing it is relatively sparse. The preponderance of lab-specific conversion packages relative to extensions is indicative of an interface and community tools problem: presumably many people are facing similar conversion problems, but because there is not a place to share these techniques in a human-readable way, the effort is duplicated in dispersed codebases. We will return to some possible solutions for knowledge preservation and format extension when we discuss tools for <a href="#shared-knowledge">shared knowledge</a>.</p>

<p>For the sake of the rest of the argument, let us assume that some relatively trivial conversion process exists to subdomain-specific data formats and we reach some reasonable penetrance of standardization. The interactions with the other pieces of infrastructure that may induce and incentivize conversion will come later.</p>

<h3 id="peer-to-peer-as-a-backbone">Peer-to-peer as a Backbone</h3>

<p>We should adopt a <em>peer-to-peer</em> system for storing and sharing scientific data. There are, of course <a href="https://www.dandiarchive.org/">many</a> <a href="https://openneuro.org/">existing</a> <a href="https://www.brainminds.riken.jp/">databases</a> <a href="https://biccn.org/">for</a> scientific data, ranging from domain-general like <a href="https://figshare.com/">figshare</a> and <a href="https://zenodo.org/">zenodo</a> to the most laser-focused subdiscipline-specific. The notion of a database, like a data standard, is not monolithic. As a simplification, they consist of at least the hardware used for storage, the software implementation of read, write, and query operations, a formatting schema, some API for interacting with it, the rules and regulations that govern its use, and especially in scientific databases some frontend for visual interaction. For now we will focus on the storage software and read-write system, returning to the format, regulations, and interface later.</p>

<p>Centralized servers<sup id="fnref:cdns" role="doc-noteref"><a href="#fn:cdns" class="footnote" rel="footnote">29</a></sup> are fundamentally constrained by their storage capacity and bandwidth, both of which cost money. In order to be free, database maintainers need to constantly raise money from donations or grants in order to pay for both. Funding can never be infinite, and so inevitably there must be some limit on the amount of data that someone can upload and the speed at which it can serve files<sup id="fnref:osfspeed" role="doc-noteref"><a href="#fn:osfspeed" class="footnote" rel="footnote">30</a></sup>. Centralized servers are also intrinsically out of the control of their users, requiring them to abide whatever terms of use the server administrators set. Even if the database is carefully backed up, it serves as a single point of infrastructural failure, where if the project lapses then at worst data will be irreversibly lost, and at best a lot of labor needs to be expended to exfiltrate, reformat, and rehost the data. The same is true of isolated, local, institutional-level servers and related database platforms, with the additional problem of skewed funding allocations making them unaffordable for many researchers.</p>

<p>Peer-to-peer (p2p) systems solve many of these problems, and I argue are the only type of technology capable of making a database system that can handle the scale of all scientific data. They are also not new for science, used in projects like <a href="https://academictorrents.com/">AcademicTorrents.com</a> <a class="citation" href="#cohenAcademicTorrentsCommunityMaintained2014">[80, 81]</a> or the now defunct BioTorrents <a class="citation" href="#langilleBioTorrentsFileSharing2010">[82]</a>. Whether we acknowledge it or not, most scientific work is already available on p2p networks via sci-hub and library genesis <a class="citation" href="#himmelsteinSciHubProvidesAccess2018">[83, 84, 85]</a>.</p>

<p>There is an enormous degree of variation between p2p systems<sup id="fnref:p2pdiscipline" role="doc-noteref"><a href="#fn:p2pdiscipline" class="footnote" rel="footnote">31</a></sup>, but they share a set of architectural advantages. The essential quality of any p2p system is that rather than each participant in a network interacting only with a single server that hosts all the data, everyone hosts data and interacts directly with each other.</p>

<p>For the sake of concreteness, we can consider a (simplified) description of Bittorrent <a class="citation" href="#cohenBitTorrentProtocolSpecification2017">[87]</a>, arguably the most successful p2p protocol. To share a collection of files, a user creates a <code class="language-plaintext highlighter-rouge">.torrent</code> file with their Bittorrent client which consists of a <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function">cryptographic hash</a>, or a string that is unique to the collection of files being shared; and a list of “trackers.” A tracker, appropriately, keeps track of the <code class="language-plaintext highlighter-rouge">.torrent</code> files that have been uploaded to it, and connects users that have or want the content referred to by the <code class="language-plaintext highlighter-rouge">.torrent</code> file. The uploader (or seeder) then leaves a <a href="https://en.wikipedia.org/wiki/Glossary_of_BitTorrent_terms#Client">torrent client</a> open waiting for incoming connections. Someone who wants to download the files (a leecher) will then open the <code class="language-plaintext highlighter-rouge">.torrent</code> file in their client, which will then ask the tracker for the IP addresses of the other peers who are seeding the file, directly connect to them, and begin downloading. So far so similar to standard client-server systems, but say another person wants to download the same files before the first person has finished downloading it: rather than <em>only</em> downloading from the original seeder, the new leecher downloads from <em>both</em> the original seeder and the first leecher by requesting pieces of the file from each until they have the whole thing. Leechers are incentivized to share among each other to prevent the seeders from spending time reuploading the pieces that they already have, and once they have finished downloading they become seeders themselves.</p>

<p>From this very simple example, we can articulate a number of attractive qualities of p2p systems:</p>

<ul>
  <li>First, p2p systems are extremely <strong>inexpensive to maintain</strong> since they take advantage of the existing bandwidth and storage space of the computers in the swarm. Near the height of its popularity in 2009, The Pirate Bay, a notorious bittorrent tracker <a class="citation" href="#vandersarPirateBayFive2011">[88]</a>, was estimated to cost $3,000 per month to maintain while serving approximately 20 million peers <a class="citation" href="#roettgersPirateBayDistributing2009">[89]</a>. According to a database dump from 2013 <a class="citation" href="#PirateBayArchiveteam2020">[90]</a>, multiplying the size of each torrent by the number of seeders (ignoring any partial downloads from leechers), the approximate instantaneous amount of data stored by The Pirate Bay was ~26 Petabytes. The comparison to centralized services is not straightforward, since it is hard to evaluate the distributed costs of additional storage media (as well as the costs avoided by being able to take advantage of existing storage infrastructure within labs and institutes), but for the sake of illustration: hosting 26PB would cost $546,000/month with standard AWS S3 hosting (<a href="https://aws.amazon.com/s3/pricing/?nc=sn&amp;loc=4">$0.021/GB/month</a>). On AWS, downloads cost extra (<a href="https://aws.amazon.com/s3/pricing/?nc=sn&amp;loc=4">$0.05/GB</a>), so the much smaller <a href="https://academictorrents.com">academictorrents.com</a> which <a href="https://github.com/academictorrents/academictorrents-docs/issues/31#issuecomment-1155917166">has served</a> nearly 18PB in 1.3m downloads since 2016 would have cost $900,000 in bandwidth costs alone — as opposed to the <a href="https://github.com/academictorrents/academictorrents-docs/issues/31#issuecomment-1152851111">literally zero dollars</a> it costs to operate.</li>
  <li>The <strong>speed</strong> of a bittorrent swarm <em>increases,</em> rather than decreases, the more people are using it since it is capable of using all of the available bandwidth in the system.</li>
  <li>The network is extremely <strong>resilient</strong> since the data is shared across many independent peers in the system. If our goal is to make a resilient and robust data architecture, we would benefit by paying attention to the tools used in the broader archival community, especially the archival communities that are frequent targets of governments and intellectual property holders<a class="citation" href="#spiesDataIntegrityLibrarians2017">[91]</a>.  Despite more than 15 years of concerted effort by governments and intellectual property holders, The Pirate Bay is still alive and kicking<sup id="fnref:knockin" role="doc-noteref"><a href="#fn:knockin" class="footnote" rel="footnote">32</a></sup> <a class="citation" href="#kim15YearsPirate2019">[92]</a>. This is because even if the entire infrastructure of the tracker is destroyed, as it was in 2006, the files are distributed across all of its users, the actual database of <code class="language-plaintext highlighter-rouge">.torrent</code> metadata is quite small, and the tracker software is extraordinarily simple to rehost <a class="citation" href="#vandersarOpenBayNow2014">[93]</a> – The Pirate Bay was back online in 2 days.  When another tracker, what.cd (which we will return to <a href="#archives-need-communities">soon</a>) was shut down, a series of successors popped up using the open source tools <a href="https://github.com/WhatCD/Gazelle">Gazelle</a> and <a href="https://github.com/WhatCD/Ocelot">Ocelot</a> that what.cd developers built. Within two weeks, one successor site had recovered and reindexed 200,000 of its torrents resubmitted by former users <a class="citation" href="#vandersarWhatCdDead2016">[94]</a>. Bittorrent is also used by archival groups with little funding like <a href="https://wiki.archiveteam.org/index.php/Main_Page">Archive Team</a>, who struggled – but eventually succeeded – to disseminate their <a href="https://wiki.archiveteam.org/index.php/GeoCities_Project">geocities archive</a> over a single “crappy cable modem” <a class="citation" href="#scottGeocitiesTorrentUpdate2010">[95]</a>.</li>
  <li>The network is extremely <strong>scalable</strong> since there is no cost to connecting new peers and the users of a system expand the storage capacity of the system depending on their needs. Rather than having one extremely fast data center, the model of p2p systems is to leverage many approachable peer/servers.</li>
</ul>

<p>Peer-to-peer systems are not mutually exclusive with centralized servers: servers are peers too, after all. A properly implemented p2p system will always be <em>at least</em> as fast and have <em>at least</em> as much storage as any alternative centralized server because peers can use <em>both</em> the bandwidth of the server <em>and</em> that of any peers that have the file. In the bittorrent ecosystem large-bandwidth/storage peers are known as “seedboxes”<a class="citation" href="#rossiPeekingBitTorrentSeedbox2014">[96]</a> when they use the bittorrent protocol, and “web seeds”<a class="citation" href="#hoffmanHTTPBasedSeedingSpecification">[97]</a> when they use a protocol built on top of traditional HTTP. <a href="https://archive.org">Archive.org</a> has been distributing all of its materials <a href="https://archive.org/details/bittorrent">with bittorrent</a> by using its servers as web seeds since 2012 and makes this point explicitly: “BitTorrent is now the fastest way to download items from the Archive, because the Bittorrent client downloads simultaneously from two different Archive servers located in two different datacenters, and from other Archive users who have downloaded these Torrents already.” <a class="citation" href="#kahle000000Torrents2012">[98]</a></p>

<p>p2p systems complement centralized servers in a number of ways beyond raw download speed, increasing the efficiency and performance of the network as a whole. Spotify began as a joint client/server and p2p system <a class="citation" href="#kreitzSpotifyLargeScale2010b">[99]</a>, where when a listener presses play the central server provides the data until the p2p system locates peers with a cached copy to download from. The central server is able to respond quickly and reliably, and is the server of last resort in the case of rare files that aren’t being shared by anyone else in the network. The p2p system alleviates pressure on the central server, improving the performance of the network and reducing server costs.</p>

<p>A peer to peer system is a particularly natural fit for many of the common circumstances and practices in science, where centralized server architectures seem (and prove) awkward and inefficient. Most labs, institutes, or other organized bodies of science have some form of local or institutional storage systems. In the most frequent cases of sharing data within a lab or institute, sending it back and forth to some nationally-centralized server is like walking across the lab by going the long way around the Earth. That’s the method invoked by a Dropbox or AWS link, which keeps a time-tested p2p system relevant: walking a flash drive across the lab. The system makes less sense when several people in the same place need to access the same data at the same time, as is frequently the case with multi-lab collaborations, or scientific conferences and workshops. Instead of needing to wait on the 300kb/s conference wifi bandwidth as it’s cheese-gratered across every machine, we instead could directly beam it between all computers in range simultaneously, full blast through the decrepit network switch that won’t have seen that much excitement in years.</p>

<p>If we take the suggestion of Andrey Andreev et al. and invest in server clusters within institutes <a class="citation" href="#andreevBiologistsNeedModern2021">[100, 101]</a>, their impact could be multiplied manyfold by fluidly combining them in a p2p swarm. While the NIH might be shy to start up another server farm for all scientific data and prefer to contract with AWS, the rest of us don’t have to be. Nervous university administrators concerned about bandwidth costs should also favor p2p systems: instead of needing to serve entire datasets to each person who wants them, the load can be spread out across many institutes naturally based on the use of the file, and sharing the dataset internally would cost nothing at all.</p>

<p>So far I have relied on the Extraordinarily Simplified Bittorrent<sup id="fnref:tm" role="doc-noteref"><a href="#fn:tm" class="footnote" rel="footnote">33</a></sup> depiction of a peer to peer system, but there are many improvements and variants that can address different needs for scientific data infrastructure.</p>

<p>One obvious need that bittorrent can’t currently support is version control<sup id="fnref:bittorrentv2" role="doc-noteref"><a href="#fn:bittorrentv2" class="footnote" rel="footnote">34</a></sup>, but more recent p2p systems do. <a href="https://ipfs.io/">IPFS</a> functions like “a single BitTorrent swarm, exchanging objects within one Git repository.” <a class="citation" href="#benetIPFSContentAddressed2014">[102]</a><sup id="fnref:whatsgit" role="doc-noteref"><a href="#fn:whatsgit" class="footnote" rel="footnote">35</a></sup> Dat <a class="citation" href="#ogdenDatDistributedDataset2017">[103]</a>, specifically designed for data synchronization and versioning, handles versioning and more. A full description of IPFS is out of scope, and it has plenty of problems <a class="citation" href="#patsakisHydrasIPFSDecentralised2019">[104]</a>, but for now it suffices to say p2p systems can handle version control.</p>

<p>Bittorrent swarms are vulnerable to data loss if all the peers seeding a file disconnect (though the tail is longer than typically assumed, see <a class="citation" href="#zhangUnravelingBitTorrentEcosystem2011">[106]</a>), but this too can be addressed with updated p2p system design. A first-order solution to this problem is a variant of IPFS’ notion of ‘pinning.’ Since backup to lab-level or institutional servers is already commonplace, one peer could be able to ‘pin’ another and automatically download all the data that they share. This concept could scale to institutes and national infrastructure as scientists can request the datasets they’d like to be saved permanently be pinned.</p>

<p>Another could be something akin to Freenet <a class="citation" href="#clarkeFreenetDistributedAnonymous2001">[107]</a>. Peers could allocate a certain amount of their unused storage space to be used to automatically download, cache, and rehost shards of other datasets. Distributing chunks and encrypting them at rest so the rehoster can’t inspect their contents would make it possible to maintain privacy and network availability for sensitive data (see, for example, <a href="https://inqlab.net/projects/eris/">ERIS</a>). IPFS has an analogous concept – BitSwap – that is makes it into a barter system. Peers who seek to download will have to ‘earn’ it by finding some chunk of data that the other peers want, download, and share them, though it seems like an empirical question whether or not a barter system works or is necessary.</p>

<p><a href="https://solidproject.org/">Solid</a> is a project that almost exactly meets all these needs <a class="citation" href="#capadisliSolidProtocol2020">[108, 64, 109]</a>. Solid allows people to share data in <a href="https://solidproject.org/about">Pods</a>, which let them control access and distribution across storage system with a unified identity system. It is implementation-agnostic, and so can support any peer-to-peer storage and transfer system that complies with its <a href="https://solidproject.org/TR/protocol">protocol specification</a>.</p>

<p>There are a number of additional requirements for a peer to peer scientific data infrastructure, but even these seemingly very technical problems of versioning and distributed storage show the clear need to consider the structure of the surrounding social system. What control do we give to researchers over the version history of their data? Should people that aren’t the originating researcher be able to issue new versions? What structure of distributed/centralized storage works? How should we incentivize sharing of excess storage and resources?</p>

<p>Even before considering additional social systems, a p2p structure in itself implies a different relationship to infrastructure. Scientists always unavoidably make their data available to at least one person: themselves; on at least one computer: theirs, and that computer is usually connected to the internet. With a p2p system that integrates metadata from domain-specific data formats, that’s it, that’s all, the data is already hosted by merely existing. Dust your palms off: open data achieved. A peer-to-peer backbone for scientific infrastructure realizes the unnecessarily radical notion that our infrastructure can be integrated into our daily practices, rather than existing exogenously as something “out there.” It helps us internalize the slyly subversive notion that <em>we can build it ourselves</em> instead of renting something out of our control from someone else.</p>

<p>Scientists don’t need to reinvent the notion of distributed, community curated data archives from scratch. In addition to scholarly work on the social systems of digital infrastructure, we can learn from communities of practice, and there has been no more important and impactful decentralized archival project than internet piracy.</p>

<h3 id="archives-need-communities">Archives Need Communities</h3>

<p>Why do hundreds of thousands of people, completely anonymously, with zero compensation, spend their time to do something that is as legally risky as curating pirated cultural archives?</p>

<p>Scholarly work, particularly from Economics, tends to focus on understanding piracy in order to prevent it<a class="citation" href="#basamanowiczReleaseGroupsDigital2011">[110, 111]</a>, taking the moral good of intellectual property markets as an <em>a priori</em> imperative and investigating why people behave <em>badly</em> and “rend [the] moral fabric associated with the respect of intellectual property.” <a class="citation" href="#hindujaDeindividuationInternetSoftware2008">[111]</a>. If we put the legality of piracy aside, we may find a wealth of wisdom and insight to draw from for building scientific infrastructure.</p>

<p>The world of digital piracy is massive, from entirely disorganized efforts of individual people on public sites to extraordinarily organized release groups <a class="citation" href="#basamanowiczReleaseGroupsDigital2011">[110]</a>, and so a full consideration is out of scope (see <a class="citation" href="#eveWarezInfrastructureAesthetics2021">[112]</a>), but many of the important lessons are taught by the structure of bittorrent trackers.</p>

<p>An underappreciated element of the BitTorrent protocol is the effect of the separation between the data transfer protocol and the “discovery” part of the system — or “overlay” — on the community structure of torrent trackers (for a more complete picture of the ecosystem, see <a class="citation" href="#zhangUnravelingBitTorrentEcosystem2011">[106]</a>). Many peer to peer networks like <a href="https://en.wikipedia.org/wiki/Kazaa">KaZaA</a> or the <a href="https://en.wikipedia.org/wiki/Gnutella">gnutella</a>-based <a href="https://en.wikipedia.org/wiki/LimeWire">Limewire</a> had searching for files integrated into the transfer interface. The need for torrent trackers to share .torrent files spawned a massive community of private torrent trackers that for decades have been iterating on cultures of archival, experimenting with different community structures and incentives that encourage people to share and annotate some of the world’s largest, most organized libraries.</p>

<p>One of these private trackers was the site of one of the largest informational tragedies of the past decade: what.cd<sup id="fnref:whatdiss" role="doc-noteref"><a href="#fn:whatdiss" class="footnote" rel="footnote">36</a></sup>, which I will use as an example to describe some of these community systems.</p>

<p>What.cd was a bittorrent tracker that was arguably the largest collection of music that has ever existed. At the time of its destruction in 2016, it was host to just over one million unique releases, and approximately 3.5 million torrents<sup id="fnref:dbsize" role="doc-noteref"><a href="#fn:dbsize" class="footnote" rel="footnote">37</a></sup> <a class="citation" href="#dunhamWhatCDLegacy2018">[113]</a>. Every torrent was organized in a meticulous system of metadata communally curated by its roughly 200,000 global users. The collection was built by people who cared deeply about music, rather than commercial collections provided by record labels notorious for ceasing distribution of recordings that are not commercially viable — or just losing them in a fire <a class="citation" href="#rosenDayMusicBurned2019">[114]</a>. Users would spend large amounts of money to find and digitize extremely rare recordings, many of which were unavailable anywhere else and are now unavailable anywhere, period. One former user describes one example:</p>

<blockquote>
  <p>“I did sound design for a show about Ceaușescu’s Romania, and was able to pull together all of this 70s dissident prog-rock and stuff that has never been released on CD, let alone outside of Romania” <a class="citation" href="#sonnadEulogyWhatCd2016">[115]</a></p>
</blockquote>

<p><img src="/infrastructure/assets/images/kanye-what.png" alt="A what.cd artist page (Kanye west) that shows each of his albums having perhaps a dozen different torrents: each time the album was released, on cd, vinyl, and web, each in multiple different audio formats." />
<em>The what.cd artist page for Kanye West (taken from <a href="https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/">here</a> in the style of pirates, without permission). For the album “Yeezus,” there are ten torrents, grouped by each time the album was released on CD and Web, and in multiple different qualities and formats (.flac, .mp3). Along the top is a list of the macro-level groups, where what is in view is the “albums” section, there are also sections for bootleg recordings, remixes, live albums, etc.</em></p>

<p>What.cd was a “private” bittorrent tracker, where unlike public trackers that anyone can access, membership was strictly limited to those who were personally invited or to those who passed an interview (for more on public and private trackers, see <a class="citation" href="#meulpolderPublicPrivateBitTorrent">[116]</a>). Invites were extremely rare, and the interview process was demanding to the point where <a href="https://opentrackers.org/whatinterviewprep.com/index.html">extensive guides</a> were written to prepare for them.</p>

<p>The what.cd incentive system was based on a required ratio of data uploaded vs. data downloaded <a class="citation" href="#jiaHowSurviveThrive2013">[117]</a>. Peer to peer systems need to overcome a free-rider problem where users might download a torrent (“leeching”) and turn their computer off, rather than leaving their connection open to share it to others (or, “seeding”). In order to download additional music, then, one would have to upload more. Since downloading is highly restricted, and everyone is trying to upload as much as they can, torrents had a large number of “seeders,” and even rare recordings would be sustained for years, a pattern common to private trackers <a class="citation" href="#liuUnderstandingImprovingRatio2010">[118]</a>.</p>

<p>The high seeder/leecher ratio made it so it was extremely difficult to acquire upload credit, so users were additionally incentivized to find and upload new recordings to the system. What.cd implemented a “bounty” system, where users with a large amount of excess upload credit would be able to offer some of it to whoever was able to upload the album they wanted. To “prime the pump” and keep the economy moving, highlight artists in an album of the week, or direct users to preserve rare recordings, moderators would also use a “freeleech” system, where users would be able to download a specified set of torrents without it counting against their download quantity <a class="citation" href="#kashEconomicsBitTorrentCommunities2012">[119, 120]</a>.</p>

<p>The other half of what.cd was the more explicitly social elements: its forums, comment sections, and moderation systems. The forum was home to roiling debates that lasted years about the structure of some tagging schema, whether one genre was just another with a different name, and so on. The structure of the community was an object of constant, public negotiation, and over time the metadata system evolved to be able to support a library of the entirety of human musical culture<sup id="fnref:subtlety" role="doc-noteref"><a href="#fn:subtlety" class="footnote" rel="footnote">38</a></sup>. To support the good operation of the site, the forums were also home to a huge amount of technical knowledge, like guides on how to make a perfect copy of a CD or how to detect a fake upload, that eased new users into being able to use and contribute to the system.</p>

<p>A critical problem in maintaining coherent databases is correcting metadata errors and departures from schemas. Finding errors was rewarded. Users were able to discuss and ask questions of the uploader in a comment section below each upload, which would allow “polite” resolution of low-level errors like typos. More serious problems could be reported to the moderation team, which caused the upload to be visibly marked as under review, and the report could then be discussed either in the comment sections or the forum. The system wasn’t perfect: being an anonymous, gray-area community, there was of course plenty of power to be abused. Rather than being a messy hodgepodge of fake, low-quality uploads, though, what.cd was always teetering just shy of perfection.</p>

<p>These structural considerations do not capture the most elusive but indisputably important feature of what.cd’s community infrastructure: <em>the sense of commmunity</em>. The What.cd forums were the center of many user’s relationships to music. Threads about all the finest scales of music nichery could last for years: it was a rare place people who probably cared a little bit too much about music could talk to people with the same condition. What made it more satisfying than other music forums was that no matter what music you were talking about, everyone else in the conversation would always have access to it if they wanted to hear it.  Beyond any structural incentives, people spent so much time building and maintaining what.cd because it became a source of community and a sink of personal investment.</p>

<p>Structural norms supported by social systems converge as a sort of <em>reputational</em> incentive. Uploading a new album to fill a bounty both makes the network more functional and complete, but also <em>people respect you for it</em> because it’s prominently displayed on your profile as well as in the bounty charts and that <em>feels good</em>. Becoming known on the forums for answering questions, writing guides, or even just having a good taste in music <em>feels good</em> and also contributes to the overall health of the system. Though there are plenty of databases, and even plenty of different communication venues for scientists, there aren’t any databases (to my knowledge) with integrated community systems.</p>

<p>The tracker overlay model mirrors and extends some of the recommendations made by Benedikt Fecher and colleagues in their work on the reputational economy surrounding data sharing <a class="citation" href="#fecherReputationEconomyHow2017">[121]</a>. They give three policy recommendations: Increasing reputational benefits, reducing transaction costs, and “increasing market transparency by making open access to research data more visible to members of the research community.” One way to accomplish implement them is to embed a data sharing system within a social system that is designed to reward communitarian behavior.</p>

<p>Many features of what.cd’s structure are undesirable for scientific infrastructure, but they demonstrate that a robust archive is not only a matter of building a database with some frontend, but also building a community <a class="citation" href="#brossCommunityCollaborationContribution2013">[122]</a>. Of course, we need to be careful with building the structural incentives for a data sharing system: the very last thing we want is another <a href="https://etiennelebel.com/cs/t-leaderboard/t-leaderboard.html">coercive leaderboard</a> that turns what should be a collaborative effort punitive. In contrast to what.cd, for infrastructure we want extremely low barriers to entry, and be agnostic to resources — researchers with access to huge server farms should not be unduly favored. We should think carefully about using downloading as the “cost,” because downloading and analyzing huge amounts of data can be <em>good</em> and exactly what we <em>want</em> in some circumstances, but a threat to privacy and data governance in others.</p>

<p>This model has its own problems, including the lack of interoperability between different trackers, the need to recreate a new set of accounts and database for each new tracker, among others. It’s also been tried before: sharing data in specific formats (as our running example, Neurodata Without Borders) on indexing systems like bittorrent trackers amounts to something like BioTorrents <a class="citation" href="#langilleBioTorrentsFileSharing2010">[82]</a> or <a href="https://academictorrents.com/">AcademicTorrents</a> <a class="citation" href="#cohenAcademicTorrentsCommunityMaintained2014">[80]</a>. Even with our extensions of version control and some model of automatic mirroring of data across the network, we still have some work to do. To address these and several other remaining needs for scientific data infrastructure, we can take inspiration from <em>federated systems.</em></p>

<h3 id="linked-data-or-surveillance-capitalism">Linked Data or Surveillance Capitalism?</h3>

<blockquote>
  <p>Having become a dense and consistent historical reality, language forms the locus of tradition, of the unspoken habits of thought, of what lies hidden in a people’s mind; it accumulates an ineluctable memory which does not even know itself as memory. Expressing their thoughts in words of which they are not the masters, enclosing them in verbal forms whose historical dimensions they are unaware of, men believe that their speech is their servant and do not realize that they are submitting themselves to its demands.</p>

  <p>Michel Foucault — <em>The Order of Things</em> <a class="citation" href="#foucaultOrderThings2001">[123]</a></p>
</blockquote>

<div class="draft-text">

  <p>ARAX is up:</p>

  <ul>
    <li>https://arax.rtx.ai/?r=e891e6e6-44fd-4684-9d36-f94e3e81b554</li>
    <li>https://arax.rtx.ai/?r=81249a42-b300-4dcf-94c9-7a9fe2f78237</li>
    <li>https://arax.rtx.ai/?r=6d23dcbe-7924-49f0-9f25-1d52dee6712b</li>
    <li>https://arax.ncats.io/?r=51780</li>
    <li>https://arax.ncats.io/?r=51779</li>
  </ul>
</div>

<p>There is no shortage of databases for scientific data, but their traditional structure chokes on the complexity of representing multi-domain data. Typical relational databases require some formal schema to structure the data they contain, which have varying reflections in the APIs used to access them and interfaces built atop them. This broadly polarizes database design into domain-specific and domain-general<sup id="fnref:trackeranalogy" role="doc-noteref"><a href="#fn:trackeranalogy" class="footnote" rel="footnote">39</a></sup>. This design pattern results in a fragmented landscape of databases with limited interoperability. How shall we link the databases? In this section we’ll consider the Icarian promise of creating the great unified database of everything as a way of motivating an alternative that blends <em>linked data</em> <a class="citation" href="#berners-leeLinkedData2006">[124]</a> with <em>federated systems</em> against our peer to peer backbone in the next section.</p>

<p>Domain-specific databases require data to be in one or a few specific formats, and usually provide richer tools for manipulating and querying by metadata, visualization, summarization, aggregation that are purpose-built for that type of data. For example, NIH’s <a href="https://www.ncbi.nlm.nih.gov/gene/12550">Gene</a> tool has several visualization tools and cross-referencing tools for finding expression pathways, genetic interactions, and related sequences (Figure xx). This pattern of database design is reflected at several different scales, through institutional databases and tools like the Allen <a href="https://connectivity.brain-map.org/">brain atlases</a> or <a href="http://observatory.brain-map.org/visualcoding/">observatory</a>, to lab- and project-specific dashboards. This type of database is natural, expressive, and powerful — for the researchers they are designed for. While some of these databases allow open data submission, they often require explicit moderation and approval to maintain the guaranteed consistency of the database, which can hamper mass use.</p>

<p><img src="/infrastructure/assets/images/nih_gene_cdh1.png" alt="An example specialized plot of genomic regions, transcripts and products for the CDH1 gene (linked above), showing how specific tools have been built for this specific dataset" />
<em>NIH’s Gene tool included many specific tools for visualizing, cross-referencing, and aggregating genetic data. Shown is the “genomic regions, transcripts, and product” plot for Mouse Cdh1, which gives useful, common summary descriptions of the gene, but is not useful for, say, visualizing reading proficiency data.</em></p>

<p>General-purpose databases like <a href="https://figshare.com/">figshare</a> and <a href="https://zenodo.org/">zenodo</a><sup id="fnref:yrcool" role="doc-noteref"><a href="#fn:yrcool" class="footnote" rel="footnote">40</a></sup> are useful for the mass aggregation of data, typically allowing uploads from most people with minimal barriers. Their general function limits the metadata, visualization, and other tools that are offered by domain-specific databases, however, and are essentially public, versioned, folders with a DOI. Most have fields for authorship, research groups, related publications, and a single-dimension keyword or tags system, and so don’t programmatically reflect the metadata present in a given dataset.</p>

<p>The dichotomy of fragmented, subdomain-specific databases and general-purpose databases makes combining information from across even extremely similar subdisciplines combinatorically complex and laborious. In the absence of a formal interoperability and indexing protocol between databases, even <em>finding</em> the correct subdomain-specific database often comes down to pure luck. It also puts researchers who want to be good data stewards in a difficult position: they can hunt down the appropriate subdomain specific database and risk general obscurity; use a domain-general database and make their work more difficult for themselves and their peers to use; or spend all the time it takes to upload to multiple databases with potentially conflicting demands on format.</p>

<p>What can be done? There are a few naïve answers from standardizing different parts of the process: If we had a universal data format, then interoperability becomes trivial. Conversely, we could make a single ur-database that supports all possible formats and tools.</p>

<p>The notion of a universal database system almost immediately runs aground on the reality that organizing knowledge is intrinsically political. Every subdiscipline has conflicting <em>representational</em> needs, will develop different local terminology, allocate differing granularity and develop different groupings and hierarchies for the same phenomena. At their mildest, differences in representational systems can be incompatible, but at their worst they can reflect and reinforce prejudices and become the site of expression for intellectual and social power struggles <a class="citation" href="#joLessonsArchivesStrategies2020">[125, 126, 127, 128]</a>. Every subdiscipline has conflicting <em>practical</em> needs, with infinite variation in privacy demands, different priorities between storage space, bandwidth, and computational power, and so on. In all cases the boundaries of our myopia are impossible to gauge: we might think we have arrived at a suitable schema for biology, chemistry, and physics… but what about the historians?</p>

<p>Matthew J Bietz and Charlotte P Lee articulate this tension in their ethnography of metagenomics databases:</p>

<blockquote>
  <p>“Participants describe the individual sequence database systems as if they were shadows, poor representations of a widely-agreed-upon ideal. We find, however, that by looking across the landscape of databases, a different picture emerges. Instead, <strong>each decision about the implementation of a particular database system plants a stake for a community boundary. The databases are not so much imperfect copies of an ideal as they are arguments about what the ideal Database should be.</strong> […]</p>

  <p>In the end, however, <strong>the system was so tailored to a specific set of research questions that the collection of data, the set of tools, and even the social organization of the project had to be significantly changed.</strong> New analysis tools were developed and old tools were discarded. Not only was the database ported to a different technology, the data itself was significantly restructured to fit the new tools and approaches. While the database development projects had begun by working together, in the end they were unable to collaborate. <strong>The system that was supposed to tie these groups together could not be shielded from the controversies that formed the boundaries between the communities of practice.</strong>” <a class="citation" href="#bietzCollaborationMetagenomicsSequence2009">[129]</a></p>
</blockquote>

<p>The pursuit of unified representation is an intimate part of the history of linked data, which relies on “ontologies” or controlled vocabularies that describe a set of objects (or classes) and the properties they can have. For example, <a href="https://schema.org">schema.org</a> maintains a widely used set of hierarchical vocabularies to describe the fundamental things that exist in the world, in particular the unfamiliar world in which a <a href="https://schema.org/Person">Person</a> has a <a href="https://schema.org/gender">gender</a> and <a href="https://schema.org/netWorth">net worth</a> but lacks a race <a class="citation" href="#poirierTurnScruffyEthnographic2017">[40]</a>. At one extreme in the world of ontology builders, the ideological nature of demarcating what is allowed to exist is as clear as a klaxon (emphasis in original):</p>

<blockquote>
  <p>An exception is the Open Biomedical Ontologies (OBO) Foundry initiative, which accepts under its label only those ontologies that adhere to the principles of ontological realism. […] Ontologies, from this perspective, are representational artifacts, comprising a taxonomy as their central backbone, whose representational units are intended to designate <em>universals</em> (such as <em>human being</em> and <em>patient role</em>) or <em>classes defined in terms of universals</em> (such as <em>patient,</em> a class encompassing <em>human beings</em> in which there inheres a <em>patient role</em>) and certain relations between them. […]</p>

  <p>BFO is a realist ontology [15,16]. This means, most importantly, that representations faithful to BFO can acknowledge only those entities which exist in (for example, biological) reality; thus they must reject all those types of putative negative entities - lacks, absences, non-existents, possibilia, and the like <a class="citation" href="#ceustersFoundationsRealistOntology2010">[130]</a></p>
</blockquote>

<p>In practice, because of the difficulty of changing the representation and encompassing database systems on a dime, using these ontologies to link disparate datasets tends to follow the pattern of metadata <em>overlays</em> where the structure of individual databases are mapped onto one “unifying” ontology to allow for aggregation and translation. This approach appears gentler than standardization at the level of individual databases, but has the same problems kicked up one level of abstraction.</p>

<p>To concretize the problems with a globally unified database or metadata overlay, the remainder of this section will trace the compromises and outcomes of the The NIH’s “Biomedical Data Translator” project. The Translator project was initially described in the 2016 Strategic Plan for Data Science as a means of translating between biomedical data formats:</p>

<blockquote>
  <p>Through its Biomedical Data Translator program, the National Center for Advancing Translational Sciences (NCATS) is supporting research to develop ways to connect conventionally separated data types to one another to make them more useful for researchers and the public. <a class="citation" href="#NIHStrategicPlan2018">[36]</a></p>
</blockquote>

<p>The original <a href="https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator">funding statement from 2016</a> is similarly humble, and press releases <a href="https://web.archive.org/web/20210709171335/https://ncats.nih.gov/pubs/features/translator">through 2017</a> also speak mostly in terms of querying the data – though some ambition begins to creep in. By 2019, the vision for the project had veered sharply away from anything a basic researcher might recognize as a means of translating between data types. In their piece “Toward a Universal Biomedical Translator,” then in a feasibility assessment phase, the members of the Translator Consortium assert that universal translation between biomedical data is impossible<sup id="fnref:impossibledata" role="doc-noteref"><a href="#fn:impossibledata" class="footnote" rel="footnote">41</a></sup><a class="citation" href="#consortiumUniversalBiomedicalData2019">[131]</a>. The impossibility they saw was not that of conflicting political demands on the structure of organization (as per <a class="citation" href="#bowkerSortingThingsOut1999">[128]</a>), but of the sheer numeracy of the data and vocabularies needed to describe them. The risk posed by a lack of a universal “language” was not being able to index all possible data, rather than inaccuracy or inequity.</p>

<p>Undaunted by their stated belief in the impossibility of a universalizing ontology, the Consortium created one in their <a href="https://biolink.github.io/biolink-model/docs/">biolink</a> model <a class="citation" href="#bruskiewichBiolinkBiolinkmodel2021">[132, 133]</a>. Biolink consists of a hierarchy of basic classes: eg. a <a href="https://biolink.github.io/biolink-model/docs/BiologicalEntity.html">BiologicalEntity</a> like a <a href="https://biolink.github.io/biolink-model/docs/Gene.html">Gene</a>, or a <a href="https://biolink.github.io/biolink-model/docs/ChemicalEntity.html">ChemicalEntity</a> like a <a href="https://biolink.github.io/biolink-model/docs/Drug.html">Drug</a>. Classes can then linked by any number of properties, or “Slots,” like a therapeutic procedure that <a href="https://biolink.github.io/biolink-model/docs/treats.html">treats</a> a disease.</p>

<p>The translator does not attempt to respond to the needs of researchers or labs who might want to link their raw data splayed out across flash drives and file structures whose chaos borders on whimsy. Instead, the Translator operates at the level of “knowledge,” or “generally accepted, universal assertions derived from the accumulation of information” <a class="citation" href="#fechoProgressUniversalBiomedical2022">[134]</a>. Rather than translating <em>between data types</em>, the meaning of “translation” shifted to meaning <em>“translating data into knowledge”</em> <a class="citation" href="#consortiumUniversalBiomedicalData2019">[131]</a>.</p>

<p>To feed the Translator, Biolink sits “on top of” a <a href="http://www.smart-api.info/registry">collection of database APIs</a> that serve structured biomedical data, each called a “knowledge source.” Individual APIs <a href="https://github.com/NCATSTranslator/ReasonerAPI">declare</a> that they are able to provide data for a particular set of classes or slots, like <a href="http://www.smart-api.info/ui/adf20dd6ff23dfe18e8e012bde686e31">drugs that affect genetic expression</a>, and are then made browsable from the <a href="http://www.smart-api.info/portal/translator/metakg">SmartAPI Knowledge Graph</a>. Queries to individual APIs do not return “raw” data, but return assertions of fact in the parlance of the Biolink model: this procedure treats that disease, etc.</p>

<p>Because individual researchers do not typically represent their data in the form of factual assertions, knowledge sources are constrained to “highly curated biomedical databases” or other aggregated systems. The NIH RePORTER tool <a href="https://reporter.nih.gov/search/DShVUhB_ZUq0X5UWFjy5WQ/projects?shared=true">gives an overview</a> of the way these knowledge sources are prepared when none already exist for a given Biolink class or predicate: automated <a href="https://reporter.nih.gov/project-details/10548337">text mining</a> tools and a series of <a href="https://reporter.nih.gov/project-details/10056962">domain-specific data provider</a> projects, rather than via tools provided to researchers.</p>

<p>The collection of knowledge sources, linked to nodes and edges in the Biolink model, are designed to be queried as a graph. To answer a query like “what drug treats this disease?” the translator considers the graph of entities linked to the disease: what symptoms does the disease have? what genes are linked to those symptoms? which drugs act on those genes? and so on <a class="citation" href="#renaissancecomputinginstituterenciBiomedicalDataTranslator2022">[135]</a>. The form of the Translator as a graph-based question answering machine bounds its application as a platform for researchers to guide their research and clinicians to guide their care <a class="citation" href="#hailuNIHfundedProjectAims2019">[136]</a>, rather than a tool for linking data.</p>

<p>One primary example currently featured by NCATS is using the translator to propose novel treatments for drug-induced liver injury (DILI) <a class="citation" href="#renaissancecomputinginstituterenciUseCasesShow2022">[137]</a> detailed in a 2021 conference paper <a class="citation" href="#goelExplanationContainerCaseBased2021">[138]</a>. To find a candidate drug, the researchers manually conducted three API queries: first they searched for phenotypes associated with DILI and selected “one of them”<sup id="fnref:dilisearch" role="doc-noteref"><a href="#fn:dilisearch" class="footnote" rel="footnote">42</a></sup> — “red blood cell count”. Then they queried for genes associated with red blood cell count to find telomerase reverse transcriptase (TERT), and then finally for drugs that affect TERT to find Zidovudine. The directionality of each of these relationships, high vs. low, increases vs. decreases, is unclear in each case. A more recent report on the Translator repeated this pattern of manual querying, arriving at a handful of different genes and drugs <a class="citation" href="#fechoProgressUniversalBiomedical2022">[134]</a>.</p>

<p>While the current examples are highly manual, providing an array of results for each query along with links to associated papers on pubmed, some algorithmic system for ranking results is necessary to make use of the information in the extended knowledge graph. Rather than just the first-order connections, it should be possible to make use of second, third, and n-th order connections to weight potential results. Algorithmic medical recommendation systems have been thoroughly problematized elsewhere (eg. <a class="citation" href="#groteEthicsAlgorithmicDecisionmaking2020">[139, 140, 141, 142]</a>). The primary ranking algorithm is developed by a defense contractor (CoVar) who has<sup id="fnref:unironically" role="doc-noteref"><a href="#fn:unironically" class="footnote" rel="footnote">43</a></sup> named it ROBOKOP <a class="citation" href="#mortonROBOKOPAbstractionLayer2019">[143]</a><sup id="fnref:fine" role="doc-noteref"><a href="#fn:fine" class="footnote" rel="footnote">44</a></sup>. Though ROBOKOP functions with a simple weighted graph metric based on citations and abstract text, the ranking system is intended to be extended with machine learning tools <a class="citation" href="#mortonROBOKOPAbstractionLayer2019">[143]</a> that can be trained based on the way the provided answers are used <a class="citation" href="#consortiumUniversalBiomedicalData2019">[131]</a>.
Algorithmic recommendation platforms are in a regulatory gray area <a class="citation" href="#ordishAlgorithmsMedicalDevices2019">[144, 145]</a>, but would arguably need to have interpretable results with clear provenance to pass scrutiny. The DILI example uses a language model which explained the recommendation of Zidovudine with all the clarity of “one of ‘DOWNREGULATOR,’ ‘INHIBITOR,’ ‘INDIRECT DOWNREGULATOR’.”</p>

<p>The arrival at a biomedical question answering platform built atop an algorithmic ranking system for a knowledge graph that queries  200+ aggregated data sources has several qualities that should give us pause.</p>

<p>First, as with any machine-learning based system, the algorithm can only reflect the structure of its input data, including its bias. The “mass of data” approach ML tools lend themselves to, in this case, querying hundreds of independently operated databases, makes dissecting the provenance of every entry from every data provider effectively impossible. For example, one of the providers, <a href="https://mydisease.info">mydisease.info</a> was more than happy to respond to a query for the outmoded definition of “transsexualism” as a disease <a class="citation" href="#ramTransphobiaEncodedExamination2021">[146]</a> along with a list of genes and variants that supposedly “cause” it - <a href="http://mydisease.info/v1/query?q=%22DOID%3A10919%22">see for yourself</a>. Tracing the source of that entry first leads to the disease ontology <a href="https://web.archive.org/web/20211007053446/https://www.ebi.ac.uk/ols/ontologies/doid/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FDOID_1234">DOID:1234</a> which seems to trace back into an entry in a graph aggregator <a href="http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234">Ontobee</a> (<a href="https://web.archive.org/web/20210923110103/http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234">Archive Link</a>), which in turn lists this <a href="https://github.com/jannahastings/mental-functioning-ontology">github repository</a> <strong>maintained by a single person</strong> as its source<sup id="fnref:ipredit" role="doc-noteref"><a href="#fn:ipredit" class="footnote" rel="footnote">45</a></sup>. This is, presumably, the fragility and inconsistency in input data that the machine learning layer is intended to putty over.</p>

<p>If the graph encodes being transgender as a disease, it is not farfetched to imagine the ranking system attempting to “cure” it. Even if it doesn’t directly, the graph-based nature of the system means that any given entry will have unpredictable consequences on recommendations made from the surrounding network of objects like genes, treatment history, and so on. If the operation of the ranking algorithm is uninterpretable, as most are, or the algorithm it itself proprietary, harmful input data could have long-range influence on both the practice of medicine as well as the course of basic research <em>without anyone being able to tell.</em> The Consortium also describes a system whereby the algorithm is continuously updated based on usage of results in research or clinical practice <a class="citation" href="#consortiumUniversalBiomedicalData2019">[131]</a>, which stands to magnify the problem of algorithmic bias by uncritically treating harmful treatment and research practices as training data.</p>

<p>The approach creates a fundamental tradeoff between algorithmic interpretability and the system being useful at all. The paper cited in the 2021 DILI example as evidence that the system gives plausible results is for a specific subclass of liver injuries caused by anti-tuberculosis drugs <a class="citation" href="#udomsinprasertLeukocyteTelomereLength2020">[147]</a>, highlighting the danger of automated recommendations from noisy data, but also calling into question what novel contribution the Translator made if telomeres were already implicated in DILI. The 2022 report gives examples where the results were already expected by the researchers, or provided a series of papers that seems difficult to imagine being much more informative than a PubMed search. If the algorithmic recommendations are unexpected — ie. the system provides novel information — the process of confirming them appears to be near-identical to the usual process of reading abstracts and hopping citation trees.</p>

<p>Perhaps most worrisome is the eventual fate of the project in the hands of the broader ecosystem of orbiting information conglomerates. Centralized infrastructure projects can be an opportunity for for-profit companies to “dance until the music stops” and then scoop up any remaining technology when the funding dries up (so far roughly <a href="https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear&amp;sort_order=desc">$81.6 million</a> since 2016 for the Translator <a class="citation" href="#RePORTRePORTERBiomedical2021">[148]</a>, and <a href="https://reporter.nih.gov/search/H4LxgMGK9kGw6SeWCom85Q/projects?shared=true">$84.7 million</a> for the discontinued NIH Data Commons pilot which morphed into the STRIDES program). I have little doubt that the scientists and engineers working on the Translator are doing so with the best of intentions — the real question is what happens to it after it’s finished.</p>

<p>Knowledge graphs in particular are promising targets for platform holders. Perhaps the most well known example is Google’s 2010 acquisition of Freebase (via Metaweb) <a class="citation" href="#subramanianGoogleBuysFreebase2010">[149]</a>, a graph of structured data with a wealth of properties for common people, places and things. Google incorporated it into their Knowledge Graph <a class="citation" href="#IntroducingKnowledgeGraph2012">[150]</a> to populate its factboxes and make its search results more semantically aware in its Hummingbird upgrade in 2013, the largest overhaul of its search engine since 2001 <a class="citation" href="#sullivanFAQAllNew2013">[151]</a>, cementing its dominance as a search engine. The connection between swallowing up knowledge organization systems into search engines is not incidental, but reflective of the broader pattern of enclosing basic digital infrastructure behind opaque platforms. Searching has a different set of cognitive expectations than browsing a database: we expect search results to be “best effort,” not necessarily complete or accurate, where when browsing a database it’s relatively clear when information is missing or inaccurate. For products packaged up into search platforms by for-profit companies, <em>it doesn’t have to actually work</em> as long as it seems like it does.</p>

<p>The platformatization of the knowledge graph, along with carefully worded terms of service, is a clean means by which “good enough” results could be jackknifed into an expanded system of biomedical surveillance. Since the algorithm needs continual training, the translator has every incentive to suck up as much personal data as it can<sup id="fnref:personalmedicaldata" role="doc-noteref"><a href="#fn:personalmedicaldata" class="footnote" rel="footnote">46</a></sup>. For-profit platform providers as a rule depend on developing elaborate personal profiles for targeted advertising algorithmically inferred from available data<sup id="fnref:googlepatent" role="doc-noteref"><a href="#fn:googlepatent" class="footnote" rel="footnote">47</a></sup>, that naturally includes diagnosed or inferred disease — a practice they explicitly describe in the patents for the targeting technology<a class="citation" href="#bharatGeneratingUserInformation2005">[152]</a>, have gone to court to defend <a class="citation" href="#SmithFacebookInc2018">[153, 154]</a>, and formed secretive joint projects with healthcare systems to pursue <a class="citation" href="#bourreauGoogleFitbitWill2020">[155]</a>.</p>

<p>So while an algorithmic recommendation tool may have limited use for the basic researchers it was originally intended for, it is likely to be extremely useful for the booming business of “personalized medicine.” Linking biomedical and patient data in a single platform is a natural route towards a multisided market where records management apps are sold to patients, treatment recommendation systems are sold to clinicians, research tools and advertising opportunities are sold to pharmaceutical companies, risk metrics are sold to insurance companies, and so on.</p>

<p>Multiple information conglomerates are poised to capitalize on the translator project. Amazon already has a broad home surveillance portfolio <a class="citation" href="#bridgesAmazonRingLargest2021">[157]</a>, and has been aggressively expanding into health technology <a class="citation" href="#AWSAnnouncesAWS2021">[158]</a> and even literally providing <a href="https://amazon.care/">health care</a> <a class="citation" href="#lermanAmazonBuiltIts2021">[159]</a>, which could be particularly dangerous with the uploading of all scientific and medical data onto AWS with entirely unenforceable promises of data privacy through NIH’s STRIDES program <a class="citation" href="#quinnYouCanTrust2021">[160]</a>.</p>

<p>RELX, parent of Elsevier, is as always the terrifying elephant in the room. In addition to distribution rights for a large proportion of scientific knowledge and a collection of research databases, it also sells a clinical reference platform in ClinicalKey, point of service products for planning patient care with ClinicalPath, medical education tools, and pharmaceutical advertisements designed to look like scientific papers <a class="citation" href="#elsevier360AdvertisingSolutions">[161]</a>, among others <a class="citation" href="#relx2021AnnualReport2021">[162]</a>. It also is explicitly expanding into “clinical decision support applications” <a class="citation" href="#relx2021AnnualReport2021">[162]</a> and recently embedded its medication management product into Apple’s watchOS 9 <a class="citation" href="#appleWatchOSDeliversNew2022">[15]</a>. Subsidiaries in RELX’s “Risk” market segment sell risk profiles to insurance companies based on what they claim to be highly comprehensive profiles of harvested personal data. The Translator infrastructure is a perfect keystone to unify these products: after the NIH fronts the money to develop it and lends the credibility of basic research, RELX can cheaply expand its surveillance apparatus to enhanced medical risk profiles to insurers, priority placement in candidate drug rankings to pharmaceutical companies, and augment its ranking systems for funders and employers to include some proprietary metric of “promisingness” to encourage researchers to follow its research recommendations. This isn’t speculative — it can just strap whatever clinical data Translator gains access to into its <a href="https://www.elsevier.com/solutions/biology-knowledge-graph">existing biomedical knowledge graph</a>.</p>

<p>Even assuming the Translator works perfectly and has zero unanticipated consequences, the development strategy still reflects the inequities that pervade science rather than challenge them. Biopharmaceutical research, followed by broader biomedical research, being immediately and extremely profitable, attracts an enormous quantity of resources and develops state of the art infrastructure, while no similar infrastructure is built for the rest of science, academia, and society.</p>

<p>The eventual form of the Translator follows from a series of decisions centered around the intended universality of the system. From the <a href="https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator">funding statement</a> in 2016, the system was conceptualized as an “informatics platform” intended to “bring together all biomedical and health data types.”  The surrounding background of cloud-based database storage imagined by the Strategic Plan for Data Science immediately constrained the design to consist of APIs that served small quantities of aggregated data, rather than potentially large quantities of raw data. Together with a platform, rather than tool-based approach, a system that allowed individual researchers to link and make sense of the subtlety of own their data was precluded from the start.</p>

<p>From these constraints, the form of the BioLink model comes into focus: high-level classes and logical relationships between them as asserted by a large number of separate knowledge sources. Since the data from each of these sources is heterogeneous, relatively uncurated, and potentially numerous for any given graph-based query, the need for a machine learning layer to make sense of it follows. The conceptualization of BioLink as a universal ontology seems to follow the lineage of the “neat” thought style <a class="citation" href="#poirierTurnScruffyEthnographic2017">[40]</a> that emphasizes “deductive inference through logical rules” <a class="citation" href="#unniBiolinkModelUniversal2022">[133]</a> or otherwise computing derived information from the structure of the knowledge graph rather than browsing the graph itself. Together, these constraints and design logics bring us to the form of the Translator as a graph-based query engine.</p>

<p>The Translator Consortium justifiably takes pride in its social organizing systems <a class="citation" href="#consortiumBiomedicalDataTranslator2019">[163]</a> — coordinating 200 researchers and engineers from dozens of institutions is no small feat. This system of social organization seems to have lent itself towards developing the individual components with an eye for them to be understood by the rest of the <em>consortium</em> rather than with the intention of inviting collaboration from the broader research community<sup id="fnref:intraconsortium" role="doc-noteref"><a href="#fn:intraconsortium" class="footnote" rel="footnote">48</a></sup>. The very notion of a platform indicates that it is something that <em>they build</em> and <em>we use</em>: There is no explicit means for proposing changes to the BioLink model, to pick and choose how answers are ranked or queries are performed, etc. This is broadly true of platform-based scientific tools, especially databases, and contributes to how they <em>feel</em>: they feel disconnected with our work, don’t necessarily help us do it more easily or more effectively, and contributing to them is a burdensome act of charity (if it is possible at all).</p>

<p>Given the real need for <em>some</em> means of combining heterogeneous data from disparate sources, what could have been done differently?</p>

<p>Problematizing the need for a system intended to link <em>all</em> or even <em>most</em> biomedical data in a single mutually coherent system opens the possibility for a very different data linking infrastructure. Perhaps paradoxically, any universal, logically complete schema intended to support algorithmic inference projects a relatively circumscribed group of people for whom it would be useful: nearly all of the publicly described use-cases are oriented around finding new drugs or targets to treat disease, presumably in part because that’s what preoccupies the ontology. Rather than a set of generalizable <em>tools</em> for linking data, the need for universality strongly constrains the form of data that can be represented by the system, and its platform structure constrains its uses to only those imagined by the platform designers. Every infrastructural model is an act of balancing constraints, and prioritizing “all data” seems to imply “for some people.” Who is supposed to be able to upload data? change the ontology? inspect the machine learning model? Who is in charge of what? Who is a knowledge-graph query engine useful for?</p>

<p>Another conceptualization might be building systems for <em>all people</em> that can <em>embed with existing practices</em> and <em>help them do their work</em> which typically involves accessing <em>some data.</em> We can imagine a system designed to integrate data with schemas written in the <em>vernacular</em> of communities of knowledge work. Rather than the dichotomy of one singular database vs. many fragmented and incompatible databases, we can imagine a <em>pluralistic</em> system capable of supporting multiple overlapping and potentially conflicting representations, governable and malleable in local communities of practice. Taking seriously the notion of “translation,” we could stand to learn from linguistics and translation studies: rather than attempting to project the dialects of each subdiscipline into some “true” meta-framework (a decidedly colonial project <a class="citation" href="#shammaTranslationColonialism2018">[164]</a>), we could resist the urge for homogenization and preserve the multiplicity of representation, embracing the imperfection of mappings between heterogeneous representational systems at multiple scales without resigning ourselves to completely isolated incompatibility.</p>

<p>Maybe we don’t <em>want</em> a universal system that presents itself with the authority of truth to be mined and spun off into derivative platforms by information conglomerates. We might abandon the techno-utopianism of a globally consistent schema that supports arbitrary logical inference by acknowledging that those inferences would always be colored by the decisions embedded in the structure of the system, unknowable beneath the shrouding weights of its ranking model.</p>

<p>Instead can we imagine a properly <em>human</em> data infrastructure? One that preserves the seams and imperfections in our representational systems, that is designed to represent precisely the contingency of representation itself? We might start with the propositional nature of links and mappings between formats — that rather than a divine received truth, the relationships between things are contextual and created. We could find grounding in <em>use,</em> that the schemas and mappings between them should arise from the need to link representations within the context of some problem, rather than to resolve their difference.</p>

<p>Picking up the thread of our peer to peer data sharing backbone, we might start to imagine the boistrous multiplicity of an infrastructure based around communication and expression, rather than platformatized perfection.</p>

<h3 id="folk-federation">Folk Federation</h3>

<blockquote>
  <p>Human language thrives when using the same term to mean somewhat different things, but automation does not. <em>Tim Berners-Lee (1999) The Semantic Web</em> <a class="citation" href="#berners-leeSemanticWeb2001">[165]</a></p>
</blockquote>

<blockquote>
  <p>Wittgenstein’s contribution to communism was his robust proof of the proposition that there is no private language, but in our time, privatized languages are everywhere. And not just languages: Images, codes, algorithms, even genes can become private property, and in turn private property shapes what we imagine the limits and possibilities of this information to be. <em>McKenzie Wark (2021) Capital Is Dead: Is This Something Worse?</em> <a class="citation" href="#warkCapitalDeadThis2021">[8]</a></p>
</blockquote>

<p>To structure our p2p data sharing system, we should use <em>Linked Data.</em> Linked data is at once exceptionally simple and deceptively complex, a set of technologies and social histories. In this section we will introduce the notion of linked data, extend it for a p2p context, and then add a twist from <em>federated systems.</em><sup id="fnref:federatedterminology" role="doc-noteref"><a href="#fn:federatedterminology" class="footnote" rel="footnote">49</a></sup> Our goal will be to articulate the foundation for a “protocol of protocols,” a set of minimal operations by which individual people can create, extend, borrow, and collectively build a space of linked folk schemas and ontologies, or <em>folksonomies.</em></p>

<p>When last we left it, we had developed the notion of a p2p system to the point where we had big torrentlike piles of files with a few additional features like versioning and sharded storage. We need to add an additional layer of <em>metadata</em> that exposes information about the contents of each of these file piles. But what is that metadata <em>made of?</em></p>

<p>The core format of linked data is the Resource Document Format (RDF) <a class="citation" href="#klyneRDFConceptsAbstract2014">[166]</a> and its related syntaxes like Turtle <a class="citation" href="#beckettRDFTurtle2014">[167]</a>. Typical hyperlinks are <em>duplet</em> links — linking from the source to the target. The links of linked data are instead <strong>triplet</strong> links that consist of a <strong>subject</strong>, a <strong>predicate</strong> that <em>describes</em> the link, and an <strong>object</strong> that is linked to. Subjects and objects (generally, nodes) have particular types like a number, or a date, or something more elaborate like an <a href="https://schema.org/Airline">Airline</a> or <a href="https://schema.org/Movie">Movie</a> that have particular sets of predicates or properties: eg. a <code class="language-plaintext highlighter-rouge">Movie</code> has a <code class="language-plaintext highlighter-rouge">director</code> property which links to a <code class="language-plaintext highlighter-rouge">Person</code>. A <code class="language-plaintext highlighter-rouge">Person</code> has an <code class="language-plaintext highlighter-rouge">address</code> which links to a <code class="language-plaintext highlighter-rouge">PostalAddress</code>, and so on. Types and properties are themselves defined in <strong>vocabularies</strong> (or, seemingly interchangeably <a class="citation" href="#w3cOntologiesW3C">[168]</a>, ontologies and schemas) by a special subset of RDF schema modeling classes and properties <a class="citation" href="#brickleyRDFSchema2014">[169]</a>. Linked data thus consists of semantically annotated <strong>graphs</strong> of linked nodes<sup id="fnref:dlg" role="doc-noteref"><a href="#fn:dlg" class="footnote" rel="footnote">50</a></sup>.</p>

<div class="draft-text">
  Put a little vocab box here.
</div>

<p>Linked data representations are very general and encompass many others like relational <a class="citation" href="#berners-leeRelationalDatabasesSemantic2009">[170]</a> and object-oriented models, but have a few properties that might be less familiar. The first is that triplet links have the status of an utterance or a proposition: much like typical duplet hyperlinks, anyone can make whatever links they want to a particular object to say what they’d like about it. As opposed to object-oriented models where a class is defined beforehand and its attributes or data are stored “within” the object, RDF schemas are composed of links just like any other, and the link, object, and predicate can all be stored in separate places by different people <a class="citation" href="#berners-leeWhatSemanticWeb1998">[171]</a>. For example:</p>

<blockquote>
  <p>One person may define a <code class="language-plaintext highlighter-rouge">vehicle</code> as having a <code class="language-plaintext highlighter-rouge">number of wheels</code> and a <code class="language-plaintext highlighter-rouge">weight</code> and a <code class="language-plaintext highlighter-rouge">length</code>, but not foresee a <code class="language-plaintext highlighter-rouge">color</code>. This will not stop another person making the assertion that a given car is red, using the color vocabulary from elsewhere. <a class="citation" href="#berners-leeWhatSemanticWeb1998">[171]</a></p>
</blockquote>

<p>Linked data has an ambivalent history of thought regarding the location and distribution of ontology building. Its initial formulation came fresh from the recent incendiary success of the internet, where without any system of organization “people were frightened of getting lost in it. You could follow links forever.” <a class="citation" href="#berners-leeWhatSemanticWeb1998">[171]</a> Linked data was conceptualized to be explicitly without authoritative ontologies, but intended to evolve like language with local cultures of meaning meshing and separating at multiple scales <a class="citation" href="#berners-leeSemanticWeb2001">[165]</a>. Perhaps one of the pieces that went missing when moving between writing about the semantic web and its realization in standards and protocols is that this language-like conception of links requires <strong>quartet,</strong> rather than triplet links: <strong>author</strong>, subject, object, predicate. The author is encoded implicitly in the source of the vocabulary: “Users are given […] a single URI […] for each persona they want to have,” <a class="citation" href="#berners-leeSociallyAwareCloud2009">[23]</a> so theoretically ontologies have the status of “schema.org says this.” Without a first-class notion of author in the links themselves there is little means of “forking” a vocabulary, or having multiple versions of a term with the same name but different authors.</p>

<p>The dream of mass automaticity, however, with computational “agents” capable of seamlessly crawling consistent graphs of linked data to extract surplus meaning necessarily requires that the meaning of terms does not “mutate” between different uses. For many early linked data architects the resolution was more automation, to use additional semantic structure about the equivalence between different ontologies as a means of estimating how trustworthy a particular result was. This tension is sewn into one of its most well known ontologies, the Simple Knowledge Organization System (skos) <a class="citation" href="#brickleySKOSCoreGuide2005">[172]</a>, which is intended to represent relationships between terms and vocabularies <a class="citation" href="#milesQuickGuidePublishing2005">[173]</a>.</p>

<p>The fluidity of the original vision for linked data never emerged, however, and is remembered instead as being monstrously overcomplicated <a class="citation" href="#palmerDitchingSemanticWeb2008">[39, 174]</a>. While HTML, CSS, and Javascript developed a rich ecosystem of abstractions that let people create websites without directly writing HTML, the same never materialized for RDF. While linked data entities are intended to be designated by the very general notion of a URI, in practice URIs are near-synonymous with URLs, and maintaining a set of URLs is hard. In the absence of interfaces for manipulating linked data and the pain of hosting them, the dream of a distributed negotiation over language-like ontologies was largely confined to information scientists and what became corporate knowledge graphs. For those war-weary RDF vets, I will again clarify that we are describing the desirable <em>qualities</em> of RDF while trying to learn from its failures.</p>

<p>In our revival of this dream we are describing a system where heterogeneous data is indicated by its metadata, rather than representing all data in a uniform format — similarly to the mixture of RDF and non-RDF data in the linked data platform standard <a class="citation" href="#speicherLinkedDataPlatform2015">[175]</a>. We want to handle a broad span of heterogeneity: data with different naming schemes, binary representations, sizes, nested structures, and so on. The first task is to describe some means of accessing this heterogeneous data in a reasonably standard way despite these differences.</p>

<p>While that may seem a tall order, researchers already do it, it’s just mostly done manually whenever we want to use anyone else’s data. One way of characterizing the task at hand is systematizing the idiosyncratic paths by which a researcher might dump out a .csv file from a sql database to load into MATLAB to save in the .mat format with the rest of their data. To do that we can draw from a parallel body of thought on <em>federated databases.</em></p>

<p>Like our p2p system, federated systems consist of <em>distributed</em>, <em>heterogeneous</em>, and <em>autonomous</em> agents that implement some minimal agreed-upon standards for mutual communication and (co-)operation. Federated databases were proposed in the early 1980’s <a class="citation" href="#heimbignerFederatedArchitectureInformation1985">[176]</a> and have been developed and refined in the decades since as an alternative to either centralization or non-integration <a class="citation" href="#litwinInteroperabilityMultipleAutonomous1990">[177, 178, 179]</a>. Their application to the dispersion of scientific data in local filesystems is not new <a class="citation" href="#busseFederatedInformationSystems1999">[180, 181, 182]</a>, but their implementation is more challenging than imposing order with a centralized database or punting the question into the unknowable maw of machine learning.</p>

<p>Amit Sheth and James Larson, in their reference description of federated database systems, describe <strong>design autonomy</strong> as one critical dimension that characterizes them:</p>

<blockquote>
  <p>Design autonomy refers to the ability of a component DBS to choose its own design with respect to any matter, including</p>

  <ul>
    <li>(a) The <strong>data</strong> being managed (i.e., the Universe of Discourse),</li>
    <li>(b) The <strong>representation</strong> (data model, query language) and the <strong>naming</strong> of the data elements,</li>
    <li>(c) The conceptualization or <strong>semantic interpretation</strong> of the data (which greatly contributes to the problem of semantic heterogeneity),</li>
    <li>(d) <strong>Constraints</strong> (e.g., semantic integrity constraints and the serializability criteria) used to manage the data,</li>
    <li>(e) The <strong>functionality</strong> of the system (i.e., the operations supported by system),</li>
    <li>(f) The <strong>association and sharing with other systems</strong>, and</li>
    <li>(g) The <strong>implementation</strong> (e.g., record and file structures, concurrency control algorithms).</li>
  </ul>
</blockquote>

<p>Susanne Busse and colleagues add an additional dimension of <strong>evolvability,</strong> or the ability of a particular system to adapt to inevitable changing uses and requirements <a class="citation" href="#busseFederatedInformationSystems1999">[180]</a>.</p>

<p>In order to support such radical autonomy and evolvability, federated systems need some means of translating queries and representations between heterogeneous components. The typical conceptualization of federated databases have five layers that implement different parts of this reconciliation process <a class="citation" href="#shethFederatedDatabaseSystems1990">[183]</a>:</p>

<ul>
  <li>A <strong>local schema</strong> is the representation of the data on local servers, including the means by which they are implemented in binary on the disk</li>
  <li>A <strong>component schema</strong> serves to translate the local schema to a format that is compatible with the larger, federated schema</li>
  <li>An <strong>export schema</strong> defines permissions, and what parts of the local database are made available to the federation of other servers</li>
  <li>The <strong>federated schema</strong> is the collection of export schemas, allowing a query to be broken apart and addressed to different export schemas. There can be multiple federated schemas to accomodate different combinations of export schemas.</li>
  <li>An <strong>external schema</strong> can further be used to make the federated schema better available to external users, but in this case since there is no notion of “external” it is less relevant.</li>
</ul>

<p>This conceptualization provides a good starting framework and isolation of the different components of a database system, but a peer-to-peer database system has different constraints and opportunities <a class="citation" href="#bonifatiDistributedDatabasesPeertopeer2008">[184]</a>. In the strictest, “tightly coupled” federated systems, all heterogeneity in individual components has to be mapped to a single, unified federation-level schema. Loose federations don’t assume a unified schema, but settle for a uniform query language, and allow multiple translations and views on data to coexist. A p2p system naturally lends itself to a looser federation, and also gives us some additional opportunities to give peers agency over schemas while also preserving some coherence across the system. I will likely make some database engineers cringe, but the emphasis for us will be more on building a system to support distributed social control over the database, rather than guaranteeing consistency and transparency between the different components.</p>

<p>Let us take the notion of a loosely coupled systems to its extreme, and invert the meaning of federation as it is used in other systems like ActivityPub: rather than a server-first federation, where peers create accounts on servers that define their operation and the other servers they federate with, ours will be peer-first federation. In this system, individual peers will maintain their own vocabularies and be able to make them available to other peers. Peers can directly connect to one another, but can also federate into groups, which can federate into groups of groups, and so on. A peer will implement the local, component, and export schema with a client that handles requests for vocabularies and and datasets according to their scheme of permissions. Translation from a metadata-based query to a particular binary representation of a file, whether it be in a relational database, binary, file, or otherwise, will also be supported by vocabularies that indicate the necessary code.</p>

<p>Clearly, we need some form of <em>identity</em> in the system so that a peer can have their links unambiguously identified and discovered. This is a challenging problem that we leave open here, but strategies ranging from URI-based resolution like <code class="language-plaintext highlighter-rouge">username@domain.com</code>, to locally-held cryptographic key based identity, to decentralized systems like the w3c’s Decentralized Identifiers <a class="citation" href="#spornyDecentralizedIdentifiersDIDs2021">[185]</a> would suffice. For the sake of example, let’s make identity simple and flat, denoted in pseudocode as <code class="language-plaintext highlighter-rouge">@username</code>. Someone would then be able to use their <code class="language-plaintext highlighter-rouge">@name</code>space as a root, under which they could refer to their data, schemas, and so on, which will be denoted <code class="language-plaintext highlighter-rouge">@name:subobject</code> (see this notion of personal namespaces for knowledge organization discussed in early wiki culture here <a class="citation" href="#MeatballWikiPersonalCategories">[186]</a>). Let us also assume that there is no categorical difference between <code class="language-plaintext highlighter-rouge">@usernames</code> used by individual researchers, institutions, consortia, etc. — everyone is on the same level.</p>

<p>To illustrate the system by example, we pick up where we left off earlier with a peer who has their data in some discipline-specific format, which let us assume for the sake of concreteness has a representation as an <a href="https://www.w3.org/OWL/">OWL</a> schema.</p>

<p>That schema could be “owned” by the <code class="language-plaintext highlighter-rouge">@username</code> corresponding to the standard-writing group — eg <code class="language-plaintext highlighter-rouge">@nwb</code> for neurodata without borders. In all the following examples, we will use a <a href="https://www.w3.org/TR/turtle/">turtle-ish</a> syntax that is <em>purposely pseudocode</em> with the intention of demonstrating general qualities without being concerned with syntactic correctness or indicating one syntax in particular. Our dataset might look like this:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">@base</span><span class="w"> </span><span class="na">@jonny</span><span class="w">

</span><span class="nl">&lt;#my-data&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@nwb:NWBFile</span><span class="w">
  </span><span class="na">@nwb:general:experimenter</span><span class="w"> </span><span class="na">@jonny</span><span class="w">
  </span><span class="na">@nwb:ElectricalSeries</span><span class="w">
    </span><span class="p">.</span><span class="n">electrodes</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">]</span><span class="w">
    </span><span class="p">.</span><span class="n">rate</span><span class="w"> </span><span class="mi">30000</span><span class="w">
    </span><span class="p">.</span><span class="n">data</span><span class="w"> </span><span class="p">[...]</span><span class="w">
</span></code></pre></div></div>

<p>Unpacking the pseudocode, this indicates:</p>

<ul>
  <li>We declare a <code class="language-plaintext highlighter-rouge">@base</code> context underneath my identity, <code class="language-plaintext highlighter-rouge">@jonny</code>,</li>
  <li>Underneath the base, individual objects are declared with their name like <code class="language-plaintext highlighter-rouge">&lt;#object-name&gt;</code>, a shorthand for <code class="language-plaintext highlighter-rouge">&lt;@base:object-name&gt;</code>. In this case I have made a dataset identified as <code class="language-plaintext highlighter-rouge">@jonny:my-data</code>.</li>
  <li>I have identified the type of this object with the <code class="language-plaintext highlighter-rouge">a</code> token, in this case a <code class="language-plaintext highlighter-rouge">@nwb:NWBFile</code></li>
  <li>Subsequent lines indicate particular properties of the indicated type and their value, specifically I have indicated that the <code class="language-plaintext highlighter-rouge">@nwb:general:experimenter</code> is me, <code class="language-plaintext highlighter-rouge">@jonny</code>, and that the dataset also contains a <code class="language-plaintext highlighter-rouge">@nwb:ElectricalSeries</code>. While my identity object might have additional links like an <code class="language-plaintext highlighter-rouge">@ORCID:ID</code>, we can assume some basic inference that resolves my identity to a string as specified in the NWB specification, or else specify it explicitly as <code class="language-plaintext highlighter-rouge">@jonny:name</code></li>
  <li>Additional subproperties are assigned with a leading <code class="language-plaintext highlighter-rouge">.</code>, so <code class="language-plaintext highlighter-rouge">.electrodes</code> would resolve to <code class="language-plaintext highlighter-rouge">@nwb:ElectricalSeries:electrodes</code>.</li>
</ul>

<p>How would my client know how to read and write the data to my disk so i can use and share it? In a system with heterogeneous data types and database implementations, we need some means of specifying different programs to use to read and write, different APIs, etc. This too can be part of the format specification. Suppose the HDF5 group (or anyone, really!) has a namespace <code class="language-plaintext highlighter-rouge">@hdf</code> that defines the properties of an <code class="language-plaintext highlighter-rouge">@hdf:HDF5</code> file, basic operations like <code class="language-plaintext highlighter-rouge">Read</code>, <code class="language-plaintext highlighter-rouge">Write</code>, or <code class="language-plaintext highlighter-rouge">Select</code>. NWB could specify that in their definition of <code class="language-plaintext highlighter-rouge">@nwb:NWBFile</code>:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;@nwb:NWBFile&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@hdf:HDF5</span><span class="w">
    </span><span class="p">.</span><span class="n">isVersion</span><span class="w"> </span><span class="s">"x.y.z"</span><span class="w">
    </span><span class="p">.</span><span class="n">hasDependency</span><span class="w"> </span><span class="s">"libhdf5"</span><span class="n">==</span><span class="s">"x.y.z"</span><span class="w">
  </span><span class="n">usesContainer</span><span class="w"> </span><span class="na">@nwb:NWBContainer</span><span class="w">
</span></code></pre></div></div>

<p>So when I receive a request for the raw data of my electrical series, my client knows to use the particular methods from the HDF5 object type to index the data contained within the file.</p>

<p>I have some custom field for my data, though, which I extend the format specification to represent. Say I have invented some new kind of solar-powered electrophysiological device — the SolarPhys2000 — and want to annotate its specs alongside my data.</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#SolarEphys&gt;</span><span class="w">
  </span><span class="n">extends</span><span class="w"> </span><span class="na">@nwb:NWBContainer</span><span class="w">
    
  </span><span class="n">UsedWith</span><span class="w"> </span><span class="na">@jonny:hw:SolarPhys2000</span><span class="w">

  </span><span class="n">ManufactureDate
</span><span class="w">    </span><span class="k">a</span><span class="w"> </span><span class="na">@schema:Date</span><span class="w">

  </span><span class="n">InputWattageSeries
</span><span class="w">    </span><span class="n">extends</span><span class="w"> </span><span class="na">@nwb:ElectricalSeries</span><span class="w">

    </span><span class="n">sunIntensity
</span><span class="w">      </span><span class="k">a</span><span class="w"> </span><span class="na">@nwb:TimeSeries</span><span class="w">
</span></code></pre></div></div>

<p>Here I create a new extension <code class="language-plaintext highlighter-rouge">@jonny:SolarEphys</code> that <code class="language-plaintext highlighter-rouge">extends</code> the <code class="language-plaintext highlighter-rouge">@nwb:NWBContainer</code> schema. We use <code class="language-plaintext highlighter-rouge">extends</code> rather than <code class="language-plaintext highlighter-rouge">a</code> because we are adding something new to the <em>description</em> of the container rather than <em>making</em> a container to store data. I declare that this container is <code class="language-plaintext highlighter-rouge">UsedWith</code> our SolarPhys2000 which we have defined elsewhere in our <code class="language-plaintext highlighter-rouge">hw</code> namespace using some hardware ontology. I then add two new fields, <code class="language-plaintext highlighter-rouge">ManufactureDate</code> and <code class="language-plaintext highlighter-rouge">InputWattageSeries</code>, declaring types from, for example <a href="https://schema.org/Date"><code class="language-plaintext highlighter-rouge">@schema:Date</code></a> and <code class="language-plaintext highlighter-rouge">@nwb</code>.</p>

<p>The abstraction around the file implementation makes it easier for others to consume my data, but it also makes it easier for <em>me</em> to use and contribute to the system. Making an extension to the schema wasn’t some act of charity, it was the most direct way for me to use the tool to do what I wanted. Win-win: I get to use my fancy new instrument and store its data by extending some existing format standard. We are able to make my work part of a cumulative schema building effort by <em>aligning the modalities of use and contribution.</em></p>

<p>For the moment our universe is limited only to other researchers using NWB. Conveniently, the folks at NWB have set up a federating group so that everyone who uses it can share their format extensions. In the same way that we can use schemas to refer to code as with our HDF5 files, we can use it to indicate the behavior of clients and federations. Say we want to make a federating peer that automatically <code class="language-plaintext highlighter-rouge">Accept</code>s request to <code class="language-plaintext highlighter-rouge">Join</code> and indexes any schema that inherits from their base <code class="language-plaintext highlighter-rouge">@nwb:NWBContainer</code>. Let’s say <code class="language-plaintext highlighter-rouge">@fed</code> defines some basic properties of our federating system — it constitutes our federating “protocol” — and loosely use some terms from the <a href="https://www.w3.org/ns/activitystreams#class-definitions">ActivityStreams</a> vocabulary as <code class="language-plaintext highlighter-rouge">@as</code></p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;@nwbFederation&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@fed:Federation</span><span class="w">
  </span><span class="n">onReceive
</span><span class="w">    </span><span class="na">@as:Join</span><span class="w"> </span><span class="na">@as:Accept</span><span class="w">
  </span><span class="n">allowSchema
</span><span class="w">    </span><span class="n">extensionOf</span><span class="w"> </span><span class="na">@nwb:NWBContainer</span><span class="w">
</span></code></pre></div></div>

<p>Now anyone that is a part of the <code class="language-plaintext highlighter-rouge">@nwbFederation</code> would be able to see the schemas we have submitted, sort of like a beefed up, semantically-aware version of the existing <a href="https://nwb-extensions.github.io/">neurodata extensions catalog</a>. In this system, many overlapping schemas could exist simultaneously under different namespaces, but wouldn’t become a hopeless clutter because similar schemas could be compared and reconciled based on their semantic properties.</p>

<p>Now that I’ve got my schema extension written and submitted to the federation, time to submit my data! Since it’s a p2p system, I don’t need to manually upload it, but I do want to control who gets it. By default, I have all my NWB datasets set to be available to the <code class="language-plaintext highlighter-rouge">@nwbFederation</code> , and I list all my metadata on, say the Society for Neuroscience’s <code class="language-plaintext highlighter-rouge">@sfnFederation</code>.</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#globalPermissions&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@fed:Permissions</span><span class="w">
  </span><span class="n">permissionsFor</span><span class="w"> </span><span class="na">@jonny</span><span class="w">

  </span><span class="n">federatedWith</span><span class="w"> 
    </span><span class="n">name</span><span class="w"> </span><span class="na">@nwbFederation</span><span class="w">
    </span><span class="na">@fed:shareData</span><span class="w"> 
      </span><span class="n">is</span><span class="w"> </span><span class="na">@nwb:NWBFile</span><span class="w">

  </span><span class="n">federatedWith
</span><span class="w">    </span><span class="n">name</span><span class="w"> </span><span class="na">@sfnFederation</span><span class="w">
    </span><span class="na">@fed:shareMetadata</span><span class="w">
</span></code></pre></div></div>

<p>Let’s say this dataset in particular is a bit sensitive — say we apply a set of permission controls to be compliant with <code class="language-plaintext highlighter-rouge">@hhs.HIPAA</code> — but we do want to make use of some public server space run by our Institution, so we let it serve an encrypted copy that those I’ve shared it with can decrypt.</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#datasetPermissions&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@fed:Permissions</span><span class="w">
  </span><span class="n">permissionsFor</span><span class="w"> </span><span class="na">@jonny:my-data</span><span class="w">

  </span><span class="n">accessRuleset</span><span class="w"> </span><span class="na">@hhs:HIPAA</span><span class="w">
    </span><span class="p">.</span><span class="n">authorizedRecipient</span><span class="w"> </span><span class="nl">&lt;#hash-of-patient-ids&gt;</span><span class="w">
  
  </span><span class="n">federatedWith
</span><span class="w">    </span><span class="n">name</span><span class="w"> </span><span class="na">@institutionalCloud</span><span class="w">
    </span><span class="na">@fed:shareEncrypted</span><span class="w">
</span></code></pre></div></div>

<p>Now I want to make use of some of my colleagues data. Say I am doing an experiment with a transgenic dragonfly and collaborating with a chemist down the hall. This transgene, known colloquially in our discipline as <code class="language-plaintext highlighter-rouge">@neuro:superstar6</code> (which the chemists call <code class="language-plaintext highlighter-rouge">@chem:SUPER6</code>) fluoresces when the dragonfly is feeling bashful, and we have plenty of photometry data stored as <code class="language-plaintext highlighter-rouge">@nwb:Fluorescence</code> objects. We think that its fluorescence is caused by the temperature-dependent conformational change from blushing. They’ve gathered NMR and Emission spectroscopy data in their chemistry-specific format, say <code class="language-plaintext highlighter-rouge">@acs:NMR</code> and <code class="language-plaintext highlighter-rouge">@acs:Spectroscopy</code>.</p>

<p>We get tired of having our data separated and needing to maintain a bunch of pesky scripts and folders, so we decide to make a bridge between our datasets. We need to indicate that our different names for the gene are actually the same thing and relate the spectroscopy data.</p>

<p>Let’s make the link explicit, say we use an already-existing vocabulary like the “simple knowledge organization system” for describing logical relationships between concepts: <a href="https://www.w3.org/2009/08/skos-reference/skos.html"><code class="language-plaintext highlighter-rouge">@skos</code></a>?</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#links:super6&gt;</span><span class="w">
  </span><span class="na">@neuro:superstar6</span><span class="w">
    </span><span class="na">@skos:exactMatch</span><span class="w"> </span><span class="na">@chem:SUPER6</span><span class="w">
</span></code></pre></div></div>

<p>Our <code class="language-plaintext highlighter-rouge">@nwb:Fluorescence</code> data has the emission wavelength in its <code class="language-plaintext highlighter-rouge">@nwb:Fluorescence:excitation_lambda</code> property<sup id="fnref:notreallynwb" role="doc-noteref"><a href="#fn:notreallynwb" class="footnote" rel="footnote">51</a></sup>, which is the value of their <code class="language-plaintext highlighter-rouge">@acs:Spectroscopy</code> data at a particular value of its <code class="language-plaintext highlighter-rouge">wavelength</code>. Unfortunately, <code class="language-plaintext highlighter-rouge">wavelength</code> isn’t metadata for our friend, but does exist as a column in the <code class="language-plaintext highlighter-rouge">@acs:Spectroscopy:readings</code> table, so where we typically have a singular value they have a set of measurements. Since the same information has a structurally different meaning across disciplines, we dont expect there to be an automated 1:1 mapping between them, but presumably their data format also specifies some means of reading the data akin to the HDF5 methods indicated by our NWB data format so we can add an additional translation later like <code class="language-plaintext highlighter-rouge">@math:mean</code> and pick it up in our analysis tools.</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#links:lambda&gt;</span><span class="w">
  </span><span class="na">@acs:Spectroscopy:readings:wavelength</span><span class="w">
    </span><span class="na">@skos:narrowMatch</span><span class="w"> </span><span class="na">@nwb:Fluorescence:excitation_lambda</span><span class="w">
      </span><span class="na">@skos:note</span><span class="w">
        </span><span class="s">"Multiple spectrographic readings are
        aggregated to a single excitation lambda"</span><span class="w">
      </span><span class="na">@translate:aggregate</span><span class="w"> </span><span class="na">@math:mean</span><span class="w">
</span></code></pre></div></div>

<p>This makes it much easier for us to index our data against each other and solves a few real practical problems we were facing in our collaboration. We don’t need to do as much cleaning when it’s time to publish the data since it can be released as a single linked entity.</p>

<p>Though this example is relatively abstract (which metadata from spectroscopy readings would need to match which in a fluorescence series to compare wavelengths to lambda?), it serves as an example in its own right of the quasi-inversion of reasoning that we can make use of in our particular version of linked data with code. We refer to the general notion of taking a <code class="language-plaintext highlighter-rouge">@math:mean</code>, but don’t specify a particular implementation of it. Other package maintainers could indicate that their function implements it, so we could be prompted to choose one when resolving the link. Alternatively, if we specified our aggregation used <code class="language-plaintext highlighter-rouge">@numpy:mean</code>, we could trace it backwards to find which general operation it implements and choose a different one. Since the objects of any triplet link have their own type, we can use the <em>context</em> of the link to infer how to use it.</p>

<p>Rinse and repeat our sharing and federating process from our previous schema extension, add a little bit of extra federation with the <code class="language-plaintext highlighter-rouge">@acs</code> namespace, and in the normal course of our doing our research we’ve contributed to the graph structure linking two common data formats. Our link is one of many, and is a proposition that other researchers can evaluate in the context of our project rather than as an authoritative reference link. We might not have followed the exact rules, but we have also changed the nature of rules — rather than logical coherence guaranteed <em>a priori</em> by adherence to a specification language, much like language the only rules that matter are those of <em>use.</em> We may have only made a few links rather than a single authoratative mapping, but if someone is interested in compiling one down the line they’ll start off a hell of a lot further than if we hadn’t contributed it! Rather than this format translation happening ad-hoc across a thousand lab-specific analysis libraries, we have created a space of <em>discourse</em> where our translation can be contextually compared to others and negotiated by the many people concerned, rather than handed down by a standards body.</p>

<p>Queries across what amounts to the federated schema, in the federated database parlance, are by design less seamless than they would be with centrally governed schema — which is a feature, not a bug. While this example deals with relatively dry fluorescence and spectrographic data, if this system were to expand to clinical, cultural, and personal data, the surveillance economy that emerged subsequent to they heydey of the semantic web has made it abundantly clear that <em>we don’t necessarily want</em> arbitrary actors to be able to index across all available data. It is much more valuable to have low-barrier, vernacular expression usable by collections of subdisciplines and communities of people than a set of high-barrier, fixed, logically correct schemas. Researchers and people alike typically are only concerned with using the information within or a few hops outside of their local systems of meaning, so who is a totalizing database of everything <em>for?</em> This framing of linked data, by rejecting the goal of global inference altogether, could be considered beyond even Lindsay Poirier’s conception of “scruffiness” to something we might properly call <em>vulgar linked data.</em></p>

<p>The act of translation is always an act of creation, and by centering the multiplicity of links between extensible schemas we center the dialogic reality of that creation: <em>who says</em> those things are equivalent? Since the act of using translating links between schemas itself creates links — ie. I link to <code class="language-plaintext highlighter-rouge">@&lt;user&gt;</code>’s link to link my dataset and another — we are both able to assess the status of consensus around which links are used, as well as bring a currently invisible form of knowledge work into a system of credit. As we will develop in the following two sections, this multiplicity also naturally lends itself to a fluid space of tools that implement translations and analyses, as well as a means of discussing and contextualizing the results.</p>

<p>We have been intentionally vague about the technical implementation here, but there are many possible strategies and technologies for each of the components.</p>

<p>For making our peers and the links within their namespace discoverable we could use a distributed hash table, or <a href="https://en.wikipedia.org/wiki/Distributed_hash_table"><strong>DHT</strong></a>, like bittorrent, which distributes references to information across a network of peers (eg. <a class="citation" href="#pirroDHTbasedSemanticOverlay2012">[187]</a>). We could use a strategy like the <a href="https://matrix.org/"><strong>Matrix</strong> messaging protocol</a>, where peers could federate with “relay” servers. Each server is responsible for keeping a synchronized copy of the messages sent on the servers and rooms it’s federated with, and each server is capable of continuing communication if any of the others failed. We could use <a href="https://www.w3.org/TR/2018/REC-activitypub-20180123/"><strong>ActivityPub</strong> (AP)</a> <a class="citation" href="#Webber:18:A">[188]</a>, a publisher-subscriber model where users affiliated with a server post messages to their ‘outbox’ and are sent to listening servers (or made available to HTTP GET requests). AP uses <a href="https://json-ld.org/">JSON-LD</a> <a class="citation" href="#spornyJSONLDJSONbasedSerialization2020">[189]</a>, so is already capable of representing linked data, and the related ActivityStreams vocabulary <a class="citation" href="#snellActivityStreams2017">[190]</a> also has plenty of relevant <a href="https://www.w3.org/TR/activitystreams-vocabulary/#activity-types">action types</a> for <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-create">creating</a>, <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-question">discussing</a>, and <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-tentativeaccept">negotiating</a> over links (also see <a href="https://github.com/openEngiadina/cpub">cpub</a>). We could use a strategy like IPFS where peers will voluntarily rehost each other’s data in order to gain trust with one another. To preserve interoperability with existing systems, we will want to make links referenceable from a URI (as IPFS does) as well as be able to resolve multiple protocols, but beyond that the space of possible technologies is broad.</p>

<p>Indexing and querying metadata across federated peers could make use of the <a href="https://www.w3.org/TR/sparql11-federated-query/">SPARQL</a> query language <a class="citation" href="#SPARQLFederatedQuery2013">[191]</a> as has been proposed for biology many times before <a class="citation" href="#simaEnablingSemanticQueries2019">[192, 181, 182]</a>. The distinction between metadata and data is largely practical — a query shouldn’t require transferring and translating terabytes of data — so we will need some means of resolving references to data from metadata as per the linked data platform specification <a class="citation" href="#speicherLinkedDataPlatform2015">[175]</a>. A mutable/changeable/human-readable name and metadata system that points to a system of unique <a href="https://en.wikipedia.org/wiki/Content-addressable_storage">content addressed</a> identifiers has been one need that has hobbled IPFS, and is the direction pointed to by DataLad<sup id="fnref:p2pdatalad" role="doc-noteref"><a href="#fn:p2pdatalad" class="footnote" rel="footnote">52</a></sup> <a class="citation" href="#hankeDefenseDecentralizedResearch2021">[193]</a>. A <a href="https://mastodon.social/@humanetech/107155144840782386">parallel</a> 
<a href="https://web.archive.org/web/20211024082055/https://socialhub.activitypub.rocks/t/which-links-between-activitypub-and-solid-project/529">set</a> of <a href="https://web.archive.org/web/20211024080845/https://socialhub.activitypub.rocks/t/how-solid-and-activitypub-complement-each-other-best/727">conversations</a> has been <a href="https://web.archive.org/web/20211024081238/https://forum.solidproject.org/t/discussion-solid-vs-activitypub/2685">happening</a> in the broader linked data community with regard to using ActivityPub as a way to index data on Solid.</p>

<p>The design of federations of peers is intended to resolve several of the problems of prior p2p protocols. Rather than a separate swarm for every dataset per bittorrent, or a single global swarm per IPFS, this system would be composed of peers that can voluntarily associate and share metadata structure at multiple scales. Bittorrent requires trackers to aggregate and structure metadata, but they become single points of failure and often function as means of gatekeeping by the beloved petty tyrants who host them. IPFS has turned to <a href="https://filecoin.io/">filecoin</a> to incentivize donating storage space among quasi-anonymous peers, a common design pattern among the radical zero-trust design of many cryptocurrencies and cryptocurrency-like systems.</p>

<p>Voluntary federations are instead explicitly social systems that can describe and organize their own needs: peers in a federation can organize tracker or serverlike re-hosting of their data for performance, discoverability, guaranteed longevity. A federation can institute a cooperative storage model akin to private bittorrent trackers that requires a certain amount of rehosted data per data shared. A small handful of researchers can form a small federation to share data while collaborating on a project in the same way that a massive international consortioum could. Without enumerating their many forms, federations can be a way to realize the evolvable community structure needed for sustained archives. As may become clearer as we discuss systems for communication, in the context of science they might be a way of reconceptualizing scientific societies as something that supports the practice of science beyond their current role as ostensibly nonprofit journal publishers and event hosts.</p>

<p>So far we have described a system for sharing data with a p2p system integrated with linked data. We have given a few brief examples of how linked data can be used for standardized and vernacular metadata, integrating with heterogeneous local storage systems, and to perform actions like creating and joining federations of peers. As described, though, the system would still be decidedly unapproachable for most scientists and doesn’t offer the kind of strong incentives that would create a broad base of use. We clearly need one or several <em>interfaces</em> to make the creation and use metadata easy. We will return to those in <a href="#shared-knowledge">Shared Knowledge</a> and also describe a set of communication and governance systems sorely needed in science. To get there, we will first turn to a means of integrating our shared data system with analytical and experimental tools to make each combinatorically more useful than if considered alone.</p>

<h2 id="shared-tools">Shared Tools</h2>

<p>Straddling our system for sharing data are the tools to gather and analyze it — combining tools to address the general need for <em>storage</em> with <em>computational resources.</em> Considering them together presents us with new opportunities only possible with cross-domain interoperability. In particular, we can ask how a more broadly integrated system makes each of the isolated components more powerful, enables a kind of deep provenance from experiment to results, and further builds us towards reimagine the form of the community and communication tools for science. Where the previous section focused on integrating linked metadata with data, here our focus is how to make linked data <em>do things</em> by integrating it with code.</p>

<p>This section will be relatively short compared to <a href="#shared-data">shared data</a>. We have already introduced, motivated, and exemplified many of the design practices of the broader infrastructural system. There is much less to argue against or “undo” in the spaces of analytical and experimental tools because so much more work has been done, and so much more power has been accrued in the domain of data systems. Distributed computing does have a dense history, with huge numbers of people working on the problem, but its dominant form is much closer to the system articulated below than centralized servers are to federated semantic p2p systems. I also have written extensively about <a href="#experimental-frameworks">experimental frameworks</a> before <a class="citation" href="#saundersAutopilotAutomatingBehavioral2019">[194]</a>, and develop <a href="https://docs.auto-pi-lot.com/en/latest/">one of them</a> so I will be brief at risk of repeating myself or appearing self-serving.</p>

<p>Integrated scientific workflows have been written about many times before, typically in the context of the “open science” movement. One of the founders of the Center for Open Science, Jeffrey Spies, described a similar ethic of toolbuilding as I have in a 2017 presentation:</p>

<blockquote>
  <p>Open Workflow:</p>
  <ol>
    <li>Meet users where they are</li>
    <li>Respect current incentives</li>
    <li>Respect current workflow</li>
  </ol>

  <ul>
    <li>We could… demonstrate that it makes research more efficient, of higher quality, and more accessible.</li>
    <li>Better, we could… demonstrate that researchers will get published more often.</li>
    <li>Even better, we could… make it easy.</li>
    <li>Best, we could… make it automatic <a class="citation" href="#spiesWorkflowCentricApproachIncreasing2017">[195]</a></li>
  </ul>
</blockquote>

<p>Similar to the impossibility of a single unified data format, it is unlikely that we will develop one tool to rule them all. We will take the same tactic of thinking about <em>frameworks</em> to integrate tools and make them easier to build, rather than building any tool in particular.</p>

<h3 id="analytical-frameworks">Analytical Frameworks</h3>

<p>The first natural companion of shared data infrastructure is a shared analytical framework. A major driver for the need for everyone to write their own analysis code largely from scratch is that it needs to account for the idiosyncratic structure of everyone’s data. Most scientists are (blessedly) not trained programmers, so code for loading and loading data is often intertwined with the code used to analyze and plot it. As a result it is often difficult to repurpose code for other contexts, so the same analysis function is rewritten in each lab’s local analysis repository. Since sharing raw data and code is still a (difficult) novelty, on a broad scale this makes results in scientific literature as reliable as we imagine all the private or semi-private analysis code to be.</p>

<p>Analytical tools (anecdotally) make up the bulk of open source scientific software, and range from foundational and general-purpose tools like numpy <a class="citation" href="#harrisArrayProgrammingNumPy2020">[196]</a> and scipy <a class="citation" href="#virtanenSciPyFundamentalAlgorithms2020">[197]</a>, through tools that implement a class of analysis like DeepLabCut <a class="citation" href="#mathisDeepLabCutMarkerlessPose2018">[31]</a> and scikit-learn <a class="citation" href="#pedregosaScikitlearnMachineLearning2011">[198]</a>, to tools for a specific technique like MoSeq <a class="citation" href="#wiltschkoRevealingStructurePharmacobehavioral2020">[199]</a> and DeepSqueak <a class="citation" href="#coffeyDeepSqueakDeepLearningbased2019">[200]</a>. The pattern of their use is then to build them into a custom analysis system that can then in turn range in sophistication from a handful of flash-drive-versioned scripts to automated pipelines.</p>

<p>Having tools like these of course puts researchers miles ahead of where they would be without them, and the developers of the mentioned tools have put in a tremendous amount of work to build sensible interfaces and make them easier to use. No matter how much good work might be done, inevitable differences between APIs is a relatively sizable technical challenge for researchers — a problem compounded by the incentives for fragmentation described previously. For toolbuilders, many parts of any given tool from architecture to interface have to be redesigned each time with varying degrees of success. For science at large, with few exceptions of well-annotated and packaged code, most results are only replicable with great effort.</p>

<p>Discontinuity between the behavior and interface of different pieces of software is, of course, the overwhelming norm. Negotiating boundaries between (and even within) software and information structures is an elemental part of computing. The only time it becomes a conceivable problem to “solve” interoperability is when the problem domain coalesces to the point where it is possible to articulate its abstract structure as a protocol, and the incentives are great enough to adopt it. That’s what we’re trying to do here.</p>

<p>It’s unlikely that we will solve the problem of data analysis being complicated, time consuming, and error prone by teaching every scientist to be a good programmer, but we can build experimental frameworks that make analysis tools easier to build and use.</p>

<p>Specifically, a shared analytical framework should be</p>

<ul>
  <li><strong>Modular</strong> - Rather than implementing an entire analysis pipeline as a monolith, the system should be broken into minimal, composable modules. The threshold of what constitutes “minimal” is of course to some degree a matter of taste, but the framework doesn’t need to make normative decisions like that. The system should support modularity by providing a clear set of hooks that tools can provide: eg. a clear place for a given tool to accept some input, parameters, and so on. Since data analysis can often be broken up into a series of relatively independent stages, a straightforward (and common) system for modularity is to build hooks to make a directed acyclic graph (DAG) of data transformation operations. This structure naturally lends itself to many common problems: caching intermediate results, splitting and joining multiple inputs and outputs, distributing computation over many machines, among others. Modularity is also needed within the different parts of the system itself – eg. running an analysis chain shouldn’t require a GUI, but one should be available, etc.</li>
  <li><strong>Pluggable</strong> - The framework needs to provide a clear way of incorporating external analysis packages, handling their dependencies, and exposing their parameters to the user. Development should ideally not be limited to a single body of code with a single mode of governance, but should instead be relatively conservative about requirements for integrating code, and liberal with the types of functionality that can be modified with a plugin. Supporting plugins means supporting people developing tools for the framework, so it needs to make some part of the toolbuilding process easier or otherwise empower them relative to an independent package. This includes building a visible and expressive system for submitting and indexing plugins so they can be discovered and credit can be given to the developers. Reciprocal to supporting plugins is being interoperable with existing and future systems, which the reader may have assumed was a given by now.</li>
  <li><strong>Deployable</strong> - For wide use, the framework needs to be easy to install and deploy locally and on computing clusters. A primary obstacle is dependency management, or making sure that the computer has everything needed to run the program. Some care needs to be taken here, as there are multiple emphases in deployability that can be in conflict. Deployable for who? A system that can be relatively challenging to use for routine exploratory data analysis but can distribute analysis across 10,000 GPUs has a very circumscribed set of people it is useful for. This is a matter of balancing design constraints, but we should prioritize broad access, minimal assumptions of technological access, and ease of use over being able to perform the most computationally demanding analyses possible when in conflict. Containerization is a common, and the most likely strategy here, but the interface to containers may need a lot of care to make accessible compared to opening a fresh .py file.</li>
  <li><strong>Reproducible</strong> - The framework should separate the <em>parameterization</em> of a pipeline, the specific options set by the user, and its <em>implementation</em>, the code that constitutes it. The parameterization of a pipeline or analysis DAG should be portable such that it, for example, can be published in the supplementary materials of a paper and reproduced exactly by anyone using the system. The isolation of parameters from implementation is complementary to the separation of metadata from data and if implemented with semantic triplets would facilitate a continuous interface from our data to analysis system. This will be explored further below and in <a href="#shared-knowledge">shared knowledge</a></li>
</ul>

<p>Thankfully a number of existing projects that are very similar to this description are actively being built. One example is <a href="https://datajoint.io/">DataJoint</a> <a class="citation" href="#yatsenkoDataJointSimplerRelational2018">[201]</a>, which recently expanded its facility for modularity with its recent <a href="https://github.com/datajoint/datajoint-elements">Elements</a> project <a class="citation" href="#yatsenkoDataJointElementsData2021">[202]</a>. Datajoint is a system for creating analysis pipelines built from a graph of processing stages (among <a href="https://docs.datajoint.org/python/v0.13/intro/01-Data-Pipelines.html#what-is-datajoint">other features</a>). It is designed around a refinement on traditional relational data models, which is reflected throughout the system as most operations being expressed in its particular schema, data manipulation, and query languages. This is useful for operations that are expressed in the system, but makes it harder to integrate external tools with their dependencies — <a href="https://github.com/datajoint/element-array-ephys/blob/1fdbcf12d1a518e686b6b79e9fbe77b736cb606a/Background.md">at the moment</a> it appears that spike sorting (with <a href="https://github.com/MouseLand/Kilosort">Kilosort</a> <a class="citation" href="#pachitariuKilosortRealtimeSpikesorting2016">[203]</a>) has to happen outside of the extracellular electrophysiology elements pipeline.</p>

<p>Kilosort is an excellent and incredibly useful tool, but its idiomatic architecture designed for standalone use is illustrative of the challenge of making a general-purpose analytic framework that can integrate a broad array of existing tools. It is built in MATLAB, which requires a paid license, making arbitrary deployment difficult, and MATLAB’s flat path system requires careful and usual manual orchestration of potentially conflicting names in different packages. Its parameterization and use are combined in a “<a href="https://github.com/MouseLand/Kilosort/blob/db3a3353d9a374ea2f71674bbe443be21986c82c/main_kilosort3.m">main</a>” script in the repository root that creates a MATLAB struct and runs a series of functions — requiring some means for a wrapping framework to translate between input parameters and the representation expected by the tool. Its preprocessing script combines <a href="https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/datashift.m#L74-L77">I/O</a>, preprocessing, and <a href="https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/datashift.m#L57-L68">plotting</a>, and requires data to be <a href="https://github.com/MouseLand/Kilosort/blob/a1fccd9abf13ce5dc3340fae8050f9b1d0f8ab7a/preProcess/preprocessDataSub.m#L82-L84">loaded from disk</a> rather than passed as arguments to preserve memory — making chaining in a pipeline difficult.</p>

<p>This is not a criticism of Datajoint or Kilosort, which were both designed for different uses and with different philosophies (that are of course, also valid). I mean this as a brief illustration of the design challenges and tradeoffs of these systems.</p>

<p>We can start getting a better picture for the way a decentralized analysis framework might work by considering the separation between the metadata and code modules, hinting at a protocol as in the federated systems sketch above. In the time since the heydey of the semantic web there has been a revolution in containerization and dependency management that makes it possible to imagine extending the notion of linked data to being able to not only indicate binary data but also <em>executable code.</em> Software dependencies form a graph structure, with one top level package specifying a version range from a 1st-order dependent, which in turn has its own set of 2nd-order packages and versions, and so on. Most contemporary dependency managers (like Python’s <a href="https://python-poetry.org/">poetry</a>, Javascript’s <a href="https://yarnpkg.com/">yarn</a>, Rust’s <a href="https://doc.rust-lang.org/cargo/">cargo</a>, Ruby’s <a href="https://bundler.io/">Bundler</a>, etc.) compute an explicit dependency graph from each package’s version ranges to create a ‘lockfile’ containing the exact versions of each package, and usually the repositories where they’re located and the content hashes to verify them. More general purpose package managers like <a href="https://spack.readthedocs.io/en/latest/">spack</a> <a class="citation" href="#gamblinSpackPackageManager2015">[204]</a>, or <a href="https://nixos.org/">nix</a> <a class="citation" href="#dolstraNixSafePolicyFree2004">[205]</a> can also specify system-level software outside of an individual programming language, and containerization tools like <a href="https://www.docker.com/">docker</a> can create environments that include entire operating systems.</p>

<p>Since we’re considering modular analysis elements, each module would need some elemental properties like the parameters that define it, its inputs, outputs, as well as some additional metadata about its implementation (eg. this one takes <em>numpy arrays</em> and this one takes <em>matlab structs</em>). The precise implementation of a modular protocol also depends on the graph structure of the analysis pipelining system. We invoked DAGs before, but analysis graph structure of course has its own body of researchers refining them into eg. <a href="https://en.wikipedia.org/wiki/Petri_net">Petri nets</a> which are graphs whose nodes necessarily alternate between “places” (eg. intermediate data) and “transitions” (eg. an analysis operation), and their related workflow markup languages (eg. <a href="https://openwdl.org/">WDL</a> or <a class="citation" href="#vanderaalstYAWLAnotherWorkflow2005">[206]</a>). In that scheme, a framework could provide tools for converting data between types, caching intermediate data, etc. between analysis steps, as an example of how different graph structures might influence its implementation.</p>

<p>The graph structure of our linked data system could flexibly extend to be continuous with these dependency pipeline graphs. With some means for a client to resolve the dependencies of a given analysis node, it would be possible to reconstruct the environment needed to run it. By example, how might a system like this work?</p>

<p>Say we use <code class="language-plaintext highlighter-rouge">@analysis</code> as the namespace for our specifying each analysis node’s properties, and someone has provided bindings to objects in <code class="language-plaintext highlighter-rouge">numpy</code> (we’ll give an example of how these bindings might work below, but for now assume they work analogously to the module structure of numpy, ie. <code class="language-plaintext highlighter-rouge">@numpy:ndarray == numpy.ndarray</code>). We can assume they are provided by the package maintainers, but that’s not necessary: this is my node and it takes what I want it to!</p>

<p>In pseudocode, I could define some analysis node for, say, converting an RGB image to grayscale under my namespace as <code class="language-plaintext highlighter-rouge">@jonny:bin-spikes</code> like this:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#bin-spikes&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@analysis:node</span><span class="w">
    </span><span class="n">Version</span><span class="w"> </span><span class="s">"&gt;=1.0.0"</span><span class="w">

  </span><span class="n">hasDescription
</span><span class="w">    </span><span class="s">"Convert an RGB Image to a grayscale image"</span><span class="w">

  </span><span class="n">inputType
</span><span class="w">    </span><span class="na">@numpy:ndarray</span><span class="w">
      </span><span class="c1"># ... some spec of shape, dtype ...</span><span class="w">

  </span><span class="n">outputType
</span><span class="w">    </span><span class="na">@numpy:ndarray</span><span class="w">
      </span><span class="c1"># ... some spec of shape, dtype ...</span><span class="w">

  </span><span class="n">params
</span><span class="w">    </span><span class="n">bin_width</span><span class="w"> </span><span class="n">int
</span><span class="w">      </span><span class="n">default</span><span class="w"> </span><span class="mi">10</span><span class="w">
</span></code></pre></div></div>

<p>I have abbreviated the specification of shape and datatype to not overcomplicate the pseudocode example, but say we successfully specify a 3 dimensional (width x height x channels) array with 3 channels as input, and a a 2 dimensional (width x height) array as output. An optional <code class="language-plaintext highlighter-rouge">bin_width</code> parameter with default “10” can also be provided.</p>

<p>The code doesn’t run on nothing! We need to specify our node’s dependencies. Say in this case we need to specify an operating system image <code class="language-plaintext highlighter-rouge">ubuntu</code>, a version of <code class="language-plaintext highlighter-rouge">python</code>, a system-level package <code class="language-plaintext highlighter-rouge">opencv</code>, and a few python packages on <code class="language-plaintext highlighter-rouge">pip</code>. We are pinning specific versions with <a href="https://semver.org/">semantic versioning</a>, but the syntax isn’t terribly important. Then we just need to specify where the code for the node itself comes from:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="n">dependsOn
</span><span class="w">    </span><span class="na">@ubuntu:"^20</span><span class="p">.</span><span class="n">*</span><span class="s">":x64
    @python:"</span><span class="mf">3.8</span><span class="s">"
    @apt:opencv:"</span><span class="n">^4</span><span class="p">.</span><span class="n">*</span><span class="p">.</span><span class="n">*</span><span class="s">"
    @pip:opencv-python:"</span><span class="n">^4</span><span class="p">.</span><span class="n">*</span><span class="p">.</span><span class="n">*</span><span class="s">"
      .extraSource "</span><span class="nn">https:</span><span class="n">//pywheels</span><span class="p">.</span><span class="n">org/</span><span class="s">"
    @pip:numpy:"</span><span class="n">^14</span><span class="p">.</span><span class="n">*</span><span class="p">.</span><span class="n">*</span><span class="s">"

  providedBy
    @git:repository 
      .url "</span><span class="nn">https:</span><span class="n">//mygitserver</span><span class="p">.</span><span class="n">com/binspikes/fast-binspikes</span><span class="p">.</span><span class="n">git</span><span class="s">"
      .hash "</span><span class="n">fj9wbkl</span><span class="s">"
    @python:class "</span><span class="n">/main-module/binspikes</span><span class="p">.</span><span class="nn">py:</span><span class="n">Bin_Spikes</span><span class="s">"
      method "</span><span class="n">run</span><span class="err">"</span><span class="w">
</span></code></pre></div></div>

<p>Here we can see the practical advantage of the “inverted” link-based system rather than an object-oriented-like approach. <code class="language-plaintext highlighter-rouge">@ubuntu</code> refers to a specific software image that would have a specific <code class="language-plaintext highlighter-rouge">providedBy</code> value, but both <code class="language-plaintext highlighter-rouge">@apt</code> and <code class="language-plaintext highlighter-rouge">@pip</code> can have different repositories that they pull packages from, and for a given version and repository there will be multiple possible software binaries for different CPU architectures, python versions, etc. Rather than needing to specify a generalized specification format, each of these different types of links could specify their own means of resolving dependencies: a <code class="language-plaintext highlighter-rouge">@pip</code> dependency requires some <code class="language-plaintext highlighter-rouge">@python</code> version to be specified. Both require some operating system and architecture. If we hadn’t provided the <code class="language-plaintext highlighter-rouge">.extraSource</code> of pywheels (for ARM architectures), someone who had defined some link between a given architecture and <code class="language-plaintext highlighter-rouge">@pip</code> could be proposed as a way of finding the package.</p>

<p>Our <code class="language-plaintext highlighter-rouge">@analysis.node</code> protocol gives us several slots to connect different tools together, each in turn presumably provides some minimal functionality expected by that slot: eg. <code class="language-plaintext highlighter-rouge">inputType</code> can expect <code class="language-plaintext highlighter-rouge">@numpy:ndarray</code> to specify its own dependencies, the programming language it is written in, shape, data type, and so on. Coercing data between chained nodes then becomes a matter of mapping between the <code class="language-plaintext highlighter-rouge">@numpy</code> and, say a <code class="language-plaintext highlighter-rouge">@nwb</code> namespace of another format. In the same way that there can be multiple, potentially overlapping between data schemas, it would then be possible for people to implement mappings between intermediate data formats as-needed. This gives us an opportunity to build pipelines that use tools from multiple languages, a problem typically solved by manually saving, loading, and cleaning intermediate data.</p>

<p>This node also becomes available to extend, say someone wanted to add an additional input format to my node:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;@friend#bin-spikes&gt;</span><span class="w">
  </span><span class="n">extends</span><span class="w"> </span><span class="na">@jonny:bin-spikes</span><span class="w">

  </span><span class="n">inputType
</span><span class="w">    </span><span class="na">@pandas:DataFrame</span><span class="w">

  </span><span class="n">providedBy
</span><span class="w">    </span><span class="p">...</span><span class="w">
</span></code></pre></div></div>

<p>They don’t have to interact with my potentially messy codebase at all, but it is automatically linked to my work so I am credited. One could imagine a particular analysis framework implementation that would then search through extensions of a particular node for a version that supports the input/output combinations appropriate for their analysis pipeline, so the work is cumulative. This functions as a dramatic decrease in the size of a unit of work that can be shared.</p>

<p>This also gives us healthy abstraction over implementation. Since the functionality is provided by different, mutable namespaces, we’re not locked into any particular piece of software — even our <code class="language-plaintext highlighter-rouge">@analysis</code> namespace that gives the <code class="language-plaintext highlighter-rouge">inputType</code> etc. slots could be forked. We could implement the dependency resolution system as, eg. a docker container, but it also could be just a check on the local environment if someone is just looking to run a small analysis on their laptop with those packages already installed.</p>

<p>The relative complexity required to define an analysis node, as well as the multiple instances of automatically computed metadata like dependency graphs hints that we should be thinking about tools that avoid needing to write it manually. We could use an <code class="language-plaintext highlighter-rouge">Example_Framework</code> that provides a set of classes and methods to implement the different parts of the node (a la <a href="https://luigi.readthedocs.io/en/stable/tasks.html">luigi</a>). Our <code class="language-plaintext highlighter-rouge">Bin</code> class inherits from <code class="language-plaintext highlighter-rouge">Node</code>, and we implement the logic of the function by overriding its <code class="language-plaintext highlighter-rouge">run</code> method and specify an <code class="language-plaintext highlighter-rouge">output</code> file to store intermediate data (if requested by the pipeline) with an <code class="language-plaintext highlighter-rouge">output</code> method. Our class is within a typical python package that specifies its dependencies, which the framework can detect. We also specify a <code class="language-plaintext highlighter-rouge">bin_width</code> as a <code class="language-plaintext highlighter-rouge">Param</code>eter for our node, as an example of how a lightweight protocol could be bidirectionally specified as an <a href="#shared-knowledge">interface</a> to the linked data format: we could receive a parameterization from our pseudocode metadata specification, or we could write a framework with a <code class="language-plaintext highlighter-rouge">Bin.export_schema()</code> that constructs the pseudocode metadata specification from code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">Example_Framework</span> <span class="kn">import</span> <span class="n">Node</span><span class="p">,</span> <span class="n">Param</span><span class="p">,</span> <span class="n">Target</span>

<span class="k">class</span> <span class="nc">Bin</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
  <span class="n">bin_width</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Target</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">Target</span><span class="p">(</span><span class="s">'temporary_data.pck'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span><span class="s">'numpy.ndarray'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s">'numpy.ndarray'</span><span class="p">:</span>
    <span class="c1"># do some stuff
</span>    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>Now that we have a handful of processing nodes, we could then describe some <code class="language-plaintext highlighter-rouge">@workflow</code>, taking some <code class="language-plaintext highlighter-rouge">@nwb:NWBFile</code> as input, as inferred by the <code class="language-plaintext highlighter-rouge">inputType</code> of the <code class="language-plaintext highlighter-rouge">bin-spikes</code> node, and then returning some output as a <code class="language-plaintext highlighter-rouge">:my-analysis:processed</code> child beneath the input. We’ll only make a linear pipeline with two stages, but there’s no reason more complex branching and merging couldn’t be described as well.</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#my-analysis&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@analysis:workflow</span><span class="w">

  </span><span class="n">inputType</span><span class="w"> 
    </span><span class="na">@jonny:bin-spikes:inputType</span><span class="w">

  </span><span class="n">outputName
</span><span class="w">    </span><span class="nn">input:my-analysis:</span><span class="n">processed

</span><span class="w">  </span><span class="n">step</span><span class="w"> </span><span class="n">Step1</span><span class="w"> </span><span class="na">@jonny:bin-spikes</span><span class="w">
  </span><span class="n">step</span><span class="w"> </span><span class="n">Step2</span><span class="w"> </span><span class="na">@someone-else:another-step</span><span class="w">
    </span><span class="n">input</span><span class="w"> </span><span class="nn">Step1:</span><span class="n">output
</span></code></pre></div></div>

<p>Since the parameters are linked from the analysis nodes, we can specify them here (or in the workflow). Assuming literally zero abstraction and using the tried-and-true “hardcoded dataset list” pattern, something like:</p>

<p><span id="myproject-analysis"></span></p>
<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#my-project&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@analysis:project</span><span class="w">

  </span><span class="n">hasDescription
</span><span class="w">    </span><span class="s">"I gathered some data, and it is great!"</span><span class="w">

  </span><span class="n">researchTopic
</span><span class="w">    </span><span class="na">@neuro:systems:auditory:speech-processing</span><span class="w">
    </span><span class="na">@linguistics:phonetics:perception:auditory-only</span><span class="w">

  </span><span class="n">inPaper
</span><span class="w">    </span><span class="na">@doi:10</span><span class="mf">.1</span><span class="mi">121</span><span class="nn">:</span><span class="mf">1.5</span><span class="mi">091776</span><span class="w"> 

  </span><span class="n">workflow</span><span class="w"> </span><span class="n">Analysis1</span><span class="w"> </span><span class="na">@jonny:my-analysis</span><span class="w">
    </span><span class="n">globalParams
</span><span class="w">      </span><span class="p">.</span><span class="nn">Step1:params:</span><span class="n">bin_width</span><span class="w"> </span><span class="mi">10</span><span class="w">

    </span><span class="n">datasets
</span><span class="w">      </span><span class="na">@jonny</span><span class="p">.</span><span class="nn">mydata1:</span><span class="n">v0</span><span class="mf">.1.0</span><span class="nn">:</span><span class="n">raw
</span><span class="w">      </span><span class="na">@jonny</span><span class="p">.</span><span class="nn">mydata2:</span><span class="n">^0</span><span class="mf">.2</span><span class="p">.</span><span class="nn">*:</span><span class="n">raw
</span><span class="w">      </span><span class="na">@jonny</span><span class="p">.</span><span class="nn">mydata3:</span><span class="err">&gt;</span><span class="n">=0</span><span class="mf">.1.1</span><span class="nn">:</span><span class="n">raw
</span></code></pre></div></div>

<p>And there we are! The missing parameters like <code class="language-plaintext highlighter-rouge">outputName</code> from our workflow can be filled in from the defaults. Our project is an abstract representation of the analysis to be performed and where its output will be found - in this case as <code class="language-plaintext highlighter-rouge">:processed</code> beneath each dataset link. From this very general pseudocode example it’s possible to imagine executing the code locally or on some remote server, pulling the data from our p2p client, installing the environment, and duplicating the resulting data to the clients configured to mirror our namespace. This system would work similar to the combination of configuration and lockfiles from package managers: we would give some abstract specification for a project’s analysis, but then running it would create a new set of links with the exact dependency graph, links to intermediate products, and so on. We get some inkling of where we’re going later by also being able to specify the paper this data is associated with, as well as some broad categories of research topics so that our data as well as the results of the analysis can be found.</p>

<p>From here we could imagine how existing tools might be integrated without needing to be dramatically rewritten. In addition to wrapping their parameters, functions, and classes with the above <code class="language-plaintext highlighter-rouge">Node</code> class, we could imagine our analysis linking framework providing some function to let us indicate code within a package and prompt us for any missing pieces like dependency specification from, for example, old style python packages that don’t require it. For packages that don’t have an explicit declarative parameterization, but rely on programmatically created configuration files, we could imagine a tool ingestion function being able to extract default fields and then refer to them with a <code class="language-plaintext highlighter-rouge">fromConfig @yaml</code> link. A single tool need not be confined to a single analysis node: for example a tool that requires some kind of user interaction could specify that with an <code class="language-plaintext highlighter-rouge">@analysis:interactive</code> node type that feeds its output into a subsequent analysis node. There are infinitely more variations to be accounted for — but adapting to them is the task of an extensible linking system.</p>

<p>As soon as we extend our relatively static protocol to the realm of arbitrary code we immediately face the question of security. Executing arbitrary code from many sources is inherently dangerous and worthy of careful thought, but any integrative framework becomes a common point where security practices could be designed into the system as opposed to the <em>relative absence of security practices of any kind</em> in most usages of scientific software. There is no reason to believe that this system is intrinsically more dangerous than running uninspected packages from PyPI, which, for example, have been known to <a href="https://blog.sonatype.com/python-packages-upload-your-aws-keys-env-vars-secrets-to-web">steal AWS keys and environment variables</a> <a class="citation" href="#sharmaPythonPackagesUpload2022">[207]</a>. Having analysis code and its dependency graph specified publicly presents opportunities for being able to check for identified vulnerabilities at the time of execution — a role currently filled by platform tools like GitHub’s <a href="https://github.com/dependabot">dependabot</a> or npm’s <a href="https://docs.npmjs.com/auditing-package-dependencies-for-security-vulnerabilities">audit</a>. Running code by default in containers or virtual environments could be a way towards making code secure by default.</p>

<p>So that’s useful, but comparable to some existing pipelining technologies. The important part is in the way this hypothetical analysis framework and markup interact with our data system — it’s worth unpacking a few points of interaction.</p>

<p>A dataset linked to an analysis pipeline and result effectively constitutes a “unit of analysis.” If I make my data publicly available, I would be able to see all the results and pipelines that have been linked to it. Within a single pipeline, comparing the results across a grid of possible parameterizations gives us a “multiverse analysis <a class="citation" href="#steegenIncreasingTransparencyMultiverse2016">[208]</a>” for estimating the effects of each parameterization for free. Conversely, “rules of thumb” for parameter selection can be replaced by an evaluation of parameters and results across prior applications of the pipeline. Since some parameters like model weights in neural networks are not trivial to reproduce, and their use is linked to the metadata of the dataset they are applied to, all analyses contribute to a collection of models like the <a href="http://www.mackenziemathislab.org/dlc-modelzoo">DeepLabCut model zoo</a> decreasing the need for fine tuning on individual datasets and facilitating metalearning across datasets.</p>

<p>Across multiple pipelines, a dataset need no longer be dead on publication, but can instead its meaning and interpretation can continuously evolve along with the state of our tools and statistical practices. Since pipelines themselves are subject to the same kind of metadata descriptions as datasets are, it becomes to find multiple analysis nodes that implement the same operation, or to find multiple pipelines that perform similar operations despite using different sets of nodes. Families of pipelines that are applied to semantically related datasets would then become the substrate for a field’s state of the art, currently buried within disorganized private code repositories and barely-descriptive methods sections. Instead of a 1:1 relationship where one dataset is interpreted once, we could have a many-to-many relationship where a cumulative body of data is subject to an evolving negotiation of interpretation over time — ostensibly how science is <em>“supposed to”</em> work.</p>

<p>This system also allows the work of scientific software developers to be credited according to use, instead of according to the incredibly leaky process of individual authors remembering to search for all the citations for all the packages they may have used in their analysis. Properly crediting the work of software developers is important not only for equity, but also for the reliability of scientific results as a whole. A common admonishment in cryptography is to “never roll your own crypto,” but that’s how most homebrew analysis code works, and the broader state of open source scientific code is not much better without incentives for maintenance. Bugs in analysis code that produce inaccurate results are inevitable and rampant <a class="citation" href="#millerScientistNightmareSoftware2006">[209, 210, 211, 212]</a>, but impossible to diagnose when every paper writes its own pipeline. A common analysis framework would be a single point of inspection for bugs and means of providing credit to people who fix them. When a bug is found, rather than irreparably damaging collective confidence in a field, it would then be trivial to re-run all the analyses that were impacted and evaluate how their results were changed.</p>

<p>Finally, much like how we are building towards the social systems to support federations for sharing data, integrating analysis pipelines into a distributed network of servers is a means of realizing a generalized <a href="https://foldingathome.org/">Folding@Home</a>-style distributed computing grid <a class="citation" href="#larsonFoldingHomeGenome2009">[213, 214]</a>. Existing projects like F@H and the Pacific Research Platform <a class="citation" href="#smarrPacificResearchPlatform2018a">[215]</a> show the promise of these distributed computing systems for solving previously-intractable problems, but they require large amounts of coordination and are typically centrally administered towards a small number of specific projects with specific programming requirements. With some additional community systems for governance, resource management, and access, they become tantalizingly in-reach from the system we are describing here. We will return to that possibility after discussing experimental tools.</p>

<h3 id="experimental-frameworks">Experimental Frameworks</h3>

<p>Across from the tools to analyze data are those to collect it, and tools to integrate the diversity of experimental practice are a different challenge altogether: <em>everyone needs completely different things!</em> Imagine the different stages of research as a cone of complexity: at the apex we can imagine the relatively few statistical outcomes from a family of tests and models. For every test statistic we can imagine a thousand analysis scripts, for every analysis script we might expect a thousand data formats, and so the complexity of the thousand experimental tools used to collect each type of data feels … different.</p>

<p>Beyond a narrow focus of the software for performing experiments itself, the surrounding contextual knowledge work largely lacks a means of communication and organization. Methods sections have been increasingly marginalized, abbreviated, pushed to the end, and relegated to the supplement. The large body of work that is not immediately germane to experimental results, like animal care, engineering instruments, lab management, etc. have effectively no formal means of communication — and so little formal means of credit assignment.</p>

<p>Extending our ecosystem to include experimental tools has a few immediate benefits: bridging the gap between collection and sharing of data would resolve the need for format conversion as a prerequisite for inclusion in the linked system, allowing the expression of data to be a fluid part of the experiment itself. It would also serve as a means of building a body of cumulative contextual knowledge in a creditable system.</p>

<p>I have previously written about the design of a generalizable, distributed experimental framework <a class="citation" href="#saundersAutopilotAutomatingBehavioral2019">[194]</a>, so to avoid repeating myself, and since many of the ideas from the section on analysis tools apply here as well, I will be relatively brief.</p>

<p>We don’t have the luxury of a natural formalism like a DAG to structure our experimental tools. Some design constraints on experimental frameworks might help explain why:</p>

<ul>
  <li>They need to support a wide variety of instrumentation, from <strong>off-the-shelf parts,</strong> to <strong>proprietary instruments</strong> as are common in eg. microscopy, to <strong>custom, idiosyncratic designs</strong> that might make up the existing infrastructure in a lab. Writing and testing embedded code that controls external hardware is a wholly different kind of difficulty than writing analysis tools.</li>
  <li>To be supportive, rather than constraining, they need to be able to <strong>flexibly perform many kinds of experiments</strong> in a way that is <strong>familiar to patterns of existing practice.</strong> That effectively means being able to coordinate heterogeneous instruments in some “task” with a flexible syntax.</li>
  <li>They need to be <strong>inexpensive to implement,</strong> in terms of both money and labor, so it can’t require buying a whole new set of hardware or dramatically restructuring existing research practices.</li>
  <li>They need to be <strong>accessible and extensible,</strong> with many different points of control with different expectations of expertise and commitment to the framework. It needs to be useful for someone who doesn’t want to learn it to its depths, but also have a comprehensible codebase at multiple scales so that reasearchers can <strong>easily extend</strong> it when needed.</li>
  <li>They need to be designed to support <strong>reproducibility and provenance,</strong> which is a significant challenge given the heterogeneity inherent in the system. On one hand, being able to produce <em>data that is clean at the time of acquisition</em> simplifies automated provenance, but enabling experimental replication requires multiple layers of abstraction to keep the idiosyncracies of an experiment separable from its implementation: it shouldn’t require building <em>exactly</em> the same apparatus with <em>exactly</em> the same parts connected in <em>exactly</em> the same way to replicate an experiment.</li>
  <li>Ideally, they need to support <strong>cumulative labor and knowledge organization,</strong> so an additional concern with designing abstractions between system components is allowing work to be made portable and combinable with others. The barriers to contribution should be extremely minimal, not requiring someone to be a professional programmer to make a pull request to a central library, and contributions should come in many modes — code is not the only form of knowing and it’s far from the only thing needed to perform an experiment.</li>
</ul>

<p>Here, as in the domains of data and analysis, the temptation to universalize is strong, and the parts of the problem that are emphasized influence the tools that are produced. A common design tactic for experimental tools is to design them as state machines, a system of states and transitions not unlike the analysis DAGs above. One such nascent project is <a href="https://archive.org/details/beadl-xml-documentation-v-0.1/mode/2up">BEADL</a> <a class="citation" href="#wulfBEADLXMLDocumentation2020">[216]</a> from a Neurodata Without Borders <a href="https://archive.org/details/nwb-behavioral-task-wg">working group</a>. BEADL is an XML-based markup for standardizing a behavioral task as an abstraction of finite state machines called <a href="https://statecharts.github.io/">statecharts</a>. Experiments are fully abstract from their hardware implementation, and can be formally validated in simulations. The working group also describes creating a standardized ontology and metadata schema for declaring all the many variable parameters for experiments, like reward sizes, stimuli, and responses <a class="citation" href="#nwbbehavioraltaskwgNWBBehavioralTask2020">[217]</a>. This group, largely composed of members from the Neurodata Without Borders team, understandably emphasize systematic description and uniform metadata as a primary design principle.</p>

<p>Personally, I <em>like</em> statecharts. The problem is that it’s not necessarily natural to express things as statecharts as you would want to, or in the way that your existing, long-developed local experimental code does. There are only a few syntactical features needed to understand the following statechart: blocks are states, they can be inside each other. Arrows move between blocks depending on some condition. Entering and exiting blocks can make things happen. Short little arrows from filled spots are where you start in a block, and when you get to the end of the chart you go back to the first one. See the following example of a statechart for controlling a light, described in the <a href="https://statecharts.dev/on-off-statechart.html">introductory documentation</a> and summarized in the figure caption:</p>

<p><img src="/infrastructure/assets/images/on-off-delayed-exit-1.svg" alt="on off delayed exit statechart, see https://statecharts.dev/on-off-statechart.html for full descriptive text" />
<em>“When you flick a lightswitch, wait 0.5 seconds before turning the light on, then once it’s on wait 0.5 seconds before being able to turn it back off again. When you flick it off, wait 2 seconds before you can turn it on again.</em></p>

<p>They have an extensive set of documents that defend the consistency and readability of statecharts on their <a href="https://statecharts.dev/">homepage</a>, and my point here is not to disagree with them. My point is instead that tools that aspire to the status of generalized infrastructure can’t ask people to dramatically change the way they think about and do science. There are many possible realizations of any given experiment, and each is more or less natural to every person.</p>

<p>The problem here is really one of emphasis, BEADL seeks to solve problems with inconsistencies in terminology by standardizing them, and in order to do that seeks to standardize the syntax for specifying experiments.</p>

<p>This means of standardization has many attractive qualities and is being led by very capable researchers, but I think the project is illustrative of how the differing structures of problems constrain the possible space of tooling. Analysis tasks are often asynchronous, where the precise timing of each node’s completion is less important than the path dependencies between different nodes.  Analysis tasks often have a clearly defined set of start, end, and intermediate cache points, rather than branching or cyclical decision paths that change over multiple timescales. Statecharts are a hierarchical abstraction of finite state machines, the primary advantage of which is that they are better able to incorporate continuous and history-dependent behavior, which cause state explosion in traditional finite-state machines.</p>

<p>The difficulty of a controlled ontology for experimental frameworks is perhaps better illustrated by considering a full experiment. In Autopilot, a full experiment can be parameterized by the <code class="language-plaintext highlighter-rouge">.json</code> files that define the task itself and the system-specific configuration of the hardware. An <a href="https://gist.github.com/sneakers-the-rat/eebe675326a157df49f66f62c4e33a6e">example task</a> from our lab consists of 7 behavioral shaping stages of increating difficulty that introduce the animal to different features of a fairly typical auditory categorization task. Each stage includes the parameters for at most 12 different stimuli per stage, probabilities for presenting lasers, bias correction, reinforcement, criteria for advancing to the next stage, etc. So just for one relatively straightforward experiment, in one lab, in one subdiscipline, there are <strong>268 parameters</strong> – excluding all the default parameters encoded in the software.</p>

<p>How might we approach this problem differently, to accommodate diversity of thought styles and to be complementary to our data and analysis systems? The primary things we need from our experimental frameworks are a) to be able to link a particular realization of an experiment with the metadata that describes it, and b) to be able to produce similarly metadata-rich data. Rather than linked data indicating code as in our analysis frameworks, we might invert our strategy and think about code that draws from linked data.</p>

<p>As an example, <a href="https://docs.auto-pi-lot.com">Autopilot</a> <a class="citation" href="#saundersAutopilotAutomatingBehavioral2019">[194]</a> approaches the problem by avoiding standardizing <em>experiments</em> themselves, instead providing smaller building blocks of experimental tools like hardware drivers, data transformations, etc. and emphasizing understanding their use in <em>context.</em> This approach sacrifices some of the qualities of a standardized system like being a logically complete or guaranteeing a standardized vocabulary in order to better support integrating with existing work patterns and making work cumulative. Because we can’t possibly predict the needs and limitations of a totalizing system, we split the problem along a different set of concerns, those of the elements of experimental practice, and give facility for describing how they are used together.</p>

<p>For concrete example, we might imagine the lightswitch in an autopilot-like framework like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">autopilot.hardware.gpio</span> <span class="kn">import</span> <span class="n">Digital_Out</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Lock</span>

<span class="k">class</span> <span class="nc">Lightswitch</span><span class="p">(</span><span class="n">Digital_Out</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
    <span class="n">off_debounce</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">on_delay</span><span class="p">:</span>     <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">on_debounce</span><span class="p">:</span>  <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
    <span class="s">"""
    Args:
      off_debounce (float): 
        Time (s) before light can be turned back on
      on_delay (float): 
        Time (s) before light is turned on
      on_debounce (float): 
        Time (s) after turning on that light can't be turned off
    """</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">off_debounce</span> <span class="o">=</span> <span class="n">off_debounce</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">on_delay</span>     <span class="o">=</span> <span class="n">on_delay</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">on_debounce</span>  <span class="o">=</span> <span class="n">on_debounce</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">on</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">lock</span> <span class="o">=</span> <span class="n">Lock</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">switch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># use a lock to make sure if
</span>    <span class="c1"># called while waiting, we ignore it
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">lock</span><span class="p">.</span><span class="n">acquire</span><span class="p">():</span>
      <span class="k">return</span>

    <span class="c1"># if already on, switch off
</span>    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">on</span><span class="p">:</span> 
      <span class="bp">self</span><span class="p">.</span><span class="n">on</span> <span class="o">=</span> <span class="bp">False</span>
      <span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">off_debounce</span><span class="p">)</span>

    <span class="c1"># otherwise switch on
</span>    <span class="k">else</span><span class="p">:</span> 
      <span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">on_delay</span><span class="p">)</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">on</span> <span class="o">=</span> <span class="bp">True</span>
      <span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">on_debounce</span><span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">lock</span><span class="p">.</span><span class="n">release</span><span class="p">()</span>
</code></pre></div></div>

<p>The class <code class="language-plaintext highlighter-rouge">Lightswitch</code> inherits from the <code class="language-plaintext highlighter-rouge">Digital_Out</code> class, which in turn inherits from <code class="language-plaintext highlighter-rouge">GPIO</code> and eventually <code class="language-plaintext highlighter-rouge">Hardware</code>. This hierarchy of inheritance carries with it a progressive refinement of meaning about what this class does. The terms <code class="language-plaintext highlighter-rouge">off_debounce</code>, <code class="language-plaintext highlighter-rouge">on_delay</code>, and <code class="language-plaintext highlighter-rouge">on_debounce</code> are certainly not part of a controlled ontology, but the context of their use bounds their meaning. Rather than being bound by, for example, the abstract <code class="language-plaintext highlighter-rouge">Latency</code> term from <a href="https://scicrunch.org/scicrunch/interlex/view/ilx_0106040#annotations">interlex</a>, we have defined terms that we need to make a hardware object do what we need it to. These terms don’t have too much meaning on their own — there isn’t even much in this class to uniquely identify it as a “lightswitch” beyond its name, it is just a timed digital output. What makes them meaningful is how they are used.</p>

<p>The way Autopilot handles various parameters are part of set of layers of abstraction that separate idiosyncratic logic from the generic form of a particular <code class="language-plaintext highlighter-rouge">Task</code> or <code class="language-plaintext highlighter-rouge">Hardware</code> class. The general structure of a two-alternative forced choice task is shared across a number of experiments, but they may have different stimuli, different hardware, and so on. Autopilot <code class="language-plaintext highlighter-rouge">Task</code>s use abstract references to classes of hardware components that are required to run them, but separates their implementation as a system-specific configuration so that it’s not necessary to have <em>exactly the same</em> components plugged into <em>exactly the same</em> GPIO pins, etc. Task parameters like stimuli, reward timings, etc. are similarly split into a separate task parameterization that both allow <code class="language-plaintext highlighter-rouge">Task</code>s to be generic and make provenance and experimental history easier to track. <code class="language-plaintext highlighter-rouge">Task</code> classes can be subclasses to add or modify logic while being able to reuse much of the structure and maintain the link between the root task and its derivatives — for example <a href="https://github.com/auto-pi-lot/autopilot-plugin-wehrlab/blob/9cfffcf5fe1886d25658d4f1f0c0ffe41c18e2cc/gap/nafc_gap.py#L13-L49">one task we use</a> that starts a continuous background sound but otherwise is the same as the root <code class="language-plaintext highlighter-rouge">Nafc</code> class. The result of these points of abstraction is to allow exact experimental replication on inexactly replicated experimental apparatuses.</p>

<p>This separation of the different components of an experiment is a balance between reusable code and clear metadata: we might allow freedom of terminology for each individual class, but by designing the system to encourage reuse of flexible classes we reduce the number of times unique terms need to be redefined. For example, we can imagine a trivial use of our lightswitch inside a task measuring an experimental subject’s estimation of time intervals: we toggle the switch once some analog sensor reaches a certain threshold, and then the subject tries to press a button at the same time as the light turns on after a fixed delay. While this is very similar to how Autopilot currently works, note that we are using pseudocode to indicate how it might extend the system we’re describing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">autopilot</span> <span class="kn">import</span> <span class="n">Task</span>
<span class="kn">from</span> <span class="nn">autopilot.data.modeling</span> <span class="kn">import</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="k">class</span> <span class="nc">Controlled_Switch</span><span class="p">(</span><span class="n">Task</span><span class="p">):</span>
  <span class="s">"""
  A [[Discipline::Psychophysics]] experiment 
  to measure [[Research Topic::Interval Estimation]].
  """</span>

  <span class="k">class</span> <span class="nc">Params</span><span class="p">(</span><span class="n">Task</span><span class="p">.</span><span class="n">Param_Spec</span><span class="p">):</span>
    <span class="n">on_delay</span><span class="p">:</span> <span class="s">'@si:seconds'</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Delay (s) before turning light on"</span><span class="p">,</span>
        <span class="n">parameterizes</span><span class="o">=</span><span class="s">"@jonny:hardware:Lightswitch"</span><span class="p">)</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Flick switch above this value"</span><span class="p">,</span>
        <span class="n">is_a</span><span class="o">=</span><span class="s">"@interlex:Threshold"</span><span class="p">)</span>

  <span class="k">class</span> <span class="nc">TrialData</span><span class="p">(</span><span class="n">Task</span><span class="p">.</span><span class="n">TrialData_Spec</span><span class="p">):</span>
    <span class="n">switch_time</span><span class="p">:</span> <span class="n">datetime</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Time the switch was flicked"</span><span class="p">)</span>
    <span class="n">target_time</span><span class="p">:</span> <span class="n">datetime</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Time the subject should respond"</span><span class="p">)</span>
    <span class="n">response_time</span><span class="p">:</span> <span class="n">datetime</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Time the subject did respond"</span><span class="p">)</span>
    <span class="n">error</span><span class="p">:</span> <span class="n">timedelta</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Difference between target and response"</span><span class="p">,</span>
        <span class="n">is_a</span><span class="o">=</span><span class="s">"@psychophys:ReactionTime"</span><span class="p">)</span>


  <span class="n">HARDWARE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'sensor'</span><span class="p">:</span> <span class="s">'Analog_In'</span><span class="p">,</span>
    <span class="s">'button'</span><span class="p">:</span> <span class="s">'Digital_In'</span><span class="p">,</span>
    <span class="s">'lightswitch'</span><span class="p">:</span> <span class="s">'@jonny:hardware:Lightswitch'</span>
  <span class="p">}</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
      <span class="n">on_delay</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span> 
      <span class="n">threshold</span><span class="p">:</span><span class="nb">float</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">on_delay</span> <span class="o">=</span> <span class="n">on_delay</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">Controlled_Switch</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">poll</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">poll</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">self</span><span class="p">.</span><span class="n">running</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">hardware</span><span class="p">[</span><span class="s">'sensor'</span><span class="p">].</span><span class="n">value</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hardware</span><span class="p">[</span><span class="s">'lightswitch'</span><span class="p">].</span><span class="n">switch</span><span class="p">()</span>
        <span class="n">switch_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">()</span>
        <span class="n">target_time</span> <span class="o">=</span> <span class="n">switch_time</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">on_delay</span>

        <span class="c1"># Wait for the subject to press the button
</span>        <span class="n">response_time</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hardware</span><span class="p">[</span><span class="s">'button'</span><span class="p">].</span><span class="n">wait</span><span class="p">()</span>

        <span class="c1"># Send the data for storage
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">node</span><span class="p">.</span><span class="n">send</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s">"DATA"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="p">{</span>
            <span class="s">'switch_time'</span><span class="p">:</span> <span class="n">switch_time</span><span class="p">,</span>
            <span class="s">'target_time'</span><span class="p">:</span> <span class="n">target_time</span><span class="p">,</span>
            <span class="s">'response_time'</span><span class="p">:</span> <span class="n">response_time</span><span class="p">,</span>
            <span class="s">'error'</span><span class="p">:</span> <span class="n">target_time</span> <span class="o">-</span> <span class="n">response_time</span>
          <span class="p">})</span>
</code></pre></div></div>

<p>In this example, we first define a data model (see section 3.2 - Data in <a class="citation" href="#saundersAUTOPILOTAutomatingExperiments2022">[218]</a>) for the Tasks <code class="language-plaintext highlighter-rouge">Params</code>, the data that the task produces as <code class="language-plaintext highlighter-rouge">TrialData</code>, and the <code class="language-plaintext highlighter-rouge">HARDWARE</code> that the task uses. Our <code class="language-plaintext highlighter-rouge">Params</code> each have a <a href="https://peps.python.org/pep-0483/">type hint</a> indicating what type of data they are, as well as a <code class="language-plaintext highlighter-rouge">Field</code> that gives further detail about them. Specifically, we have exposed the Lightswitch’s <code class="language-plaintext highlighter-rouge">on_delay</code> parameter, indicated that it will be in seconds by referring to some namespace that defines SI units <code class="language-plaintext highlighter-rouge">@si</code> and that it parameterizes the lightswitch object that we defined above. The <code class="language-plaintext highlighter-rouge">TrialData</code> is similarly annotated, and by default Autopilot will use this specification to create an hdf5 table to store the values. The <code class="language-plaintext highlighter-rouge">HARDWARE</code> dictionary makes abstract references the hardware objects that will be made available in the task, each of which would have its configuration — which GPIO pin they are plugged into, the polarity of the signal, etc. — using some local system configuration. Finally, the single <code class="language-plaintext highlighter-rouge">poll()</code> method continuously compares the value of the sensor to the threshold, switches the lightswitch when the threshold is crossed, records the time the button was pressed, and sends it for storage with its network node.</p>

<p>As before, we are using our experimental framework as an interface to our linked data system. Currently, Autopilot uses a <a href="https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki">semantic wiki</a> to organize technical knowledge and to share <a href="https://docs.auto-pi-lot.com/en/latest/guide/plugins.html">plugins</a> - <a href="https://wiki.auto-pi-lot.com">https://wiki.auto-pi-lot.com</a>. In this case, I would write my task and hardware classes inside a git repository and then add them to Autopilot’s <a href="https://wiki.auto-pi-lot.com/index.php/Autopilot_Plugins">plugin registry</a>, which uses a <a href="https://wiki.auto-pi-lot.com/index.php/Form:Autopilot_Plugin">form</a> to fill in semantic properties and allows further annotation in free text and semantic markup.</p>

<p>We could instead imagine being able to document the task in its <a href="https://peps.python.org/pep-0257/">docstring</a>, including describing the relevant subdiscipline, research topic, and any other relevant metadata. Rather than manually entering it in the wiki, then, we might export the triplet annotations directly from the class and make them available from my <code class="language-plaintext highlighter-rouge">@jonny</code> namespace and mirroring that to the wiki. Since the plugin specifies its dependencies using standard Python tools, it would then be possible for other researchers to use its task and hardware objects by referring to them as above.</p>

<p>In our pseudocode, the (abbreviated) exported metadata for this task might look like this:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#tasks:Controlled_Switch&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@autopilot:Task</span><span class="w">

  </span><span class="n">hasDescription
</span><span class="w">    </span><span class="s">"A Psychophysics experiment 
    to measure Interval Estimation."</span><span class="w">

  </span><span class="n">Discipline</span><span class="w"> </span><span class="s">"Psychophysics"</span><span class="w">
  </span><span class="n">Research_Topic</span><span class="w"> </span><span class="s">"Interval Estimation"</span><span class="w">

  </span><span class="n">Params
</span><span class="w">    </span><span class="n">on_delay</span><span class="w"> </span><span class="na">@si:seconds</span><span class="w">
      </span><span class="n">hasDescription</span><span class="w"> </span><span class="s">"..."</span><span class="w">
      </span><span class="n">parameterizes</span><span class="w"> </span><span class="na">@jonny:hardware:Lightswitch</span><span class="w">
    </span><span class="p">...</span><span class="w">

  </span><span class="n">TrialData
</span><span class="w">    </span><span class="n">switch_time</span><span class="w"> </span><span class="na">@python:datetime</span><span class="w">
    </span><span class="p">...</span><span class="w">

  </span><span class="n">usesHardware
</span><span class="w">    </span><span class="na">@autopilot:hardware:Analog_In</span><span class="w">
      </span><span class="n">hasID</span><span class="w"> </span><span class="s">"sensor"</span><span class="w">
    </span><span class="na">@autopilot:hardware:Digital_In</span><span class="w">
      </span><span class="n">hasID</span><span class="w"> </span><span class="s">"button"</span><span class="w">
    </span><span class="na">@jonny:hardware:Lightswitch</span><span class="w">
      </span><span class="n">hasID</span><span class="w"> </span><span class="s">"lightswitch"</span><span class="w">
</span></code></pre></div></div>

<p>and we might combine it with metadata that describes our particular use of it like this, where we combine that task with a series of other <code class="language-plaintext highlighter-rouge">level</code>s that shape the behavior, make it more challenging, or measure something else entirely:</p>

<p><span id="myproject-experiment"></span></p>
<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">&lt;#projects:my-project&gt;</span><span class="w">
  </span><span class="k">a</span><span class="w"> </span><span class="na">@autopilot:protocol</span><span class="w">
  </span><span class="n">experimenter</span><span class="w"> </span><span class="na">@jonny</span><span class="w">
  </span><span class="p">...</span><span class="w">

  </span><span class="n">level</span><span class="w"> </span><span class="na">@jonny:tasks:Controlled_Switch</span><span class="w">
    </span><span class="n">on_delay</span><span class="w"> </span><span class="mi">2</span><span class="w">
    </span><span class="n">threshold</span><span class="w"> </span><span class="mf">0.5</span><span class="w">
    </span><span class="n">graduation</span><span class="w"> </span><span class="na">@autopilot:graduation:ntrials</span><span class="w">
      </span><span class="n">n_trials</span><span class="w"> </span><span class="mi">200</span><span class="w">

  </span><span class="n">level</span><span class="w"> </span><span class="na">@jonny:tasks:Another_Task</span><span class="w">
    </span><span class="p">...</span><span class="w">

  </span><span class="n">hardwareConfig
</span><span class="w">    </span><span class="n">button</span><span class="w"> </span><span class="na">@autopilot:hardware:Digital_In</span><span class="w">
      </span><span class="n">gpioPin</span><span class="w"> </span><span class="mi">17</span><span class="w">
      </span><span class="n">polarity</span><span class="w"> </span><span class="mi">1</span><span class="w">
    </span><span class="n">sensor</span><span class="w"> </span><span class="na">@autopilot:hardware:Analog_In</span><span class="w">
      </span><span class="n">usesPart</span><span class="w"> </span><span class="na">@apwiki:parts:&lt;Part_Number&gt;</span><span class="w">
      </span><span class="p">...</span><span class="w">
</span></code></pre></div></div>

<p>On the other side, our output data can be automatically exported to NWB<sup id="fnref:nwbisanexample" role="doc-noteref"><a href="#fn:nwbisanexample" class="footnote" rel="footnote">53</a></sup>. Our experimental framework knows that data contained within a <code class="language-plaintext highlighter-rouge">TrialData</code> model is a <code class="language-plaintext highlighter-rouge">@nwb:behavior:BehavioralEvents</code> object, and can combine it with the metadata in our task docstring and system configuration. If we needed more specific data export - say we wanted to record the timeseries of the analog sensor - we could use the same <code class="language-plaintext highlighter-rouge">is_a</code> parameter to declare it as a <code class="language-plaintext highlighter-rouge">@nwb:TimeSeries</code> and create an extension to store the metadata about the sensor alongside it<sup id="fnref:autopilotv050" role="doc-noteref"><a href="#fn:autopilotv050" class="footnote" rel="footnote">54</a></sup>.</p>

<p>So while our code is mildly annotated and uses a mixture of standard and nonstandard terminology, we make use of the structure of the experimental framework to generate rich provenance to understand our data and task in context. It’s worth pausing to consider what this means for our infrastructural system as a whole</p>

<p>To start, we have a means of integrating our task with the knowledge that precedes it in the hardware and system configuration that runs it. In addition to documenting plugins, among others, the Autopilot wiki also has schema for <a href="https://wiki.auto-pi-lot.com/index.php/Autopilot_Behavior_Box">custom built</a> and <a href="https://wiki.auto-pi-lot.com/index.php/Parts">off-the-shelf</a> hardware like <a href="https://wiki.auto-pi-lot.com/index.php/TT_Electronics_OPB901L55">sensors</a> and <a href="https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2">sound cards</a>. These correspond to local hardware configuration entries that link them to the hardware classes required to use them<sup id="fnref:futureversions" role="doc-noteref"><a href="#fn:futureversions" class="footnote" rel="footnote">55</a></sup>. That link can be used bidirectionally: metadata about the hardware used to perform an experiment can be used in the experiment and be included with the produced data data, but the data from experiments can also be used to document the hardware. That means that usage data like calibrations and part longevity can be automatically collected and contributed to the wiki and then used to automatically configure hardware in future uses. This makes using the experimental framework more powerful, but also makes building a communal library of technical knowledge a normal part of doing experiments. Though the wiki is a transitional medium towards what we will discuss in the next section, since contributions are tracked and versioned that allows a currently undervalued class of knowledge work to be creditable.</p>

<p>This gives us a different model of designing and engineering experiments than we typically follow. Rather than designing most of it from scratch or decoding cryptic methods sections, researchers could start with a question and basic class of experiment, browse through various implementations based on different sets of tools, see which hardware they and analogous experiments use, which is then linked to the code needed to run it. From some basic information researchers would then be most of the way to performing an experiment: clone the task, download the necessary system configuration information to set up the hardware, make incremental modifications to make the experiment match what they had designed, all the while contributing and being credited for their work.</p>

<p>Much of this is possible because of the way that Autopilot isolates different components of an experiment: hardware is defined separately from tasks, both are separate from their local configuration. In addition to thinking about how to design tools for our infrastructural system, we can also think of the way it might augment existing tools. Another widely used and extremely capable tool, Bonsai <a class="citation" href="#lopesBonsaiEventbasedFramework2015a">[219, 220]</a>, is based on XML documents that <a href="https://github.com/bonsai-rx/bonsai-examples/blob/cbc2c1decc11e1dc1df920421ef88a16fd2e184c/RoiTrigger/RoiTrigger.bonsai">combine the pattern of nodes</a> that constitute an experiment with specific parameters like a <a href="https://github.com/bonsai-rx/bonsai-examples/blob/cbc2c1decc11e1dc1df920421ef88a16fd2e184c/RoiTrigger/RoiTrigger.bonsai#L76-L85">crop bounding box</a>. That makes sharing and reusing tasks difficult without exactly matching the original hardware configuration, but we could use our metadata system to <em>generate</em> code for Bonsai in addition to consuming data from it. Given some schematic pattern of nodes that describes the operation of the experiment, we could combine that with the same notion of separable parameterization and hardware configurations as we might use in Autopilot to generate the XML for a bonsai workflow. As with analytical tools, our infrastructural system could be used to make a wide array of experimental tools interoperable with an evolving set of vernacular metadata schema.</p>

<p>Together, our data, experimental, and analytical infrastructures would dramatically reshape what is possible in science. What we’ve described is a complete provenance chain that can be traced from analyzed results back through to the code and hardware used to perform the experiment. Trivially, this makes the elusive workflow where experimental data is automatically scooped up and analyzed as soon as it is collected that is typically a hard-won engineering battle within a single lab the normal mode of using the system. Developing tools that give researchers control over the mode of exported data renders the act of cleaning data effectively obsolete. The role of our experimental tool is to be able to make use of collected technical knowledge, but also to lower the barriers to using the rest of the system by integrating it with normal experimental practice.</p>

<p>The effects on collaboration and metascience are deeper though. Most scientific communication describes collecting and analyzing a single dataset. Making sense of many experiments is only possible qualitatively as a review or quantitatively as meta-analysis. Even if we have a means of linking many datasets and analysis pipelines as in the previous section, the subtle details in how a particular experiment is performed matter: things as small as milliseconds of variation in valve timings through larger differences in training sequences or task design powerfully influence the collected data. This makes comparing data from even very similar experiments — to say nothing of a class of results from a range of different experiments — a noisy and labor-intensive statistical process, to the degree that it’s possible at all. This system extends the horizon of meta-analysis to the experiment itself and turns experimental heterogeneity into a strength rather than a weakness. Is some result a byproduct of some unreported parameter in the experimental code? Is a result only visible when comparing across these different conditions? Individual experiments only allow a relatively limited set of interpretations and inferences to be drawn, but being able to look across the variation in experimental design would allow phenomena to be described in the full richness supported by available observations.</p>

<p>This would also effectively dissolve the “file drawer problem.” <a class="citation" href="#sterlingPublicationDecisionsTheir1959">[221, 222]</a> Though malice is not uncommon in science, I think it’s reasonably fair to assume that most researchers do not withhold data given a null result in order to “lie” about an effect, but because there is no reward for a potentially laborious cleaning and publication process. Collecting data that is clean and semantically annotated at the time of acquisition resolves the problem. Even without the analysis or paper, being able to index across experiments of a particular kind would make it possible to have a much fairer look at a landscape distorded by the publication process and prevent us from repeating the same experiments because no one has bothered to publish the null. This would also open new avenues for collaboration as we will explore in the next section.</p>

<p>To take stock:</p>

<p>We have described a system of three component modalities: <strong>data, analytical tools, and experimental tools</strong> connected by a <strong>linked data</strong> layer. We started by describing the need for a <strong>peer-to-peer</strong> data system that makes use of <strong>data standards</strong> as an onramp to linked metadata. To interact with the system, we described an identity-based linked data system that lets individual people declare linked data resources and properties that link to <strong>content addressed</strong> resources in the p2p system, as well as <strong>federate</strong> into multiple larger organizations. We described the requirements for <strong>DAG-based analytical frameworks</strong> that allow people to declare individual nodes for a processing chain linked to code, combine them into workflows, and apply them to data. Finally, we described a design strategy for <strong>component-based experimental frameworks</strong> that lets people specify experimental metadata, tools, and output data.</p>

<p>This system as described is a two-layer system, with a few different domains linked by a flexible metadata linking layer. The metadata system as described is not merely <em>inert</em> metadata, but metadata linked to code that can <em>do something</em> — eg. specify access permissions, translate between data formats, execute analayis workflows, parameterize experiments, etc. Put another way, we have been attempting to describe a system that <em>embeds the act of sharing and curation in the practice of science.</em> Rather than a thankless post-hoc process, the system attempts to provide a means for aligning the daily work of scientists so that it can be cumulative and collaborative. To do this, we have tried to avoid rigid specifications of system structure, and instead described a system that allows researchers to pluralistically define the structure themselves.</p>

<div class="draft-text">
Point here is to lead into interfaces --- these are two examples of classes of interfaces to and from the linked data system. Ways to create, read, and use links. Translation of our metadata system into code and computation. 

    
We also haven't described any sort of governance or development system that makes these packages anything more than "some repository on GitHub somewhere" with all the propensity to calcify into fiefdoms that those entail. This leads us back to a system of communication, the central piece of missingness that we have been circling around the whole piece. If you'll allow me one more delay, I want to summarize the system so far before finally arriving there.
</div>

<h2 id="shared-knowledge">Shared Knowledge</h2>

<p>The remaining set of problems implied by the infrastructural system sketched so far are the <em>communication</em> and <em>organization</em> systems that make up the interfaces to maintain and use it. We can finally return to some of the breadcrumbs laid before: the need for negotiating over distributed and conflicting data schema, for incentivizing and organizing collective labor, and for communicating information within and without academia.</p>

<p>The communication systems that are needed double as <em>knowledge organization</em> systems. Knowledge organization has the rosy hue of something that might be uncontroversial and apolitical — surely everyone involved in scientific communication wants knowledge to be organized, right? The reality of scientific practice might give a hint at our naivete. Despite being, in some sense, itself an effort to organize knowledge, <em>scientific results effectively have no system of explicit organization.</em> There is no means of, say, “finding all the papers about a research question.”<sup id="fnref:marderjoyof" role="doc-noteref"><a href="#fn:marderjoyof" class="footnote" rel="footnote">56</a></sup> The problem is so fundamental it seems natural: the usual methods of using search engines, asking around on Twitter, and chasing citation trees are flex tape slapped over the central absence of a system for formally relating our work as a shared body of knowledge.</p>

<p>Information capitalism, in its terrifying splendor, here too pits private profit against public good. Analogously to the necessary functional limitations of SaaS platforms, artificially limiting knowledge organization opens space for new products and profit opportunities. In their 2020 shareholder report, RELX, the parent of Elsevier, lists increasing the number of journals and papers as a primary means of increasing revenue <a class="citation" href="#RELXAnnualReport2020">[48]</a>. This represents a shift in their business model from subscriptions to deals like open access, which according to RELX CEO Erik Nils Engström “is where revenue is priced per article on a more explicit basis” <a class="citation" href="#relx2020ResultsPresentation2021">[224]</a>.</p>

<p>In the next breath, they describe how “in databases &amp; tools and electronic reference, representing over a third of divisional<sup id="fnref:whatdivision" role="doc-noteref"><a href="#fn:whatdivision" class="footnote" rel="footnote">57</a></sup> revenue, we continued to drive good growth through content development and enhanced machine learning [ML] and natural language processing [NLP] based functionality.”</p>

<p>What ML and NLP systems are they referring to? The 2019 report is a bit more revealing (emphases mine):</p>

<blockquote>
  <p>Elsevier looks to enhance quality by building on its premium brands and <strong>grow article volume</strong> through <strong>new journal launches,</strong> the expansion of open access journals and growth from emerging markets; and add value to core platforms by implementing capabilities such as <strong>advanced recommendations on ScienceDirect and social collaboration through reference manager and collaboration tool Mendeley.</strong></p>

  <p><strong>In every market, Elsevier is applying advanced ML and NLP techniques</strong> to help researchers, engineers and clinicians perform their work better. For example, in research, ScienceDirect Topics, a free layer of content that enhances the user experience, uses <strong>ML and NLP techniques to classify scientific content and organise it thematically,</strong> enabling users to get faster access to relevant results and related scientific topics. The feature, launched in 2017, is proving popular, generating 15% of monthly unique visitors to ScienceDirect via a topic page. <strong>Elsevier also applies advanced ML techniques that detect trending topics per domain,</strong> helping researchers make more informed decisions about their research. <strong>Coupled with the automated profiling and extraction of funding body information from scientific articles,</strong> this process supports the whole researcher journey; from planning, to execution and funding. <a class="citation" href="#RELXAnnualReport2019">[225]</a></p>
</blockquote>

<p>Reading between the lines, it’s clear that the difficulty of finding research is a feature, not a bug of their system. Their explicit business model is to increase the number of publications and sell organization back to us with recommendation services. The recommendation system might be free<sup id="fnref:notfree" role="doc-noteref"><a href="#fn:notfree" class="footnote" rel="footnote">58</a></sup>, but the business is to maintain the self-reinforcing system of prestige where researchers compete for placement in highly visible journals to stand out among a wash of papers, in the process reifying the mythology <a class="citation" href="#brembsPrestigiousScienceJournals2018">[226]</a> of the “great journals.” With semantic structure to locate papers, it becomes much more difficult to sell high citation count as a product — people can find what they need, rather than needing to pay attention to a few high-profile journals. Without it, which papers might a paper discovery system created by a publisher recommend? The transition from a strictly journal-based discovery system to a machine learning powered search and feed model mirrors the strategic displacement of explicit organization by search in the rest of the digital economy, and presents similar opportunities for profit. Every algorithmically curated feed is an opportunity to sell ad placement<sup id="fnref:nativeads" role="doc-noteref"><a href="#fn:nativeads" class="footnote" rel="footnote">59</a></sup> — which they proudly describe as looking very similar to their research content <a class="citation" href="#springernatureBrandedContent">[227, 161]</a>.</p>

<p>The extended universe of profitmaking from knowledge disorganization gets more sinister: Elsevier sells multiple products to recommend ‘trending’ research areas likely to win grants, rank scientists, etc., algorithmically filling a need created by knowledge disorganization. The branding varies by audience, but the products are the same. For pharmaceutical companies <a href="https://www.elsevier.com/solutions/professional-services/drug-design-optimization#opportunity">“scientific opportunity analysis”</a> promises custom reports that answer questions like “Which targets are currently being studied?” “Which experts are not collaborating with a competitor?” and “How much funding is dedicated to a particular area of research, and how much progress has been made?” <a class="citation" href="#elsevierDrugDesignOptimization">[230]</a>. For academics, <a href="https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science#how">“Topic Prominence in Science”</a> offers university administrators tools to “enrich strategic research planning with portfolio overviews of their own and peer institutions.” Researchers get tools to “identify experts and potential cross-sector collaborators in specific Topics to strengthen their project teams and funding bids and identify Topics which are likely to be well funded.” <a class="citation" href="#elsevierTopicProminenceScienceb">[231]</a>  This reflects RELX’s transition “from electronic reference, information reference tools, databases to […] analytics and decision tools.” (missing reference) Publishing is old news, the real money is in tools for extending control through the rest of the process of research.</p>

<p>These tools are, of course, designed for a race to the bottom — if my colleague is getting an algorithmic leg up, how can I afford not to? Naturally only those labs that <em>can</em> afford them and the costs of rapidly pivoting research topics will benefit from them, making yet another mechanism that reentrenches scientific inequity for profit. Knowledge disorganization, coupled with a little surveillance capitalism that monitors the activity of colleagues and rivals <a class="citation" href="#brembsReplacingAcademicJournals2021">[25, 232]</a>, has given publishers powerful control over the course of science, and they are more than happy to ride algorithmically amplified scientific hype cycles in fragmented research bubbles all the way to the bank.</p>

<p>One more turn of the screw: the ability of the (former) publishers to effectively invent the metrics that operationalize “prestige” in the absence of knowledge organization sytems gives them broad leverage with governments and funding agencies. In an environment of continuously dwindling budgets and legislative scrutiny, seemingly mutually beneficial platform contracts offer the sort of glossy comfort that only predictive analytics can. In 2020 the National Research Foundation of Korea (NRF) and Elsevier published a joint report that used a measurement derived from citation counts - “Field-weighted citation impact”, or FWCI - to argue for the underrated research prestige of South Korea <a class="citation" href="#researchfoundationofkoreaSouthKoreaTechnological2020">[233]</a>. While I don’t dispute the value of South Korea’s research program, the apparent bargain that was struck is chilling. South Korea gets a very fancy report arguing that more scientists in other countries should work with theirs, and Elsevier gets to cement itself into the basic operation of science. Elsevier controls the journals that can guarantee high citation counts <em>and</em> the metrics built on top of them. The Brain Korea program Phase II report <sup id="fnref:rand" role="doc-noteref"><a href="#fn:rand" class="footnote" rel="footnote">60</a></sup> <a class="citation" href="#seongBrainKorea212008">[234]</a>, issued just before the 2009 formation of the NRF argued that rankings and funding should be dependent on citation counts. The NRF now relies on SciVal and their FWCI measurement as a primary means of ranking researchers and determining funding, built into the Brain Korea 21 funding system <a class="citation" href="#elsevierCaseStudyNational2019">[235, 236]</a>. Without exaggeration, scientific disorganization and reliance on citation counts allowed Elsevier to buy control over the course of research in South Korea.</p>

<p>The consequences for science are hard to overstate. In addition to literature search being an unnecessarily huge sink of time and labor,  science operates as a wash of tail-chasing results that only rarely seem to cumulatively build on one another. The need to constantly reinforce the norm that purposeful failure to cite prior work is research misconduct is itself a symptom of how engaging with a larger body of work is both extremely labor intensive and <em>strictly optional</em> in the communication regime of journal publication. The combination of more publications translating into more profit and the strategic disorganization of science contributes to conditions for scientific fraud. An entirely fraudulent paper can be undetectable even by domain experts. Since papers can effectively be islands — given legitimacy by placement in a journal strongly incentivized to accept all comers — and there is no good means of evaluating them in context with their immediate semantic neighbors, investigating fraud is extremely time consuming and almost entirely without reward. And since traditional peer review happens once, rather than as a continual public process, the only recourse outside of posting on PubPeer is to wait on journal editorial boards to self-police by reviewing each individual complaint. Forensic peer-reviewers have been ringing the alarm bell, saying that there is “no net” to bad research <a class="citation" href="#heathersRealScandalIvermectin2021">[237]</a>, and brave and highly-skilled investigators like <a href="https://scienceintegritydigest.com/">Elisabeth Bik</a> have found thousands of papers with evidence of purposeful manipulation <a class="citation" href="#shenMeetThisSuperspotter2020">[238, 239]</a>. The economic structure of for-profit journals pits their profit model against their function as providing a venue for peer review — the one function most scientists are still sympathetic to. Trust in science is critical for addressing our most dire problems from global pandemics to climate change <a class="citation" href="#westMisinformationScience2021">[240]</a>, but attitudes towards scientists are lukewarm at best <a class="citation" href="#kennedyAmericansTrustScientists2022">[241]</a>. Even when it isn’t fake news, why would anyone trust us when it’s <em>effectively impossible</em>  to find or assess the quality of scientific information? <a class="citation" href="#krauseTrustFallacyScientists2021">[242]</a> Not even scientists can: despite the profusion of papers, by some measures progress in science has slowed to a crawl <a class="citation" href="#chuSlowedCanonicalProgress2021">[243]</a>.</p>

<p>While Chu and Evans <a class="citation" href="#chuSlowedCanonicalProgress2021">[243]</a> correctly diagnose <em>symptoms</em> of knowledge disorganization like the need to “resort to heuristics to make continued sense of the field” and reliance on canonical papers, by treating the journal model as a natural phenomenon and citation as the only means of ordering research, they misattribute root <em>causes.</em> The problem is not people publishing <em>too many papers,</em> or a <em>breakdown of traditional publication hierarchies,</em> but the <em>profitability of knowledge disorganization.</em> Their prescription for “a clearer hierarchy of journals” misses the role of organizing scientific work in journals ranked by prestige, rather than by the content of the work, as a potentially major driver of extremely skewed citation distributions. It also misses the publisher’s stated goals of <em>publishing more papers</em> within an ecosystem of algorithmic recommendations, and there is nothing recommendation algorithms love recommending more than things that are already popular. Without diagnosing knowledge disorganization as a core part of the business model of scientific publishers, we can be led to prescriptions that would make the problem worse.</p>

<p>It’s hard to imagine an alternative to journals that doesn’t look like, well, journals. While a full treatment of the journal system is outside the scope of this paper, the system we describe here renders them <em>effectively irrelevant</em> by making papers as we know them <em>unnecessary.</em> Rather than facing the massive collective action problem of asking everyone to change their publication practices on a dime, by reconsidering the way we organize the surrounding infrastructure of science we can flank journals and replace them “from below” with something qualitatively more useful.</p>

<p>Beyond journals, the other technologies of communication that have been adopted out of need, though not necessarily design, serve as <a href="https://en.wikipedia.org/wiki/Desire_path">desire paths</a> that trace other needs for scientific communication. As a rough sample: Researchers often prepare their manuscripts using platforms like Google Drive, indicating a need for collaborative tools in perparation of an idea. When working in teams, we often use tools like Slack to plan our work. Scientific conferences reflect the need for federated communication within subdisciplines, and we have adopted Twitter as a de facto platform for socializing and sharing our work to a broader audience. We use a handful of blogs and other sites like <a href="https://edspace.american.edu/openbehavior/">OpenBehavior</a> <a class="citation" href="#whiteFutureOpenOpenSource2019">[244]</a>, <a href="https://open-neuroscience.com/">Open Neuroscience</a>, and many others to index technical knowledge and tools. Last but not finally, we use sites like <a href="https://pubpeer.com">PubPeer</a> and ResearchGate for comment and criticism.</p>

<p>These technologies point to a few overlapping and not altogether binary axes of communication systems.</p>

<ul>
  <li><strong>Durable vs Ephemeral</strong> - journals seek to represent information as permanent, archival-grade material, but scientific communication also necessarily exists as contextual, temporally specific snapshots.</li>
  <li><strong>Structured vs Chronological</strong> - scientific communication both needs to present itself as a structured basis of information with formal semantic linking, but also needs the chronological structure that ties ideas to their context. This axis is a gradient from formally structured references, through intermediate systems like forums with hierarchical topic structure that embeds a feed, to the purely chronological feed-based social media systems.</li>
  <li><strong>Messaging vs Publishing</strong> - Communication can be person-to-person, person-to-group with defined senders and recipients, or person-to-all statement to an undefined public. This ranges from private DMs through domain-specific tool indexes like OpenBehavior through the uniform indexing of Wikipedia.</li>
  <li><strong>Public vs. Private</strong> - Who gets to read, who gets to contribute? Communication can be composed of entirely private notes to self, through communication in a lab, collaboration group, discipline, and landing in the entirely public realm of global communication.</li>
  <li><strong>Formal vs. Informal</strong> - Journal articles and encyclopedia-bound writing that conforms to a particular modality of expression vs. a vernacular style intended to communicate with people outside the jargon culture.</li>
  <li><strong>Push vs. Pull</strong> - Do you go to get information from a reference location, or does information come to you as an alert or message? Or, generally, where is the information “located,” is an annotation pushed and overlaid on a document, or stored elsewhere requiring the audience to explicitly pull it?</li>
</ul>

<p>“Peer reviewed vs. unrefereed” is purposely excluded as an axis of communication tools, as the ability to review and annotate multiple versions of a document — subject to the context of the medium — should be a basic part of any communication system. Fear over losing the at once immutable but also paradoxically fragile ecosystem of journal-led peer review is one of the first strawmen that stops consideration of radically reorganizing scientific communication<sup id="fnref:quittingreviewing" role="doc-noteref"><a href="#fn:quittingreviewing" class="footnote" rel="footnote">61</a></sup>. The belief that peer review as we know it is an intrinsic part of science is ahistorical (eg. <a class="citation" href="#baldwinScientificAutonomyPublic2018">[245]</a>), and the belief that journal-led peer review is somehow a unique venue for evaluating scientific work ignores the immense quantity of criticism and discussion that happens in almost every communicative context, scientific and otherwise. The notion that the body of scientific knowledge is best curated by passing each paper through a gauntlet of three anonymous reviewers, after which it becomes Fact is ridiculous on its face. Focusing on preserving peer review is a red herring that unnecessarily constrains the possible forms of scientific communication. Instead we will try and sketch systems that address the needs for communication and knowledge organization left unmet precisely because of the primacy of peer reviewed journal publications.</p>

<p>Clearly a variety of different types of communication tools are needed, but there is no reason that each of them should be isolated and inoperable with the others. We have already seen several of the ideas that help bring an alternative into focus. Piracy communities demonstrate ways to build social systems that can sustain distributed infrastructure. Federated and protocol-based systems show us that we don’t need to choose between a single monolithic system or many disconnected ones, but can have a heterogeneous space of tools linked by a basic protocol. The semantic web gives us the unfilled promise of triplet links as a very general means of structuring data and building interfaces for disparate systems. We can bridge these lessons with some from wiki culture to get a more practical sense of distributed governance and organization. Together with our sketches of data, analytical, and experimental tools we can start imagining a system for coordinating them — as well as displacing some of the more intractable systems that misstructure the practice of science.</p>

<h3 id="the-wiki-way">The Wiki Way</h3>

<blockquote>
  <p>If we take radical collaboration as our core, then it becomes clear that extending Wikipedia’s success doesn’t simply mean installing more copies of wiki software for different tasks. It means figuring out the key principles that make radical collaboration work. What kinds of projects is it good for? How do you get them started? How do you keep them growing? What rules do you put in place? What software do you use? <a class="citation" href="#swartzMakingMoreWikipedias2006">[246]</a></p>
</blockquote>

<blockquote>
  <p>So that’s it — insecure but reliable, indiscriminate and subtle, user hostile yet easy to use, slow but up to date, and full of difficult, nit-picking people who exhibit a remarkable community camaraderie. Confused? Any other online community would count each of these “negatives” as a terrible flaw, and the contradictions as impossible to reconcile. Perhaps wiki works because the other online communities don’t. <a class="citation" href="#leufWikiWayQuick2001a">[247]</a> and in <a href="http://wiki.c2.com/?WhyWikiWorks">WhyWikiWorks</a><sup id="fnref:wikiwording" role="doc-noteref"><a href="#fn:wikiwording" class="footnote" rel="footnote">62</a></sup></p>
</blockquote>

<p>Aside from maybe the internet itself, there is no larger public digital knowledge organization effort than Wikipedia. While there are many lessons to be learned from Wikipedia itself, it emerged from a prior base of thought and experimentation in radically permissive, self-structuring read/write — sometimes called “peer production” <a class="citation" href="#hillWikipediaEndOpen2019">[248]</a> — communities. Wikis are now quasi-ubiquitous<sup id="fnref:corporatewikis" role="doc-noteref"><a href="#fn:corporatewikis" class="footnote" rel="footnote">63</a></sup>, perhaps largely thanks to Wikipedia, but its specific history and intent to be an <em>encyclopedia</em> entwines it with a very particular technological and social system that obscures some of the broader dreams of early wikis.</p>

<p>Aaron Swartz recounts a quote from Jimmy Wales, co-founder of Wikipedia:</p>

<blockquote>
  <p>“I’m not a wiki person who happened to go into encyclopedias,” Wales told the crowd at Oxford. “I’m an encyclopedia person who happened to use a wiki.” <a class="citation" href="#swartzWhoWritesWikipedia2006">[249]</a></p>
</blockquote>

<p>And further describes how this origin and mission differentiates it from other internet communities:</p>

<blockquote>
  <p>But Wikipedia isn’t even a typical community. Usually Internet communities are groups of people who come together to discuss something, like cryptography or the writing of a technical specification. Perhaps they meet in an IRC channel, a web forum, a newsgroup, or on a mailing list, but the focus is always something “out there”, something outside the discussion itself.</p>

  <p>But <strong>with Wikipedia, the goal is building Wikipedia.</strong> It’s not a community set up to make some other thing, it’s a community set up to make itself. And since Wikipedia was one of the first sites to do it, we know hardly anything about building communities like that. <a class="citation" href="#swartzMakingMoreWikipedias2006">[246]</a></p>
</blockquote>

<p>We know a lot more now than in 2006, of course, but Wikipedia still has outsized structuring influence on our beliefs about what Wikis can be. Wikipedia has since spawned a <a href="https://meta.wikimedia.org/wiki/Complete_list_of_Wikimedia_projects">large number</a> of technologies and projects like <a href="https://meta.wikimedia.org/wiki/Wikidata">Wikidata</a> and <a href="https://commons.wikimedia.org/wiki/Main_Page">Wikimedia Commons</a>, each with their own long and occasionally torrid histories. I won’t dwell on the obvious and massive feat of collective organzation that the greater Wikipedia project represents — we should build on and interoperate with its projects and respect the amount of work the foundation and its editors have put in to preserve free access to information, but learning from its imperfections is more useful to us here, especially for things that aren’t encyclopedias. The dream of a centralized, but mass-edited “encyclopedia of everything” seems to be waning, and its slow retreat from wild openness has run parallel to a long decline in contributors <a class="citation" href="#hillWikipediaEndOpen2019">[248, 250]</a>. Throughout that time, there has been a separate (and largely skeptical) set of wiki communities holding court on what a radically open web can be like, inventing their worlds in real time. These communities have histories that are continuous with one another, and in their mutual reaction and inspiration sometimes teach similar lessons from across the divides of their very different structure.</p>

<p>The first wiki was launched in 1995<sup id="fnref:wikistart" role="doc-noteref"><a href="#fn:wikistart" class="footnote" rel="footnote">64</a></sup> (<a href="http://wiki.c2.com/">it’s still up</a>) and came to be known as Ward’s wiki after its author <a href="http://wiki.c2.com/?WardCunningham">WardCunningham</a>. Technically, it was extremely simple: a handful of <a href="http://wiki.c2.com/?TextFormattingRules">TextFormattingRules</a> and use of <a href="http://wiki.c2.com/?WikiCase">WikiCase</a> where if you <a href="http://wiki.c2.com/?JoinCapitalizedWords">JoinCapitalizedWords</a> you create a link to a (potentially new) <a href="http://wiki.c2.com/?WikiPage">WikiPage</a> — and the ability for anyone to edit any page. These very simple <a href="http://wiki.c2.com/?WikiDesignPrinciples">WikiDesignPrinciples</a> led to a sprawling and continuous conversation that spanned more than a <a href="http://wiki.c2.com/?WardsWikiTenthAnniversary">decade</a> and thousands<sup id="fnref:npages" role="doc-noteref"><a href="#fn:npages" class="footnote" rel="footnote">65</a></sup> of pages that, because of the nature of the medium, is left fully preserved in amber. Those conversations are a history of thought on what makes wiki communities work (eg. <a href="http://wiki.c2.com/?WhyWikiWorks">WhyWikiWorks</a>, <a href="http://wiki.c2.com/?WhyWikiWorksNot">WhyWikiWorksNot</a>), and what is needed to sustain them.</p>

<p>One tension that emerged early without satisfying resolution is the balance between “<a href="http://wiki.c2.com/?DocumentMode">DocumentMode</a>” writing that serves as linearly-readable reference material, similar to that of Wikipedia, and “<a href="http://wiki.c2.com/?ThreadMode">ThreadMode</a>” writing that is a nonlinear representation of a conversation. Order vs spontaneity is a fundamental challenge of inventing culture in plaintext. The purpose of using a wiki as opposed to other technologies that existed at the time like bulletin boards, newsgroups, IRC, etc. was that it provided a means of fluid structure<sup id="fnref:wardcause" role="doc-noteref"><a href="#fn:wardcause" class="footnote" rel="footnote">66</a></sup>. The parallel need to communicate and attribute work made it a seeming inevitability that even if you went out of your way to restructure a lot of writing into a sensible DocumentMode page, someone would soon after create a new horizontal divider and start a fresh ThreadMode section.</p>

<p>Ward Cunningham and other more organizationally-oriented contributors opposed ThreadMode (eg. <a href="http://wiki.c2.com/?ThreadModeConsideredHarmful">ThreadModeConsideredHarmful</a>, <a href="http://wiki.c2.com/?InFavorOfDissertation">InFavorOfDissertation</a>) for a number of reasons, largely due to the <a href="http://wiki.c2.com/?ThreadMess">ThreadMess</a> and <a href="http://wiki.c2.com/?WikiChaos">WikiChaos</a> it had the potential of creating.</p>

<blockquote>
  <p>I occasionally suggest how this site should be used. My <a href="http://wiki.c2.com/?GoodStyle">GoodStyle</a> suggestions have been here since the beginning and are linked from the edit page should anyone forget. I have done my best to discourage dialog <a href="http://wiki.c2.com/?InFavorOfDissertation">InFavorOfDissertation</a> which offers a better fit to this medium. I’ve been overruled. I will continue to make small edits to pages for the sake of brevity. – <a href="http://wiki.c2.com/?WardCunningham">WardCunningham</a> <a class="citation" href="#C2wikiWikiHistory">[251]</a></p>
</blockquote>

<p>Most pages are thus a combination of both, usually with some DocumentMode text at the top with ThreadMode conversations interspersed throughout without necessarily having any clean delineation between the two. Far from just being raw disorder, this mixed mode of writing gave it a peculiar character of being <em>both</em> a folk reference for a library of concepts <em>as well as</em> a history of discussion that made the contingency of that reference material plain. Beka Valentine put it well:</p>

<blockquote>
  <p>c2wiki is an exercise in dialogical methods. of laying bare the fact that knowledge and ideas are not some truth delivered from On High, but rather a social process, a conversation, a dialectic, between various views and interests <a class="citation" href="#valentineC2wikiExerciseDialogical2021">[252]</a></p>
</blockquote>

<p>This tension and its surrounding discussions point to the need for multiple representations of a single idea: that both the social and reference representations of a concept are valuable, but aren’t necessarily best served by being represented in the same place. There was relatively common understanding that the intended order of things was to have many ThreadMode conversations that would gradually be converted to DocumentMode in a process of <a href="http://wiki.c2.com/?BrainStormFirstCleanLater">BrainStormFirstCleanLater</a>. Many <a href="http://wiki.c2.com/?ConvertThreadModeToDocumentMode">proposed solutions</a> orbit around making parallel pages with similar names (like &lt;pagename&gt;Discussion) to clean up a document while preserving the threads (though there were plenty of interesting alternatives, eg. <a href="http://wiki.c2.com/?DialecticMode">DialecticMode</a>)<sup id="fnref:contemporarywikithought" role="doc-noteref"><a href="#fn:contemporarywikithought" class="footnote" rel="footnote">67</a></sup>.</p>

<p>Wikipedia, in its attendant <a href="http://wiki.c2.com/?WikiEngines">WikiEngine</a> <a href="https://meta.wikimedia.org/wiki/MediaWiki">MediaWiki</a>, cut the Gordian Knot by splitting each page into a separate <em>Article</em> and <em>Talk</em> pages, with the talk page in its own <strong>Namespace</strong> – eg. <a href="https://en.wikipedia.org/wiki/Gordian_Knot">Gordian_Knot</a> vs <a href="https://en.wikipedia.org/wiki/Talk:Gordian_Knot">Talk:Gordian_Knot</a>. Talk pages resemble a lot of the energy of early wikis: disorganized, sometimes silly, sometimes angry, and usually charmingly pedantic. Namespaces extend the traditional “everything is a page” notion encoded in the WikiCase link system by giving different pages different roles. In addition to having parallel conversations on articles and talk pages, it is possible to have template pages that can be included on wiki pages with <code class="language-plaintext highlighter-rouge">{{double curly bracket}}</code> syntax – eg. <a href="https://en.wikipedia.org/wiki/Template:Citation_needed">Template:Citation_Needed</a> renders <code class="language-plaintext highlighter-rouge">{{Citation needed}}</code> as <sup><i>[citation needed]</i></sup>. Talk pages have their own <strong>functional differentiation,</strong> with features for threading and annotating discussions that aren’t present on the main article pages (see <a href="https://en.wikipedia.org/wiki/Wikipedia:Flow">Wikipedia:Flow</a> <a class="citation" href="#WikipediaFlow2021">[253]</a>). Generalized beyond the context of wikis, functional differentiation of a single item into its multiple representations is relatively common in computing: eg. this document exists as a <a href="https://github.com/sneakers-the-rat/infrastructure">git repository</a>, the <span id="gobackhere"><a href="/infrastructure/goback.html">rendered page</a></span>, a <a href="/infrastructure/tex/decentralized_infrastructure_render.pdf">pdf</a>, hypothes.is annotations, etc.</p>

<p>The complete segregation of discussion to Talk pages is driven by Wikipedia’s exclusivity as an encyclopedia, with reminders that it is the “<a href="https://en.wikipedia.org/wiki/Wikipedia:Don't_lose_the_thread#Move_to_the_article_talk_page">sole purpose</a>” peppered throughout the rules and guidelines. The presence of messy subjective discussions would of course be discordant with the very austere and “neutral” articles of an encyclopedia. There are no visible indications that the talk pages even exist in the main text, and so even deeply controversial topics have no references to the conversations in talk pages that contextualize them — despite this being a requested feature by both administrators and editors <a class="citation" href="#schneiderUnderstandingImprovingWikipedia2011">[254]</a>.</p>

<p>Talk pages serve as one of the primary points of coordination and conflict resolution on Wikipedia, and also provide a low-barrier entrypoint for questions posed to a space they perceive to be “an approchable community of experts” <a class="citation" href="#viegasTalkYouType2007">[255]</a>. The separation of Talk pages and the <a href="https://en.wikipedia.org/wiki/Wikipedia:Talk_page_guidelines">labyrinthine rules</a> governing their use function to obscure the dialogical and collective production of knowledge at the heart of wikis and Wikipedia. The body of thought that structures Wikipedia, most of which is in its <a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal">Wikipedia:*</a> namespace, is immense and extremely valuable, but is largely hidden except from those who care to look for it. Since Wikipedia is “always already there” often without trace of its massively collective nature, relatively few people ever contribute to it. Reciprocally, since acknowledging personal contribution is or point of view is <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">explicitly</a> against some of its <a href="https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view">core</a> policies and <a href="https://en.wikipedia.org/wiki/Wikipedia:Avoid_thread_mode">traditions</a>, there is little public credit outside the Wikipedia community itself for the labor of maintaining it.</p>

<p>The forking of Wards Wikis into the first <a href="http://wiki.c2.com/?SisterSites">SisterSites</a> teaches a parallel strain of lessons. Ward’s Wiki started as a means of organizing knowledge for the Portland Pattern Repository<sup id="fnref:wardinvite" role="doc-noteref"><a href="#fn:wardinvite" class="footnote" rel="footnote">68</a></sup>, a programming community (referred to as <a href="http://wiki.c2.com/?DesignPatterns">DesignPatterns</a> below), and in 1998 they were overwhelmed with proponents of <a href="http://wiki.c2.com/?ExtremeProgramming">ExtremeProgramming</a> (or XP), which caused the first fissure in the wiki:</p>

<blockquote>
  <p>XP advocates seemed to be talking about XP at every possible opportunity and seemingly on every page with content the least bit related to software development. This annoyed a number people who were here to discuss patterns, leading to the tag <a href="http://wiki.c2.com/?XpFreeZone">XpFreeZone</a>, as a request not to talk about ExtremeProgramming on that page.</p>

  <p>It was difficult to pick out the <a href="http://wiki.c2.com/?DesignPatterns">DesignPatterns</a> discussion on <a href="http://wiki.c2.com/?RecentChanges">RecentChanges</a><sup id="fnref:recentchanges" role="doc-noteref"><a href="#fn:recentchanges" class="footnote" rel="footnote">69</a></sup>, because most of the activity was related to ExtremeProgramming. Eventually, most of the <a href="http://wiki.c2.com/?DesignPatterns">DesignPatterns</a> people left, to discuss patterns in a “quieter” environment, and people started referring to this site as <a href="http://wiki.c2.com/?WardsWiki">WardsWiki</a> instead of the <a href="http://wiki.c2.com/?PortlandPatternRepository">PortlandPatternRepository</a> <a class="citation" href="#C2wikiWikiHistory">[251]</a></p>
</blockquote>

<p>One of the first and most influential Sister Sites was <a href="http://meatballwiki.org/">Meatball Wiki</a>, described on Wards Wiki:</p>

<blockquote>
  <p>SunirShah founded MeatballWiki to absorb and enlarge the discussion of what wiki and wiki like sites might be. That discussion still simmers here. But here it can take on a negative tone sounding more like complaining. On meatball, under Sunir’s careful leadership, the ideas, wild or not, stay amazingly upbeat. - <a href="http://wiki.c2.com/?SisterSites">SisterSites</a></p>
</blockquote>

<p>MeatballWiki became the spiritual successor to Ward’s Wiki, which at that point had its own momentum of culture less interested in being the repository of wiki thought<sup id="fnref:meatballthought" role="doc-noteref"><a href="#fn:meatballthought" class="footnote" rel="footnote">70</a></sup>. Meatball has its own prolific history of thought, including reflections on its very existence as a SisterSite. These were a series of discussions that melded thoughts from open source computing with social systems; in part: <a href="http://meatballwiki.org/wiki/RightToFork">RightToFork</a>, <a href="http://meatballwiki.org/wiki/RightToLeave">RightToLeave</a>, <a href="http://meatballwiki.org/wiki/EnlargeSpace">EnlargeSpace</a>, and <a href="http://meatballwiki.org/wiki/TransClusion">TransClusion</a>.</p>

<p>What can be done when the internal divisions in a wiki community and the weight of its history make healthy contribution impossible? The simplest is to exercise the <a href="http://meatballwiki.org/wiki/RightToLeave">RightToLeave</a>, as it is almost always possible to just stop being part of a digital community. This approach is clearly the most destructive, as it involves abandoning the emotional bonds of a community, prior work (see the <a href="http://wiki.c2.com/?WikiMindWipe">WikiMindWipe</a> where a user left and took all their contributions with them), and doesn’t necessarily provide an alternative that alleviates the cause of the tension. The next idea is to <em>fork</em> the community, where its body — in the case of wikis the pages and history — can be duplicated so that it can proceed along two parallel tracks. Exercising the right to fork is, according to Meatball, “people exercising their RightToLeave whilst maintaining their emotional stake” <a class="citation" href="#MeatballWikiRightToLeave">[256]</a>.</p>

<p>The discussion around the Right to Fork on Meatball is far from uniformly positive, and is certainly colored by the strong presence of its <a href="http://meatballwiki.org/wiki/BenevolentDictator">BenevolentDictator</a> Sunir Shah who viewed it as a last resort after all attempts at <a href="http://meatballwiki.org/wiki/ConflictResolution">ConflictResolution</a> have failed. They point to the potentially damaging effects of a fork, like bitterness, disputes over content ownership (see <a href="http://meatballwiki.org/wiki/MeatballIsNotFree">MeatballIsNotFree</a>), and potentially an avoidance of conflict resolution that is a normal and healthy part of any community. Others place it more in the realm of a radical <em>political</em> action rather than a strictly social action. Writing about the fork of OpenOffice to LibreOffice, Terry Hancock writes:</p>

<blockquote>
  <p>[In] proprietary software [a] political executive decision can kill a project, regardless of developer or user interest. But with free software, the power lies with the people who make it and use it, and the freedom to fork is the guarantee of that power. […] The freedom to fork a free software project is [a] “tool of revolution” intended to safeguard the real freedoms in free software. <a class="citation" href="#hancockOpenOfficeOrgDead2010">[257]</a></p>
</blockquote>

<p>Forking digital communities can be much less acrimonious than physically-based communities because of the ability to <a href="http://meatballwiki.org/wiki/EnlargeSpace">EnlargeSpace</a> given by the medium:</p>

<blockquote>
  <p>In order to preserve <a href="http://meatballwiki.org/wiki/GlobalResource">GlobalResources</a>, create more public space. This reduces limited resource tension. Unlike the <a href="http://meatballwiki.org/wiki/RealWorld">RealWorld</a>, land is cheap online. In effect, this nullifies the <a href="http://meatballwiki.org/wiki/GlobalResource">TragedyOfTheCommons</a> by removing the resource pressure that created the “tragedy” in the first place. <strong>You can’t overgraze the infinity.</strong> - <a class="citation" href="#MeatballWikiEnlargeSpace">[258]</a></p>
</blockquote>

<p>Enlarging space has the natural potential to make the broader social scene bewildering with a geyser of pages and communities, but can be made less damaging by having mechanisms to link histories, trace their divergence, and potentially resolve a fork as is common in open source software development. Forking is then a natural process of community regeneration, allowing people to regroup to make healthier spaces when needed, where the fork is itself part of the history of the community rather than an unfathomable rift.</p>

<p>Forking communities is not the same as forking community resources: “you can’t fork a community […] what you can do is fork the content and to <em>split</em> the community” <a class="citation" href="#MeatballWikiForkingOfOnlineCommunitiesa">[259]</a>. As described so far, a fork divides people into unreconciled and separate communities. In some cases this makes forking difficult, in others it makes it impossible: the prime example, again, is Wikipedia. It is simply too large and too culturally dominant to fork. Even though it is <a href="https://en.wikipedia.org/wiki/Wikipedia:FAQ/Forking#Am_I_allowed_to_fork_Wikipedia?">technically possible</a> to fork Wikipedia, if you succeeded, then what? Who would come with you to build it, and who would that be useful for? This is partly a product of its totalizing effort to be an encyclopedia of everything (what good would <em>another</em> encyclopedia of everything be?) but also the weight of history: you won’t get enough long-encultured Wikipedians to join you, and you probably won’t be able to recruit a new generation of them on your own.</p>

<p>The last major effort to fork Wikipedia was in 2002 with an effort led by Edgar Enyedy to move the Spanish Wikipedia to The Enciclopedia Libre Universal en Español <a class="citation" href="#tkaczSpanishForkWikipedia2011">[260, 261]</a>. Though it was brief and unsuccessful, Enyedy claims that because Jimmy Wales was worried about other non-English communities following their lead, he and the other admins capitulated to the demands for no advertising and a transfer to a .org domain, among others<sup id="fnref:jimmydisagrees" role="doc-noteref"><a href="#fn:jimmydisagrees" class="footnote" rel="footnote">71</a></sup>. Even a politically symbolic fork is dependent on the perceived threat to the original project, and that window seems to have been closed after 2002.</p>

<p>The cultural tensions and difficulties that lead other wikis and projects to fork have taken their toll on the editorship and culture of Wikipedia. The community is drawn into <a href="https://meta.wikimedia.org/wiki/Conflicting_Wikipedia_philosophies">dozens</a> of conflicting philosophical camps: the <a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/Deletionism">Deletionists</a><sup id="fnref:AWWDMBJAWGCAWAIFDSPBATDMTAD" role="doc-noteref"><a href="#fn:AWWDMBJAWGCAWAIFDSPBATDMTAD" class="footnote" rel="footnote">72</a></sup> vs. the <a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/Inclusionism">Inclusionists</a>, <a href="https://meta.wikimedia.org/wiki/Eventualism">Eventualists</a> vs. <a href="https://meta.wikimedia.org/wiki/Immediatism">Immediatists</a>, <a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/Mergism">Mergists</a> vs. <a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/Separatism">Separatists</a>, and yes even a stub page for <a href="https://meta.wikimedia.org/wiki/Wikisecessionism">Wikisecessionism</a>. Editorship has steadily declined from a peak in 2007. Its relatively invisible community systems make it mostly a matter of chance or ideology that new contributors are attracted in the first place. In its calcification of norms, largely to protect against legitimate challenges to the integrity of the encyclopedia, any newcomers that do find their way into editing now have little chance to catch a foothold in the culture before they are frustrated by (sometimes algorithmic) rejection <a class="citation" href="#hillWikipediaEndOpen2019">[248, 250]</a>.</p>

<p>Arguably all internet communities have some kind of <a href="http://meatballwiki.org/wiki/WikiLifeCycle">life cycle</a>, so the question becomes how to design systems that support healthy forking without replicating the current situation of fragmentation. Wikis, including <a href="http://www.meatballwiki.org/wiki/TransClusion">Meatball</a> and <a href="https://www.mediawiki.org/wiki/Extension:Interwiki">MediaWiki</a>, as well as other projects like <a href="https://en.wikipedia.org/wiki/Project_Xanadu">Xanadu</a> often turn to <strong>transclusion</strong> — or being able to reference and include the content of one wiki (or wiki page) in another. Rather than copying and pasting, the remote content is kept updated with any changes made to it.</p>

<p>Transclusion naturally brings with it a set of additional challenges: Who can transclude my work? Whose work can I transclude? Can my edits be propagated back to their work? What can be transcluded, at what level of granularity, and how? While before we had characterized splitting communities as an intrinsic part of a fork, that need not be the case in a system built for transclusion. Instead relationships post-fork are then made an <em>explicit social process</em> within the system, where even if a community wants to work as separate subgroups, it is possible for them to arrive at some agreement over what they want to share and what they want to keep separate. This kind of decentralized work system resembles radical organizing tactics like affinity groups where many autonomous groups fluidly work together or separately on an array of shared projects without aspiring to create “one big movement” <a class="citation" href="#kleinWereDCSeattle2001">[262]</a>. Murray Bookchin describes:</p>

<blockquote>
  <p>The groups proliferate on a molecular level and they have their own “Brownian movement.” Whether they link together or separate is determined by living situations, not by bureaucratic fiat from a distant center. […]</p>

  <p>[N]othing prevents affinity groups from working together closely on any scale required by a living situation. They can easily federate by means of local, regional or national assemblies to formulate common policies and they can create temporary action committees (like those of the French students and workers in 1968) to coordinate specific tasks. […] As a result of their autonomy and localism, the groups can retain a sensitive appreciation of new possibilities. Intensely experimental and variegated in lifestyles, they act as a stimulus on each other as well as on the popular movement. <a class="citation" href="#bookchinNoteAffinityGroups1969">[263]</a></p>
</blockquote>

<p>To cherrypick a few lessons from more than 25 years of thought from tens of thousands of people: The differing models of document vs. thread modes and separate article vs. talk pages show us that using <strong>namespaces</strong> is an effective way to bridge multimodal expression on the same topic across <a href="https://communitywiki.org/wiki/TimeInWikis">perceived timescales</a> or other conflicting communicative needs. This is especially true when the namespaces have <strong>functional differentiation</strong><sup id="fnref:timbldifferentiation" role="doc-noteref"><a href="#fn:timbldifferentiation" class="footnote" rel="footnote">73</a></sup> like the tools for threading conversations on Wikipedia Talk pages and the parsing and code generation tools of Templates. These namespaces need to be <strong>visibly crosslinked</strong> both to preserve the social character of knowledge work, but also to provide a means of credit assignment and tool development between namespaces. Any communication system needs to be designed to <strong>prioritize ease of leaving</strong> and <strong>ease of forking</strong> such that a person can take their work and represent it on some new system or start a new group to encourage experimentation in governance models and technologies. One way of accomplishing these goals might be to build a system around <strong>social transclusion</strong> such that work across many systems and domains can be linked into a larger body of work without needing to create a system that becomes too large to fork. The need for communication across namespaces and systems, coupled with transclusion further implies the need for <strong>bidirectional transclusion</strong> so that in addition to being able to transclude something in a document, there is visible representation on the original work being transcluded (eg. commented on, used in an analysis, etc.) by allowed peers and federations.</p>

<p>These lessons, coupled with those from private bittorrent trackers, linked data communities, and the p2p federated system we have sketched so far give us some guidelines and motivating examples to build a varied space of communication tools to communicate our work, govern the system, and grow a shared, cumulative body of knowledge.</p>

<h3 id="rebuilding-scientific-communication">Rebuilding Scientific Communication</h3>

<div class="draft-text">
  <ul>
    <li><a class="citation" href="#berners-leeSociallyAwareCloud2009">[23]</a> - separate app from storage, or much like our tracker/protocol model from bittorrent, the same data can have many potential interfaces that interpret and use different parts of its graph.</li>
  </ul>

  <p>todo:</p>

  <ul>
    <li>mention the idea behind the threadodobot app, maybe in additional ‘adversarial interoperability’ section? put in context with the agora</li>
    <li>bump up sections of rebuilding communication by 1 heading level.</li>
    <li>Also need to talk about the client</li>
    <li>The only thing special about a feed is that it is linked to a preceding post.</li>
  </ul>

</div>

<p>It’s time to start thinking about interfaces. We have sketched our system in turtle-like pseudocode, but directly interacting with our linking syntax would be labor intensive and technically challenging. Instead we can start thinking about tools for interacting with it in an abstract way. Beneath every good interface we’re familiar with, a data model lies in wait. A .docx file is just a zipped archive full of xml, so a blank word document that contains the single word “melon” is actually represented (after some preamble) like:</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;w:body&gt;</span>
  <span class="nt">&lt;w:p</span> 
    <span class="na">w14:paraId=</span><span class="s">"0667868A"</span> 
    <span class="na">w14:textId=</span><span class="s">"50600F77"</span> 
    <span class="na">w:rsidR=</span><span class="s">"002B7ADC"</span> 
    <span class="na">w:rsidRDefault=</span><span class="s">"00A776E4"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;w:r&gt;</span>
        <span class="nt">&lt;w:t&gt;</span>melon<span class="nt">&lt;/w:t&gt;</span>
    <span class="nt">&lt;/w:r&gt;</span>
  <span class="nt">&lt;/w:p&gt;</span>  
<span class="nt">&lt;/w:body&gt;</span>
</code></pre></div></div>

<p>Same thing with jupyter notebooks, where a block of code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">rating</span> <span class="o">=</span> <span class="mi">100</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'I rate this dream </span><span class="si">{</span><span class="n">rating</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="s">'I rate this dream 100'</span>
</code></pre></div></div>

<p>is represented as JSON (simplified for brevity):</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"cell_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"code"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"thousand-vermont"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"outputs"</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span><span class="w">
    </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"stdout"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"output_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"stream"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"I rate this dream 100</span><span class="se">\n</span><span class="s2">"</span><span class="w">
    </span><span class="p">]</span><span class="w">
  </span><span class="p">}],</span><span class="w">
  </span><span class="nl">"source"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"rating = 100</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"print(f'I rate this dream {rating}')"</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>So we are already used to working with interfaces to data models, we just need to think about what kind of interfaces we need for a scientific communication system.</p>

<p>Let’s pick up where we left off with our linked data and tools. Recall that we had a <code class="language-plaintext highlighter-rouge">project</code> named <code class="language-plaintext highlighter-rouge">#my-project</code> that linked an <a href="#myproject-experiment">experiment</a>, a few datasets that it produced, and an <a href="#myproject-analysis">analysis pipeline</a> that we ran on it. We <em>could</em> just ship the raw numbers from the analysis, wash our hands of it, and walk straight into the ocean without looking back, but usually scientists like to take a few additional steps to visualize the data and write about what it means.</p>

<p>To explore the communicative tools that might be useful, we can start by considering traditional documents, and attempt to generalize them by separating their form as “units” or “cells” of information with accompanying metadata from their representation in interfaces for interacting and communicating about them.</p>

<h4 id="documents--notebooks">Documents &amp; Notebooks</h4>

<p>Say we have reached the stage where we are writing a brief summary of our experiment and analysis, but not yet at the stage of writing a “formal” scientific paper. We might do so in a notebook-like <a class="citation" href="#kluyverJupyterNotebooksPublishing2016">[264]</a> environment with different kinds of “cells,” specifically cells that execute <em>code</em> and cells that render <em>markdown.</em> We want to plot some of the results of our analysis, so to do that we might load the data and use <a href="https://matplotlib.org/">matplotlib</a> <a class="citation" href="#hunterMatplotlib2DGraphics2007">[265]</a> to make our point:</p>

<div class="jupyter-notebook">
  <div class="jupyter-notebook-iframe-container" style="padding-bottom: 18px;">
    <iframe src="/infrastructure/assets/notebooks/smile.html" style="position: absolute; top: 0; left: 0; border-style: none;" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'" width="100%" height="100%"></iframe>
  </div>
</div>

<p>Our notebook file would then include an array of JSON objects that describe the contents of its cells. For example, our data loading cell would look something like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
   </span><span class="nl">"cell_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"code"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"execution_count"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
   </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rapid-information"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"scrolled"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
   </span><span class="p">},</span><span class="w">
   </span><span class="nl">"outputs"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"..."</span><span class="w">
   </span><span class="p">],</span><span class="w">
   </span><span class="nl">"source"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"x, y, sizes = get_data('@jonny:my-project:Analysis1')"</span><span class="w">
   </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">"outputs"</code> description has been abbreviated above, but it describes to the jupyter notebook server <a href="https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Low%20Level.html">how to display it</a>. Regular text piped through <a href="https://en.wikipedia.org/wiki/Standard_streams">stdout</a> is represented like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"stdout"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"output_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"stream"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"Downloading dataset @jonny:my-dataset</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"--------------------</span><span class="se">\n</span><span class="s2">"</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>And multiple output types can be combined in a single cell, for example a widget like our loading progress bar is described like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"data"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"application/vnd.jupyter.widget-view+json"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"model_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"5799ac2959084a4596ffbad3f9940f48"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"version_major"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
      </span><span class="nl">"version_minor"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"text/plain"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"  0%|          | 0/100 [00:00&lt;?, ?it/s]"</span><span class="w">
    </span><span class="p">]</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span><span class="w">
  </span><span class="nl">"output_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"display_data"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>where the <code class="language-plaintext highlighter-rouge">model_id</code>, <code class="language-plaintext highlighter-rouge">version_major</code>, and <code class="language-plaintext highlighter-rouge">version_minor</code> describe which rendering code to use for the cell, similarly to the “metadata that indicates code” that we discussed in <a href="#analytical-frameworks">analytical frameworks</a>.</p>

<p>Notice that there is already a metadata field! In order to link our notebook to our analysis — and thus to our extended graph of data, experiment, etc. — we could do it <a href="https://jupyterbook.org/en/stable/content/metadata.html">manually</a>, but since we’re thinking about interfaces we can also imagine that our <code class="language-plaintext highlighter-rouge">p2p_framework</code> is capable of filling it in for us. We don’t need to invent a new metadata protocol for JSON, <a href="https://json-ld.org/">JSON-LD</a> is already quite similar to the syntax we’ve been using already. For simplicity, say we use a <code class="language-plaintext highlighter-rouge">@comms</code> ontology to denote various features of our communication system. Our data loading function might then populate a field in our cell like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nl">"scrolled"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
  </span><span class="nl">"@comms:usesData"</span><span class="p">:</span><span class="w"> </span><span class="s2">"@jonny:my-project:Analysis1"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Other frameworks might make their own metadata annotations, like an indication that we’re plotting some feature of the data, or performing some statistical analysis on the data. These annotations might be responsive to the parameterization of the function call or its results, but if we emphasize a design process that makes interfaces at multiple levels we could also imagine using something like iPython “<a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">magic commands</a>” to declare metadata for our cell. For example, each cell is automatically assigned a random combination of words as an ID, but if we wanted to be able to specifically refer to a cell we could give it an explicit one:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%%meta @comms:cellID smilePlot
plt.scatter(x, y, s=sizes)
</code></pre></div></div>

<p>We’re familiar with two types of cells, code and markdown, but we can extend our thinking to arbitrary cell types. What is a cell? A cell has a a) <strong>type</strong> that indicates its capabilities and representation, b) <strong>metadata</strong> that describes it, we can also generalize that to include <em>arguments</em> that parameterize it, and c) the content, or information contained by the cell. The jupyter <a href="https://jupyterlab.readthedocs.io/en/stable/api/classes/cells.cellmodel-1.html">document model</a> more or less reflects this already, but in its base model only has <a href="https://jupyterlab.readthedocs.io/en/stable/api/classes/cells.codecellmodel-1.html">code</a>, <a href="https://jupyterlab.readthedocs.io/en/stable/api/classes/cells.markdowncellmodel.html">markdown</a>, and <a href="https://jupyterlab.readthedocs.io/en/stable/api/classes/cells.rawcellmodel.html">raw</a> cell types, and the metadata field is unstructured. Its extension system allows for additional cell types as well as restructuring the program more generally, but since we’re focused on self-contained documents we’ll limit our discussion to additional cell types.</p>

<p>From this it’s relatively trivial to imagine additional cell types that serve common needs in academic writing: a citation cell type<sup id="fnref:notebookcites" role="doc-noteref"><a href="#fn:notebookcites" class="footnote" rel="footnote">74</a></sup> that takes a <a href="http://www.bibtex.org/Format/">BibTeX</a> object (or its fields) as arguments and then preserves the full metadata as well as renders it in a chosen style. A figure cell type that takes an image or plot and a caption. A contributor cell type that takes an author’s name, affiliation, ORCID, email, and so on. Currently jupyter extensions use the NPM registry, but we could imagine being able to use other people’s cell types directly by referring to them like <code class="language-plaintext highlighter-rouge">@jonny:celltypes:citation</code>.</p>

<p>Notebooks have multiple levels of metadata, so we can also specify document-level metadata that describe the type of our document (like a <a href="https://schema.org/ScholarlyArticle"><code class="language-plaintext highlighter-rouge">@schema:ScholarlyArticle</code></a>), its <a href="https://schema.org/creativeWorkStatus"><code class="language-plaintext highlighter-rouge">creativeWorkStatus</code></a> as a <code class="language-plaintext highlighter-rouge">Draft</code>, our authorship information, permissions, and whatever else we’d like. But what is a document? In the case of our jupyter notebook, it’s a series of cell descriptions in a JSON array. Trivially, a document is a cell that contains other cells. What about in the other direction? The contents of our cells are <em>also</em> a cell-like system. The very notion of a programming language is a means of mapping structured syntax to machine instructions, and to do that code (in some languages) is interpreted or compiled by parsing it into an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract syntax tree</a> that relates data and its structuring metadata. Markdown can also be thought of as a series of subcells, where using a <code class="language-plaintext highlighter-rouge"># header</code> indicates how the text is to be represented as compared to <code class="language-plaintext highlighter-rouge">*italic*</code> text or <code class="language-plaintext highlighter-rouge">[links](https://link.com)</code>. The use of a programming language or markup syntax is represented by the <code class="language-plaintext highlighter-rouge">cell_type</code> field, which the notebook server knows to translate <code class="language-plaintext highlighter-rouge">"code"</code> to mean Python and <code class="language-plaintext highlighter-rouge">"markdown"</code> to mean its particular flavor of markdown (of which there are <a href="https://www.iana.org/assignments/markdown-variants/markdown-variants.xhtml">several</a>).</p>

<p>This points towards a model of <strong>recursive</strong> cells that can contain other cells. An editor could, for example, draw from templating engines like <a href="https://shopify.github.io/liquid/">liquid</a>, where an abstract representation of the content of a cell could include a `&lt;h1 id="im-not-sure"&gt;I’m not sure&lt;/h1&gt;</p>
<h2 id="where-you-thought">Where You Thought</h2>
<h3 id="you-were-going">You Were Going</h3>

<p><em>(you were already <a href="/infrastructure/#gobackhere">there</a>)</em></p>
<p>` marker that indicates that additional cells can be included inside of it. Recursive models, coupled with structuring metadata that indicates the relationship between a parent and child cell could then be used to model compound concepts. Another simple example using citation might be to have a cell with one child cell containing a reference to another work that ours <a href="https://sparontologies.github.io/cito/current/cito.html#d4e449"><code class="language-plaintext highlighter-rouge">@cito:disagrees_with</code></a> <a class="citation" href="#peroniFaBiOCiTOOntologies2012">[266]</a>, and another child cell that in turn contains some writing in markdown and a plot. Recursive cells also naturally lend themselves to <strong>transclusion</strong> by making each of the individual subcomponents of a document referenceable with full granularity. We will expand on both compount concepts and transclusion in a moment in talking about the extension of our cellular system to <a href="#trackers-clients--wikis">wikis</a>.</p>

<p>Before we go beyond a document system that would be unrecognizable to most scientists, and thus yet another nice pipedream, it’s important to pause on the continuity with existing document systems. Microsoft Word, or Word-like WYSIWYG editors like <a href="https://www.libreoffice.org/">LibreOffice</a> or Google docs are the dominant mode of preparing academic documents. Word-like editors are <em>already</em> create recursive cell-like documents, though their interface obscures them. They support semantic markup like heading styles (though their use compared to manual formatting is far from universal <a class="citation" href="#sorgaardUseParagraphStyles1996">[267]</a>), and every paragraph can be considered a cell, with the default paragraph styling as its metadata and additional styled elements like bolded words as sub-cells. It should then be possible to import existing word documents into a cellular document system. Care should also be taken to smooth the cognitive transition from word-like editors: Jupyter currently treats cells as being strictly separate, and new cells need to be created manually. Instead it should be possible for cells to “recede into the background” and be created with common gestures like a double return to make a new paragraph. The “insert” menu used to create things like tables or images is already a familiar tool in word-like editors, so the notion of adding elaborated types like citations shouldn’t be that big of a lift.</p>

<p>The other major document preparation tool modalities are markup syntaxes and their associated builders like LaTeX. Though TeX-like tools have an exceedingly opinionated and obscure design history <a class="citation" href="#knuthTeXbook1986">[268]</a>, they have three major affordances: 1) document-level structure provided by document classes, packages, and the options they provide, 2) environments that enclose some section of text between <code class="language-plaintext highlighter-rouge">\begin{}</code> and <code class="language-plaintext highlighter-rouge">\end{}</code> and provide some specific functionality or formatting like <a href="https://www.overleaf.com/learn/latex/Lists">lists</a>, and 3) commands that accept arguments and modify some smaller unit of text like creating a link with <code class="language-plaintext highlighter-rouge">\href{https://url.com}{link text}</code>. Each of these maps onto a cellular document system, with document-level metadata or the templates commonly used to render markdown, and cells that take arguments to approximate environments and commands. Markdown extensions like <a href="https://myst-parser.readthedocs.io/en/latest/">MyST</a> (missing reference) make this translation even more straightforward with direct analogies to LaTeX commands and environments and their “role” and “directive” counterparts in <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html">reStructuredText</a>. Since the goal should be a 1:1 relationship between source code and visual editor, the difference between representing a cell visually versus in markup should be left as a matter of author preference.</p>

<p>Bidirectional translation from a WYSIWYG editor to its markup is not a trivial task — the mediawiki team started writing theirs in <a href="https://www.mediawiki.org/wiki/VisualEditor">2011</a> and rolled it out as a default feature in <a href="https://en.wikipedia.org/wiki/MediaWiki_version_history">2020</a> <a class="citation" href="#forresterInventingWeGo2012">[269]</a>. It’s a careful balance between ease of use, power of syntax, and accomodation of historical usage patterns. Markdown is on one extreme of ease with only a handful of markup elements to learn, but has a relatively steep learning curve to do anything more complex. On the other end is the wonderful <a href="https://dokie.li/">dokieli</a> <a class="citation" href="#capadisliDecentralisedAuthoringAnnotations2017">[270]</a> (and see Sarven’s <a href="https://csarven.ca/linked-research-decentralised-web">masterpiece</a> <a class="citation" href="#capadisliLinkedResearchDecentralised2019">[271]</a>, spiritual cousin to this document), which does essentially everything that we want our linked documents to do, but requires authors to write their documents in HTML and manually manage the semantic markup. Extending Notebooks to use recursive cells with reusable types sacrifices some of the ability to directly edit the source of a document as a potential way to balance familiarity and expressiveness.</p>

<p>Notebooks, with some architectural and interfaces then become a straightforward way of breaking up the scientific paper as a singular unit of knowldge work when embedded in a linked data system. Their use in scholarly publishing has been proposed many times before, but our linking system lets us resolve some of the largest outstanding limitations <a class="citation" href="#chattopadhyayWhatWrongComputational2020">[272]</a>: dependency management <a class="citation" href="#ruleTenSimpleRules2019">[273]</a>, archiving <a class="citation" href="#woffordJupyterNotebooksDiscovery2020">[274]</a>, and discovery, among others. The same gradient of access control rules we discussed in controlling access to sensitive data would support a process of gradual publication of smaller units of work, from a private demo in our lab meeting to a public part of scientific discourse.</p>

<p>What happens when we invite other people to respond?</p>

<h4 id="forums--feeds">Forums &amp; Feeds</h4>

<p>What if we think of our documents as “threads” and their cells as “posts?” What makes a cellular document a document is some (relatively arbitrary) notion of a ‘root’ cell that contains the others — ie. for notebooks a JSON array of cells. That could be trivially reformulated as cells with metadata indicating that they are <a href="https://schema.org/isPartOf"><code class="language-plaintext highlighter-rouge">PartOf</code></a> a document, each indicating their <a href="https://schema.org/position"><code class="language-plaintext highlighter-rouge">position</code></a> or linked to the cells they are before and after. If we also allow cells to be <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-inreplyto"><code class="language-plaintext highlighter-rouge">inReplyTo</code></a> each other, we have the basis of a threaded communication system continuous with documents. Where cells in a linear document have at most one preceeding and succeeding cell, multiple replies allow a tree structure that maps onto the patterns of most contemporary social media. Metadata that describes category and content extends this to include the structure of forums, and could be the basis of a rich continuum of media spanning order and chaos, permanence and ephemerality, between the <em>magnum opus</em> and the shitpost: media absent but sorely needed in academic communication.</p>

<p>Traditional forums like <a href="https://www.phpbb.com/">phpBB</a> and contemporary social media operate from a single host with a fixed interface and representation of posts. What would a communication system that decouples hosting, identity, interface, and format look like? We can draw inspiration from the “<a href="https://en.wikipedia.org/wiki/Fediverse">fediverse</a>,” a collection of interoperable software platforms and protocols. The fediverse makes it possible to communicate across radically different interfaces: someone using <a href="https://funkwhale.audio/">Funkwhale</a>, which resembles music software like spotify, can communicate with people on <a href="https://joinpeertube.org/">PeerTube</a>, a p2p video streaming program like YouTube, and <a href="https://joinmastodon.org/">Mastodon</a>, a microblogging medium like Twitter. Rather than a single host, instances of each of these programs are hosted independently and can choose to federate with other instances to enable communication between them. Most of these programs use the <a href="https://www.w3.org/TR/2018/REC-activitypub-20180123/">ActivityPub</a> <a class="citation" href="#Webber:18:A">[188]</a> protocol, which defines a standard set of capabilities for client-server and server-server communication.</p>

<p>Mastodon posts (or “toots”) already resemble the kind of document-interoperable medium hinted at above. For example <a href="https://web.archive.org/web/20220708215201/https://social.coop/@jonny/107328829457619549">this post</a> is represented in (abbreviated) JSON:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"107328829457619549"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"created_at"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2021-11-23T22:52:49.044Z"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"in_reply_to_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"107328825611826508"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"in_reply_to_account_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"274647"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"visibility"</span><span class="p">:</span><span class="w"> </span><span class="s2">"public"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://social.coop/@jonny/107328829457619549"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;p&gt;and making a reply to the post to show the in_reply_to and context fields&lt;/p&gt;"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"account"</span><span class="p">:</span><span class="w">
    </span><span class="p">{</span><span class="w">
        </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"274647"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"username"</span><span class="p">:</span><span class="w"> </span><span class="s2">"jonny"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"fields"</span><span class="p">:</span><span class="w">
        </span><span class="p">[</span><span class="w"> </span><span class="err">...</span><span class="w"> </span><span class="p">]</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"media_attachments"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
    </span><span class="nl">"mentions"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
    </span><span class="nl">"tags"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>As described <a href="#for-making-our-peers-and-the-links-within-their-namespace-discov">previously</a>, ActivityPub supports linked data with JSON-LD – a remarkable feat despite the justifiable angst with the protocol <a class="citation" href="#kaniiniActivityPubPresentState2019">[275, 71]</a> given the historical grudges between linked data and indieweb communities (See this retrospective by one of its authors, Christine Lemmer-Webber <a class="citation" href="#lemmer-webberStandardsDivisionsCollaboration2018">[66]</a>). So we could imagine that post using a reference to a document or one of its cells in its <code class="language-plaintext highlighter-rouge">in_reply_to</code> field.</p>

<p>Mastodon might be a good transitional medium, but we can extend it to make use of our linked p2p system. The fediverse decouples the network from a single platform, but instances still bundle together the underlying data of a post with an interface, host, and account (but see <a href="https://hubzilla.org//page/hubzilla/hubzilla-project">hubzilla</a>). p2p helps us decouple accounts from hosts (see this discussion on a p2p ActivityPub <a class="citation" href="#webberActivityPubDecentralizedDistributed2017">[276]</a>), but we would also like to decouple interfaces from the underlying data so that we have a continuous communication medium where different interfaces are just <em>views</em> on the data. To do that we would want to start by replacing Mastodon’s flat “<code class="language-plaintext highlighter-rouge">content</code>” field with the kind of typed cells in our documents that indicate what kind of message they are. For example a simple text-based message might use the ActivityStreams <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-note"><code class="language-plaintext highlighter-rouge">Note</code></a> type:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"@context"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://www.w3.org/ns/activitystreams"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Note"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"My Message"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"A note I send to you!"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>But we might equivalently send a <code class="language-plaintext highlighter-rouge">@jupyter:Notebook</code> as a message, or some compound object like a <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-collection"><code class="language-plaintext highlighter-rouge">Collection</code></a>:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"@context"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://www.w3.org/ns/activitystreams"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"summary"</span><span class="p">:</span><span class="w"> </span><span class="s2">"A Compound Message!"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Collection"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"totalItems"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
  </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Note"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Hey how ya doin here's a notebook"</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"@context"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://jupyter.com/"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Notebook"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"content"</span><span class="p">:</span><span class="w"> </span><span class="s2">"..."</span><span class="w">
    </span><span class="p">},</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>So the <em>existence</em> of a particular type of message is not bound to the ability of any given program’s ability to render it. Our notebook program might not be able to understand what it means to have people responding to and making threads about its cells, but we would still be able to receive them and open them with an interface that does, and we could further imagine the ability for a type to recommend a program to us for rendering it as we did with the ability for analysis nodes to specify the code to execute them. We will set aside for a moment the issues of moderation and permission for which messages can link to our work and the practicalities of sending, receiving, storing, and serving messages and return to them in the context of <a href="#overlays--adversarial-interoperability">annotations</a> and <a href="#trackers-clients--wikis">trackers</a>, respectively.</p>

<p><em>Where</em> do our posts go? For concreteness, we can start with a forum called “NeuroChat.” <code class="language-plaintext highlighter-rouge">@neurochat</code> is a peer like any other, and it supports some of the basic ActivityStreams vocabulary. We can request to join it by sending a <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-join"><code class="language-plaintext highlighter-rouge">@as:Join</code></a> request, which gives it permission to index our public posts and issue links on our behalf through its web interface. It has a few broad categories like “Neuromodulation” and “Sensory Neuroscience,” within which are collections of threads full of chronologically-sorted posts. Threads are objects that indicates a category like <code class="language-plaintext highlighter-rouge">@neurochat:categories:Neuromod</code>, and when we post in them we create links that are <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-attributedto"><code class="language-plaintext highlighter-rouge">@as:attributedTo</code></a> us with the <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-context"><code class="language-plaintext highlighter-rouge">@as:context</code></a> of the thread we’re posting in and any <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-inreplyto"><code class="language-plaintext highlighter-rouge">@as:inReplyTo</code></a> links to preceding or quoted posts.</p>

<p>We want to announce and describe some recent results in our document <code class="language-plaintext highlighter-rouge">@jonny:my-project:Writeup</code>. This kind of post is common in <code class="language-plaintext highlighter-rouge">@neurochat</code>, and so instead of a generic citation we use a <code class="language-plaintext highlighter-rouge">@neurochat:AnnouncesResult</code> link to indicate the relevant document. In our forum pseudocode we’ll use a <code class="language-plaintext highlighter-rouge">#prefix</code> macro to give a short name to our project and semantic wikilinks with a <code class="language-plaintext highlighter-rouge">[[predicate::object]]</code> syntax for the purpose of demonstration, though ideally these would be part of the forum’s interface.  We think we really have something that challenges some widely held previous results:</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#prefix project @jonny:my-project </span><span class="w">
</span><span class="c1">#prefix nc @neurochat</span><span class="w">

</span><span class="n">Hi</span><span class="w"> </span><span class="n">everyone,</span><span class="w"> </span><span class="n">happy</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">work
</span><span class="p">[[</span><span class="nn">nc:</span><span class="n">AnnouncesResult</span><span class="w"> </span><span class="nn">::</span><span class="w"> </span><span class="nn">project:</span><span class="n">Writeup</span><span class="p">]].</span><span class="w">

</span><span class="n">I</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">raises</span><span class="w"> </span><span class="k">a</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">interesting</span><span class="w"> </span><span class="n">questions,
in</span><span class="w"> </span><span class="n">particular</span><span class="w"> </span><span class="na">@rival's</span><span class="w"> </span><span class="n">longstanding</span><span class="w"> </span><span class="n">argument
</span><span class="p">[[</span><span class="na">@cito:disputes</span><span class="w"> </span><span class="nn">::</span><span class="w"> </span><span class="na">@rival:TheBrainIsInTheLiver]]</span><span class="p">.</span><span class="w">

</span><span class="n">I</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">wonder</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">means</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">conversation
we've</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">generally</span><span class="w"> </span><span class="n">about
</span><span class="p">[[</span><span class="na">@cito:discusses</span><span class="w"> </span><span class="nn">::</span><span class="w"> </span><span class="na">@discipline:whereAreTheOrgans]]</span><span class="p">.</span><span class="w">

</span><span class="n">Anyway,</span><span class="w"> </span><span class="n">write</span><span class="w"> </span><span class="n">back</span><span class="w"> </span><span class="n">soon,</span><span class="w"> </span><span class="n">xoxo</span><span class="p">.</span><span class="w">
</span></code></pre></div></div>

<p>Our rival takes the criticism in stride but wants to run their own analysis. They follow the links back to find our data, and reanalyze it. Their analysis framework has already issued a link indicating that it reanalyzes our data, and rather than do an independent writeup our rival returns to the thread to continue the discussion.</p>

<div class="language-turtle highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Interesting</span><span class="w"> </span><span class="n">result,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">old</span><span class="w"> </span><span class="n">scoundrel</span><span class="p">.</span><span class="w"> 

</span><span class="n">That</span><span class="w"> </span><span class="n">indeed</span><span class="w"> </span><span class="p">[[</span><span class="n">disputes</span><span class="w"> </span><span class="nn">::</span><span class="w"> </span><span class="na">@doi:&lt;id&gt;]]</span><span class="p">,</span><span class="w">
</span><span class="n">in</span><span class="w"> </span><span class="n">particular</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">section</span><span class="w"> </span><span class="p">[[.</span><span class="nn">:results:</span><span class="n">main</span><span class="p">]]</span><span class="w">
</span><span class="n">and</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">re-analysis</span><span class="w"> </span><span class="n">adds</span><span class="w"> </span><span class="n">another</span><span class="w"> </span><span class="n">wrinkle</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">problem!
Take</span><span class="w"> </span><span class="k">a</span><span class="w"> </span><span class="nn">look:</span><span class="w">

</span><span class="p">[[</span><span class="nn">nc:</span><span class="n">embed</span><span class="w"> </span><span class="nn">::</span><span class="w"> </span><span class="na">@rival:reanalysis]]</span><span class="w">

</span><span class="n">This</span><span class="w"> </span><span class="n">really</span><span class="w"> </span><span class="n">complicated</span><span class="w"> </span><span class="n">another</span><span class="w"> </span><span class="n">project</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">mine,
</span><span class="p">[[</span><span class="na">@rival:projects:NeuronsCanSwim]]</span><span class="w">
</span></code></pre></div></div>

<p>Our forum’s <code class="language-plaintext highlighter-rouge">embed</code> link knows how to embed the notebook our rival used to do their reanalysis and in the underlying message indicates the the current version so if they update it in the future the message will still be comprehensible. Our rival doesn’t use a predicate for their link to their side-project and our forum uses its default <code class="language-plaintext highlighter-rouge">Mentions</code> predicate. It’s still more informative than a duplet link because the context of being a discussion in our forum the links in the surrounding posts. We could imagine additional capabilities we give to our forum, like the ability to automatically trigger a re-analysis by someone mentioning a different pipeline for a given dataset, but we’ll leave those as an exercise to the reader.</p>

<p>This example is a relatively trivial instance of scientific communication: sharing results, relating them to previous findings, and thinking about the broader implications on the field. However in our current regime of scientific communication, even in the most progressive publication venues that allow communication directly on a work, this kind of communication is <em>entirely invisible</em> to the broader state of our understanding. With our system of linked communication, however, the entire provenance chain from our experiment through its analysis and contextualizing discussion is related to immediately related work as well as the standing questions in our field. Our work is enriched by the additional analysis from our rival, and their work is continuously contextualized as the state of our understanding develops. We were capable of making incremental refinements to our shared understanding using units of work that were much smaller than the traditional scientific paper. It would be possible for someone entirely outside our field to browse through the general links from basic research questions to relevant work and its surrounding discussion. If they were to ask questions, our answers would represent the latent diffusion of understanding to other disciplines based on the graph context of our respective work — and we could be credited the time we spent doing so! In short, scientific communication could actually be <em>cumulative.</em></p>

<p>Forums are just one point in a continuous space of threaded media. If we were to take forum threads out of their categories, pour them into our water supply, and drink whatever came our way like a dog drinking out of an algorithmic fire hydrant, we would have Twitter. Remove the algorithm and arrange them strictly chronologically and we have Mastodon. In both, the “category” that organizes threads is the author of the initial post. Algorithmic, rather than purposefully organized threaded systems have their own sort of tachycardic charm. They are effective at what they aim to do, presenting us whatever maximizes the amount of time we spend looking at them in a sort of hallucinatory timeless now of infinite disorganization — at the expense of desirable features of a communication system like a sense of stable, autonomously chosen community, perspective on broader conversation, and cumulative collective memory.</p>

<p>Nevertheless the emergence of a recognizable “Science Twitter” points towards a need for relatively informal all-to-all communication. Serendipitously being able to discover unlikely collaborators or ideas is a beautiful dream, if one ill-served by the for-profit attention economy. Our formulation of the <code class="language-plaintext highlighter-rouge">@neurochat</code> forum was as an equal peer that mirrored, collected, and organized posts that otherwise are issued from other peers such as ourselves. In the same way that we might use the ActivityStreams <code class="language-plaintext highlighter-rouge">Join</code> action to have our posts mirrored by it, we might also use <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-follow"><code class="language-plaintext highlighter-rouge">@as:Follow</code></a> to receive posts from any peer, and in the case of a federation that might include posts from its members sent to the federation.</p>

<p>We can take advantage of the graph structure and rich metadata of our social network in ways that are impossible in corporate social media networks that require the expectation of disorder to be able to sell “native” ad placement. The instance-to-instance federation model of the fediverse, and the accompanying absence of any “global” scope of all posts, results in the need for multiple views on the network: in Mastodon, a “local” timeline that shows only posts from within the host instance, and a “federated” timeline that shows posts from all instances that the host instance has federated with. Since our network allows peer-to-peer, federation-to-federation, and peer-to-federation interaction, we can extend that further. We can construct views of the network based on granular control over graph depth: instead of seeing just the posts from the peers that we follow, we can request to see n-depth posts, from the peers that our peers follow, and so on. This could be done at the level of a “view” or at the level of the follow link itself — since I know this person well, I want to see a graph depth of 2 from them, and a depth of 1 from others. At the federation level, we might imagine that <code class="language-plaintext highlighter-rouge">@neurochat</code> is federated with another <code class="language-plaintext highlighter-rouge">@linguisticsChat</code> group and the two mirror and rehost each other’s posts. We could then make use of our extended social graph and prioritize posts from people who are part of overlapping subsets of the federations we are a part of. The peer-based nature of our social network serves as the basis for a system of fluid scoping and filtering of the kind of communication we are looking for at any given time. So rather than a disorganized public melee or the empty rooms and new logins from yet another closed Slack, our communication could be part of a coherent scientific conversation.</p>

<p>Across from filtering what we receive, the same could be done to what we send by choosing where our posts are addressed and who can see them. The same multimodality of “following” used to indicate the graph depth of the posts we see could let us indicate different kinds of relationships. We should be able to send global, undirected messages on a public feed, but we don’t necessarily want to talk to our friends in the same way that we talk to strictly professional colleagues. We might want to organize privately with a few colleagues, or prevent trolls or hostile groups from accessing or making use of our work. Effectively, we should be able to direct our messages to different groups of peers to support the multiple registers of our communication.</p>

<p>The need for rapid and informal scientific communication being mediated by corporate social networks has the unfortunate byproduct of needing to carefully manage a “personal brand.” To be seen as a “serious,” we need to maintain some proximity to the stilted academic voice, forfeiting any approachability to science that might be gained from public communication. If we are to expand the scope of what we consider as the labor of scientific communication, we should also take seriously its many registers and contexts. Informal media like alt accounts, mailing lists, groupchats, zines, and whisper networks are also an integral part of science, particularly for marginalized and vulnerable scientists <a class="citation" href="#jimenezBorderlandingAcademicResearchers2020">[277]</a>. Parallel to organizing our communication in empirical professional communication, we might build systems that support our organization into federations to more effectively bargain over our working conditions and protect ourselves. The venues that organize our communication being limited to journals, and the accompanying regulation over the registers of communication that count as “real” science, is even more limiting than its profound effects on scientific literature proper. The absence of infrastructure to support the multiregister communication of science limits our ability to organize over the broader state of our work, form extended communities, and reduces what should be the collective project of making our work broadly understandable to the individualistic projects of “scicomm influencers.” It shouldn’t take a lot of additional critical analysis to say “shitposts are good, actually, for science.”</p>

<p>There’s a balance to be struck between a system of granular control over the messages we send and receive with the ease of a monolithic algorithmic feed. Mastodon sorts all posts purely chronologically, which translates into relatively steep limits on the size of communities as feeds become unintelligible washes of posts. Instead of forgoing algorithmic organization altogether, another means by which we could take advantage of the graph structure of our network is by being able to <em>choose</em> the sorting algorithms we use. We might want to prioritize posts from someone who we don’t necessarily follow but is interacting with people that we do in contexts that we share, or be able to deprioritize posts that are “close” to us in our social graph in order to discover new things. This too could be a cumulative, community-driven project, where we might want to try out our friend’s <code class="language-plaintext highlighter-rouge">@friends:sorting:NewAlgorithm</code>, tweak it a bit for our preferences, and republish a new version.</p>

<p>Generally, the impact of having a communication system that decouples hosting, identity, interface, and format on an underlying linked data grpah gives us a broad space to build different views and tools to use the underlying data. Specifically, without predicting the infinite future of communication media, our system of linked, cell-like communication generalizes threadlike media like forums and feeds into a continuous system that can blend their features as needed. Durable, cumulative discussion about the state of our understanding should be able to live side-by-side with ephemeral, informal conversations. It should be possible for us to serendipitously discover people and information as well as for a newcomer to have a place to ask questions and build their understanding. It should be possible for us to form and dissolve communities fluidly without substantial technical start-up costs and the total loss of memory when they close. A system that supports the fullness of continuous communication would be an unfathomably richer way of building reliable, accessible, and multivalent understanding of our reality than the current system of a gladitorial thumbs up/down indictment on years of your life that is journal-based peer review.</p>

<h4 id="overlays--adversarial-interoperability">Overlays &amp; Adversarial Interoperability</h4>

<p>We can’t expect the entire practice of academic publishing to transition to cell-based text editors in a p2p linked data swarm anytime soon. In the same way that we discussed frameworks for integrating heterogeneous analytical and experimental tools, we need some means of <strong>bridging</strong> communication tools and <strong>overlays</strong> for interacting with existing communication formats. There are many examples of bridging communication protocols, eg. the <a href="https://matrix.org/bridges/">many ways to use Matrix</a> with <a href="https://matrix.org/bridges/#slack">Slack</a>, <a href="https://matrix.org/bridges/#email">email</a>, <a href="https://matrix.org/bridges/#signal">Signal</a>, etc. The overlays for websites, pdfs, and other more static media that we’ll discuss are means to bring them into the system whether they support it or not: our interoperability should be willing to be adversarial if it needs to be <a class="citation" href="#doctorowAdversarialInteroperabilityReviving2019">[278, 279]</a>. In representing the intrinsically interactive and social nature of reading (eg. see <a class="citation" href="#jacksonMarginaliaReadersWriting2001">[280]</a>), overlays as interfaces also supplement the “horizontal” connections between cells by injecting information into them or transcluding it elsewhere: creating a fuzzy boundary between writing <em>on</em> something vs <em>about</em> something.</p>

<p>We don’t need to look far to find a well-trod interface for annotation overlays for document-like media: the humble highlighter. <a href="https://hypothes.is">Hypothes.is</a>, enabled on this page, lets readers highlight and annotate any webpage with a <a href="https://chrome.google.com/webstore/detail/hypothesis-web-pdf-annota/bjfhmglciegochdpefhhlphglcehbmek">browser extension</a> or javascript bookmarklet. This interface is a near match to the highlighting and review tools of Microsoft Word and Google Docs used for the same purpose. At its heart is a system for making anchors, references to specific places in a text, and the means of matching them even when the text changes or the reference is ambiguous <a class="citation" href="#csillagFuzzyAnchoring2013">[281]</a>. For example, <a href="https://hypothes.is/a/oLw4uk7_Eeyt5N-FVlE3fw">this anchor</a> has three features, a <code class="language-plaintext highlighter-rouge">RangeSelector</code> that anchors it given the position within the paragraph, an absolute <code class="language-plaintext highlighter-rouge">TextPositionSelector</code>, and a contextual <code class="language-plaintext highlighter-rouge">TextQuoteSelector</code> that you can see with an <a href="https://api.hypothes.is/api/annotations/oLw4uk7_Eeyt5N-FVlE3fw">API call</a>. Anchors like these, along with references to the <a href="https://github.com/hypothesis/client/blob/fb08cdf38191643d7a35d84ca3b822589c2e880a/src/annotator/anchoring/types.js">code that resolves them</a>, could be the objects to which we could link from the rest of our communication system.</p>

<p>On its own, it serves to give a <code class="language-plaintext highlighter-rouge">Talk:</code> page to every website. With an integration into a system of linked data and identity, it also serves as a means of extending the notion of bidirectional transclusion described above to work that is not explicitly formatted for it. Most scientific work is represented as <code class="language-plaintext highlighter-rouge">.pdf</code>s rather than <code class="language-plaintext highlighter-rouge">.html</code> pages, and hypothes.is <a href="https://web.hypothes.is/help/annotating-locally-saved-pdfs/">already supports</a> annotating PDFs. With an integration into pdf reading software, for example <a href="https://www.zotero.org/support/pdf_reader_preview">Zotero’s PDF reader</a>, there would be a relatively low barrier to integrating collaborative annotation into existing workflows and practices.</p>

<p>Digital publishing makes imagining the social regulation of science as a much more broadly based and continuous process much easier, but the problem of moderation remains (as it has since at least the coiner of the terms “Gold” and “Green” open access lost faith in ahierarchical scientific communication after someone said poo-poo words at him on Internet while defending the use of they/them as gender-ambiguous pronouns <a class="citation" href="#harnadSkyWriting1987">[282, 283, 284]</a>). Some movement has been made towards public peer review: eLife has integrated hypothes.is since 2016 <a class="citation" href="#ELifePartnersHypothes2016">[285]</a>, and bioRxiv had decided to integrate it as well in 2017 <a class="citation" href="#dwhlyBioRxivSelectsHypothesis2017">[286]</a> before getting cold feet about the genuinely hard problem of moderation (among others <a class="citation" href="#heatherstainesPreprintServicesGather2018">[287]</a>) and instead adopting the more publisher-friendly TRiP system of refereed peer-reviews <a class="citation" href="#nateangellAnnouncingTRiPTransparent2019">[288]</a>.</p>

<p>Overlays raise basic questions about control over the representation of our work, about who is able to write what on it. As with potential incompatibility between interfaces, we should be able to control what comments appear <em>on</em> our work, but there is no way to control – even in our current communication systems – what someone says <em>about</em> it. Our system gives us some ability to identify bad actors and regulate the avenues of communication without overcorrecting into a system where criticism becomes impossible – even if we don’t want to represent someone’s comments on our work, it is possible to make them and for others to find them, but it’s also possible to contextualize their context if they’re made in bad faith.</p>

<p>Though a description of the norms and tools needed to maintain healthy public annotation is impossible here, our system <em>provides a space for having that conversation.</em> Authors could, for example, allow the display of annotations from a professional society like <code class="language-plaintext highlighter-rouge">@sfn</code> that has a code of conduct and moderation team, or annotations associated with comments on <code class="language-plaintext highlighter-rouge">@pubpeer</code>, or from a looser organization of colleagues and other <code class="language-plaintext highlighter-rouge">@neurofriends</code>. Conversely, being able to make annotations and comments from different federations gives us a rough proxy to different registers of communication and preserves the plurality of our expression. Social tools like these are in the hypothes.is team’s <a href="https://web.archive.org/web/20211015213849/https://github.com/hypothesis/product-backlog/projects/6">development roadmap</a>, but I intend it as a well-developed and mature example of a general type of technology<sup id="fnref:genius" role="doc-noteref"><a href="#fn:genius" class="footnote" rel="footnote">75</a></sup> rather than a recommendation.</p>

<p>In addition to annotating other works, overlays can come in the form of bots or other tools for interacting with existing systems in a way that’s compatible with a new one. One particularly impressive example of aggressive interoperability in this domain is Eduardo <a href="https://flancia.org/">“flancian”</a> Ivanec’s <a href="https://anagora.org/">agora</a> <a class="citation" href="#ivanecFutureNoteTaking2021">[289, 290]</a>. An agora is a <a href="https://anagora.org/wiki-like">wiki-like</a> project with pages (or nodes) for each named concept, but it also allows for multiple representations of a given node: so notes from multiple people across multiple mediums will be present on the same page. Accompanying the agora is the anagora bot (on <a href="https://botsin.space/@agora">Mastodon</a> and <a href="https://twitter.com/an_agora">Twitter</a>), which makes links to, and backlinks from pages mentioned as <code class="language-plaintext highlighter-rouge">[[wikilinks]]</code> by accounts that follow them (for example: a <a href="https://social.coop/@jonny/108621001205679783">post</a>, the bot’s <a href="https://botsin.space/@agora/108621001318792297">reply</a>, and one of the linked pages, <a href="https://anagora.org/Wikilinks+Everywhere"><code class="language-plaintext highlighter-rouge">[[wikilinks everywhere]]</code></a>). This becomes natural quickly: it’s common for people associated with the agora (or <a href="https://flancia.org/manifesto/">flancians</a>) to speak with wikilinks, or index links and conversations that they come across for mutual discovery.</p>

<p>The agora makes linked annotation a basic part of using the web without requiring fundamental changes in communication practices. The agora is an exercise in radically permissive protocol-like thinking: rather than creating a new app or platform, theoretically any bot could be made to crawl different mediums for wikilinks and index them. It illustrates that interfaces can precede formal protocols and serve as a testing and development ground for them.</p>

<p>Another bridging overlay for more author-focused scientific communication would be to explicitly archive the threads that increasingly serve as companions to published work — or original works of scholarship on their own (eg. <a class="citation" href="#bostonNeedKnowInformationSeeking2022">[291]</a>). I have started experimenting with this with the <a href="https://twitter.com/threadodo_bot"><code class="language-plaintext highlighter-rouge">@threadodo_bot</code></a>, a bot that converts a thread to a PDF<sup id="fnref:markdowntoo" role="doc-noteref"><a href="#fn:markdowntoo" class="footnote" rel="footnote">76</a></sup> and uploads it to Zenodo when it is tagged beneath one. This bot is being programmed as a <a href="https://github.com/sneakers-the-rat/threadodo/blob/58d5f13f88728babdf2da0b34310c88349725566/threadodo/actions/commands.py#L145-L168">generalizable framework for bots</a> that can accept parameterized commands. For example, someone can set their authorship information by tweeting “<a href="https://twitter.com/json_dirs/status/1542305909983936512">identify</a>” at threadodo, which accepts a series of key-value pairs to set your name, affiliation, and orcid. Future versions will support automatic reference generation for linked works, including previously archived threads, as well as setting prefixes for OWL schema for use in semantic <code class="language-plaintext highlighter-rouge">[[predicate::object]]</code> wikilinks.</p>

<p>When some recognizably different communication medium begins to coalesce, it should support bidirectional <em>crossposting</em> to and from existing mediums. Crossposting substantially eases transition — for example between <a href="https://crossposter.masto.donte.com.br/">Twitter and Mastodon</a> — as patterns of usage that have been trained for years on hyperoptimized attention-capturing platforms are hard to break. Together with bridges, bots, and overlays for annotation, linking, and archiving, the dream of rewriting the norms of academic communication looks less like some “if you build it they will come” pipe dream and more like a transitional period of demonstrating what we can dream of together. Adversarial interoperability not only <em>works</em> <a class="citation" href="#doctorowAdversarialInteroperability2019">[279]</a>, it’s also a gift of the <a href="https://www.gwern.net/Unseeing">hacker mindset</a> that teaches us how to make building a better world an act of unrepentant <em>joy.</em></p>

<h4 id="trackers-clients--wikis">Trackers, Clients, &amp; Wikis</h4>

<div class="draft-text">
One of the basic weaknesses is the document-only model, but recursive cells fix that. 

Having a wiki page like [[topic]] in the context of a federated wiki looks different, like the agora, where we can combine multiple people's representation of the topic and pull them. Since we can split documents into subcells, we can then also transclude them in a wiki bidirectionally: in the document metadata, but also from the wiki side.

This is how the fedwiki works! http://marc.tries.fed.wiki/view/c2-for-me/ward.eu.wiki.org/c2-for-me/wellspring.fed.wiki/welcome-visitors

!! in a linear system this gets us transclusion. one of the major flaws of the mediawiki ecosystem is the adherence to the page-only document model. If instead a page was an arbitrary structuring unit of any number of other containers, then it would be possible to do things like subdocument transclusion, building complex and recursive concepts that consist of many layers of meaning (eg. all the times you're forced to use subobjects which are awkward and unnecessary).

</div>

<p>The final set of social interfaces is effectively the “body” of social technology. So far our infrastructural systems have an unsatisfyingly hollow center: there’s a lot of talk about tool frameworks and protocols for linked data, but <em>where is it? what does it look like?</em> We can pick up the threads left hanging from our description of <a href="#archives-need-communities">bittorrent trackers</a> and knit them in with those from <a href="#the-wiki-way">the wiki way</a> and describe how systems for surfacing procedural and technical knowledge work can also serve as a basis of searching, indexing, and governing the rest of the system.</p>

<p>Bittorrent trackers serve to index data and organize a curation community — we need that too, let’s start there. Say we have a tracker that indexes a particular format of data, as <a href="https://hub.dandiarchive.org"><code class="language-plaintext highlighter-rouge">@dandihub</code></a> does with <code class="language-plaintext highlighter-rouge">@nwb</code>. We can search for data using all the fields of NWB, but don’t want to rely just on the peers that are active, so the role of the tracker is to maintain a searchable index of metadata that refers to the datasets shared by peers. We want to be interoperable with other trackers that index compatible data, so let’s say that’s implemented as a database that supports <a href="https://www.w3.org/TR/sparql11-federated-query/">SPARQL federated queries</a><sup id="fnref:sparqldb" role="doc-noteref"><a href="#fn:sparqldb" class="footnote" rel="footnote">77</a></sup> where requests can be spread across many databases. For concreteness, let’s assume that the results of our search are some content-addressed reference to a resource on a p2p network like a <a href="https://en.wikipedia.org/wiki/Magnet_URI_scheme">magnet link</a>.</p>

<p>We need some kind of <em>client</em> that can download files and run in the background to share them. We can start with the image of a bittorrent client like <a href="https://www.qbittorrent.org/">qBittorrent</a> that does just that, but we also need a means of making the link declarations that we did before in pseudocode, and it makes sense for the client to handle that as well. Let’s say our client handles our identity, either by a self-created cryptographic hash as in IPFS<a class="citation" href="#benetIPFSContentAddressed2014">[102]</a>, or attested by some trusted third party as in ActivityPub. Instead of our identity being tied to the services provided by the server, however, we can think of this as a peer-to-peer ActivityPub where we can directly send and receive messages containing our links and negotiating our connections. As an interface, say we have a typical file browser that we can set permissions for files, group them into projects, and share them with others. Since the system consists of links, an editor that allows users to visualize and edit a hierarchical graph of nodes and (typed) edges:</p>

<p>!! input network editing React figure from presentation here!</p>

<p>So say it’s time for us to share a dataset. We click the ‘share’ button in our client which sends an ActivityPub-style message saying we have <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-create"><code class="language-plaintext highlighter-rouge">@as:Create</code></a>d a new resource to the other peers indicated in our permission settings. This message both uploads the metadata for our dataset to the, say, <code class="language-plaintext highlighter-rouge">@dandihub</code> tracker, but since <code class="language-plaintext highlighter-rouge">@dandihub</code> is an equivalent peer in our system, and modeling off ActivityPub we are able to have “friends,” we can notify other researchers directly. The tracker can host our metadata pointing to our data so it’s available from any other peer that’s hosting it even if we go offline, but peers can query us directly to enumerate all the links, datasets, etc. we have allowed them to.</p>

<p>What about handling format extensions not included in the base <code class="language-plaintext highlighter-rouge">@nwb</code> format? Since we own the representation of our data, we can imagine a strict base <code class="language-plaintext highlighter-rouge">@nwb</code>-only tracker, but also think of <code class="language-plaintext highlighter-rouge">@dandihub</code> that has built tools to handle extensions. So alongside our dataset we can upload an extension like our <code class="language-plaintext highlighter-rouge">@jonny:SolarEphys</code> example that derives from <code class="language-plaintext highlighter-rouge">@nwb:ElectricalSeries</code>, and the tracker then can display our extension as well as all the other extensions that branch off the various points of the standard. At this point we can imagine a spray of thousands of trivially different extensions to handle overlapping data types, which is where most data stores typically stop, but let’s explore community systems built on forums and wikis for schema resolution as an example of <em>distributed governance.</em></p>

<p>!! figure of lots of leaf nodes hanging off ElectricalSeries</p>

<p>Wikis are not magical systems of infinite pluralistic knowledge, but one thing they do well is provide the means of developing durable but plastic systems norms and policies for a wide variety of social systems. Butler, Joyce and Pike, emphasis mine:</p>

<blockquote>
  <p>Providing tools and infrastructure mechanisms that support the development and management of policies is an important part of creating social computing systems that work. […]</p>

  <p>When organizations invest in [collaborative] technologies, […] their first step is often to put in place a collection of policies and guidelines regarding their use. <strong>However, less attention is given to the policies and guidelines created by the groups that use these systems which are often left to “emerge” spontaneously.</strong> The examples and concepts described in this paper highlight the complexity of rule formation and suggest that support should be provided to help collaborating groups create and maintain effective rulespaces.</p>

  <p>[…] <strong>The true power of wikis lies in the fact that they are a platform that provides affordances which allow for a wide variety of rich, multifaceted organizational structures.</strong> Rather than assuming that rules, policies, and guidelines are operating in only one fashion, wikis allow for, and in fact facilitate, the creation of policies and procedures that serve a wide variety of functions <a class="citation" href="#butlerDonLookNow2008">[292]</a></p>
</blockquote>

<p>So between discussion on the forum or in <code class="language-plaintext highlighter-rouge">Talk:</code>-like pages, we can imagine a set of norms and policies evolving from the community on this particuar tracker, perhaps unlike other trackers. In this case we can imagine someone wanting to clean up some near-equivalent extensions by starting a thread in the forum to discuss the proposed changes. Say we want to merge <code class="language-plaintext highlighter-rouge">@jonny:Extension1</code> and <code class="language-plaintext highlighter-rouge">@rumbly:Extension2</code> – the forum notifies us that someone is talking about our extension so we have a chance to weigh in. If we reach some sort of amicable consensus where we agree to supercede it with a merged <code class="language-plaintext highlighter-rouge">@forum:Extension3</code> type, the forum could send us a <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-offer"><code class="language-plaintext highlighter-rouge">@as:Offer</code></a> to <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-update"><code class="language-plaintext highlighter-rouge">@as:Update</code></a> our extension, which should we <a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-accept"><code class="language-plaintext highlighter-rouge">@as:Accept</code></a> from our client then notifies all the downstream consumers of our data and extension that its format has changed.</p>

<p>What if consensus fails? Since every link in the system is underneath a <code class="language-plaintext highlighter-rouge">@namespace</code>, links never have a pretense of “correctness,” but have the ontological status of a linguistic gesture: links are “something someone said” that we’re free to disagree with<sup id="fnref:timblcar" role="doc-noteref"><a href="#fn:timblcar" class="footnote" rel="footnote">78</a></sup>. In that case, the <code class="language-plaintext highlighter-rouge">@forum:Extension3</code> exists as “someone said these are equivalent, but I don’t necessarily agree” and the forum is free to represent its cleaned up representation while preserving the plurality of expression in our data format. If I want to go to greener pastures to a forum that has policies and culture closer to mine, it’s relatively straightforward to federate with a new tracker and move my data there since I still own it all.</p>

<p>Let’s pick up scientific communication in linked data <a href="#forums--feeds">forums</a> in conversation with the <a href="#archives-need-communities">social incentives for curation</a> of trackers. This system as described is a forum where everyone in the conversation has access to the data and results in question reminiscent of What.cd and access to music. While upload/download ratio might not be the best social incentive system for scientific trackers, there are plenty of others.</p>

<p>For example, we briefly mentioned a Folding@Home-like system of donated computing resources, and separately described embedding analyses in a forum by calling our own compute resources. Together, a tracker could implement a compute ratio where to use shared computing resources you need to contribute a certain amount of your own. The bounty system where peers would donate their excess upload in exchange for uploading a rare album on what.cd could translate to one where someone who has donated a lot of excess compute time could donate it for someone uploading or collecting a particular dataset.</p>

<p>Another tracker more focused on sharing and reviewing results might make a review ratio system, where for every review your work receives you need to review n other works. This would effectively function as a <strong>reviewer co-op</strong> that can make the implicit labor of reviewing explicit, and develop systems for tying the reviews required for frequent publication with explicit norms around reciprocal reviewing.</p>

<p>Forum and feedlike media are good for organizing continuous conversation, but wikis serve as a more durable knowledge store for cumulative reference information. We don’t need to imagine wikis as being text-only, with wiki formatting used just to change the appearance of text, but as a means of declaring and manipulating semantic links. For example, <a href="https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki">Semantic MediaWiki</a> is an extension to Wikipedia’s wiki system that extends <code class="language-plaintext highlighter-rouge">[[Wikilinks]]</code> to be able to declare semantic links like <code class="language-plaintext highlighter-rouge">[[linkType::Target]]</code>. For example, if our project had a wiki page like <code class="language-plaintext highlighter-rouge">[[My Project]]</code> we could say it <code class="language-plaintext highlighter-rouge">[[hasType::@analysis:project]]</code> and <code class="language-plaintext highlighter-rouge">[[usesDataset::@jonny:mydata1]]</code> etc. These wikis have the capability to not only organize knowledge, but also serve as a flexible means of declaring new programming interfaces and assigning credit.</p>

<p>As a live example, let’s consider the <a href="https://wiki.auto-pi-lot.com">Autopilot Wiki</a> at <a href="https://wiki.auto-pi-lot.com">https://wiki.auto-pi-lot.com</a>. This wiki has a set of categories, properties, templates, and forms for describing the additional contextual technical knowledge needed to use <a href="https://docs.auto-pi-lot.com/en/latest/">Autopilot</a>, a framework for behavioral experiments <a class="citation" href="#saundersAutopilotAutomatingBehavioral2019">[194]</a>. The semantic structure of the links is useful for designing interfaces based on complex queries, for example “find me all the <a href="https://wiki.auto-pi-lot.com/index.php/Category:Passive_Component"><code class="language-plaintext highlighter-rouge">passive electronic components</code></a> that have a <a href="https://wiki.auto-pi-lot.com/index.php/Category:Guide"><code class="language-plaintext highlighter-rouge">guide</code></a> that describes <a href="https://wiki.auto-pi-lot.com/index.php/Property:Uses_Tool"><code class="language-plaintext highlighter-rouge">using</code></a> a <a href="https://wiki.auto-pi-lot.com/index.php?title=Property%3AUses+Tool&amp;limit=20&amp;offset=0&amp;filter=Soldering+Iron"><code class="language-plaintext highlighter-rouge">soldering iron</code></a> to build <a href="https://wiki.auto-pi-lot.com/index.php?title=Property%3AModality&amp;limit=20&amp;offset=0&amp;filter=Illumination"><code class="language-plaintext highlighter-rouge">lighting</code></a> for a behavioral <a href="https://wiki.auto-pi-lot.com/index.php?title=Property%3AModality&amp;limit=20&amp;offset=0&amp;filter=Enclosures"><code class="language-plaintext highlighter-rouge">enclosure</code></a>”. Each page can have a <a href="https://wiki.auto-pi-lot.com/index.php/Autopilot_Behavior_Box#tab-content-facts-list">rich semantic description</a> with multimodal links describing tools, CAD diagrams, associated DOIs, software dependencies, etc. Links can be declared <code class="language-plaintext highlighter-rouge">[[linkModality::inline]]</code> as a fluid part of writing, but also can be submitted by using forms (eg for new <a href="https://wiki.auto-pi-lot.com/index.php/Form:Part">Parts</a>) with structured, autocompleting properties to lower syntax barriers for new users.</p>

<p>The “soft durability” of wikis makes space to discuss “off-label” uses for hardware common across many disciplines that typically exists as lab lore rather than documented. For example, an early-adopter of Autopilot sent me a message saying they weren’t able to get ultrasound from an <a href="https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2">amplifier</a> that was advertised up to 192kHz. Upon further study, we found there was a 20kHz low-pass output filter and were able to find and remove the components and leave a trail of breadcrumbs for future users. Though this is a simple example, it is emblematic of the kind of knowledge work that currently has no good means of communication or professional valuation.</p>

<p>The blend of programmatic and natural language descriptions makes it easy to contribute to, but also makes knowledge organization improve the software that uses it. The <a href="https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2">Amp2</a> page lists which of the GPIO pins of a raspberry pi it depends on, so Autopilot will be extended to check for conflicting hardware configurations<sup id="fnref:mutingamp2" role="doc-noteref"><a href="#fn:mutingamp2" class="footnote" rel="footnote">79</a></sup>. Better: since it’s possible for anyone to make new templates, forms, categories, and pages, the wiki can be used to build new programming interfaces entirely. Autopilot’s <a href="https://docs.auto-pi-lot.com/en/latest/guide/plugins.html">plugin system</a> is built this way, where one submits a <a href="https://wiki.auto-pi-lot.com/index.php/Autopilot_Plugins">plugin</a> with a <a href="https://wiki.auto-pi-lot.com/index.php/Form:Autopilot_Plugin">form</a> which then makes it immediately available to any Autopilot user.</p>

<p>The addition of structured contextual knowledge to our system gives us an almost comical degree of provenance: from conversations in a forum that reference a paper, that links to its analysis, data, experimental software, all the way back to the properties of the solenoids used in the experiment. It’s not just provenance for provenance’s sake as extra labor, every step is <em>useful</em> to the experimenter. I give the example of the Autopilot wiki for concreteness, but the broader point is that forums and wikis can serve the role of negotiating systems of expression for different parts of the system.</p>

<p>The same combination of trackers, forums, and wikis has a natural application to analysis pipelines. Ideally, to move beyond fragile code reduplicated in every lab, we need some means of reaching consensus on a few canonical implementations of fundamental analysis operations. Given a system where analysis chains are linked to the formats and subdisciplines they are used with, we can map a semantically dense map of the analysis paths used in a research domain. In neurophysiology: “What are the different ways spikes are extracted and analyzed from extracellular electrophysiology recordings?” Having the ability to discuss and contextualize different analytical methods elevates all the exasperated methods critiques and exhortations to “not use this statistically unsound technique” into something <em>structurally expressed in the practice of science.</em> See all the <code class="language-plaintext highlighter-rouge">@neurotheory</code> threads about this specific analysis chain, or the <code class="language-plaintext highlighter-rouge">@methodswiki</code> page that summarizes this general category of techniques.</p>

<p>We’re now in a place where we can address the problem of a cumulative knowledge system for science directly. In many (most?) scientific epistemologies, scientific results do not directly reflect some truth about reality, but instead instead are embedded in a system of meaning through a process of active interpretation (eg. <a class="citation" href="#meehlTheoreticalRisksTabular1978">[293]</a>). The interpretation of every scientific result is left as the responsibility of the authors to recreate and a few reviewers to evaluate, which would be a monumental amount of labor given the velocity of papers, so researchers do the best they can engaging with a small amount of research. Since the space of argumentation is built from scratch each time from incomplete information, there’s no guarantee of making cumulative progress on a shared set of theories, and most fall far from the supposed ideal of hard refutation and can have long lives as “zombie theories.” van Rooij and Baggio describe the “collecting seashells” approach of gathering many results and leaving the theory for later with an analogy:</p>

<blockquote>
  <p>“In a sense, trying to build theories on collections of effects is much like trying to write novels by collecting sentences from randomly generated letter strings. Indeed, each novel ultimately consists of strings of letters, and theories should ultimately be compatible with effects. Still, the majority of the (infinitely possible) effects are irrelevant for the aims of theory building, just as the majority of (infinitely possible) sentences are irrelevant for writing a novel.” <a class="citation" href="#vanrooijTheoryTestHow2021">[294]</a></p>
</blockquote>

<p>They and others (eg. <a class="citation" href="#guestHowComputationalModeling2021">[295]</a>) have argued for an iterative process of experiments informed by theory and modeling that confirm or constrain future models. Their articulation of the need for multiple registers of formality and rigidity is particularly resonant here. van Rooij and Baggio again:</p>

<blockquote>
  <p>“The first sketch of an f need not be the final one; what matters is how the initial f is constrained and refined and how the rectification process can actually drive the theory forward. Theory building is a creative process involving a dialectic of divergent and convergent thinking, informal and formal thinking.” <a class="citation" href="#vanrooijTheoryTestHow2021">[294]</a></p>
</blockquote>

<p>Let’s turn our provenance chain into a circle: a means of linking theories to analytical results and interpretation as well as experimental design and tooling. Say the theorists have a wiki. They start making some loose schematic descriptions of their theories and linking them to different experimental results that constrain, affirm, refute, or otherwise interact with them. These could be forward or backlinks: declared by the original author or by someone else describing their results.</p>

<p>In the most optimistic case, where we have a full provenance chain from analytical results back through experimental practice, we have a means of formally evaluating the empirical contingencies that serve as the evidence for scientific theories. For a given body of experimental data bearing on a theoretical question, what kinds of evidence exist? As the state of the art in analytical tooling changes, how are the interpretations of prior results changed by different analyses? How do different experimental methodologies influence the form of our theories? The points of conflicting evidence and unevaluated predictions of theory are then a means of distributed coordination of future experiments: guided by a distributed body of evidence and interpretation, rather than the amount of the literature base individual researchers are able to hold in mind, what are the most informative experiments to do?</p>

<p>The pessimistic case where we only have scientific papers in their current form to evaluate is not that much worse — it requires the normal reading and evaluation of experimental results of a review paper, but the process of annotating the paper to describe its experimental and analytical methods as a shared body of links makes that work cumulative. Even more pessimistic, where for some reason we aren’t able to formulate theories even as rough schematics but just link experimental results to rough topic domains is still vastly better than the current state of disorganization and proprietary indices.</p>

<p>For both researchers and the public at large a meta-organization of experimental results changes the way we interact with scientific literature. It currently takes many years of implicit knowledge to understand any scientific subfield: finding canonical papers, knowing which researchers to follow, which keywords to search in table of contents alerts. Being able to find a collection of papers about an object of research, as well as the conversations at all levels of formality that contextualize them — to say nothing of building a world without paywalls — would profoundly lower barriers to access to primary scientific knowledge for <em>everyone.</em></p>

<p>It is worth pausing to compare a world where we boisterously and fluidly organize knowledge explicitly as a collective project of understanding with one where knowledge organization is weaponized into a product that lets us get ahead of our competitors without necessarily improving our understanding of the body of scientific literature. One sounds like science, the other sounds like industry capture.</p>

<p>All the technological-social tools described here are not a definitive set of tools needed for scientific communications infrastructure, but <em>examples of interfaces to a linked data system.</em> Using JSON-LD notebooks to enable us to embed links in our writing to be mentioned or transcluded elsewhere. Using a forum as a means of creating linked discussions about experimental results and analyses. Using linked microblogging tools for a rapid, informal means of organizing and discussing knowledge. Using all of the above to represent the many expressions of a work across multiple linked namespaces. Using annotation tools to create anchors and links for referencing links in other communication media. Using tracker-like and wiki-like systems to interact with, negotiate about, and govern a wily body of autonomously declared links.</p>

<p>Each is intended to be mutable, easy to iterate on, uncontrolling, mutually coordinated. Each interacts with and augments the previously described systems for shared data, analytical, and experimental tools. The purpose of this section is not to advocate a specific set of technologies, but to describe a base layer of familiar technologies for an indefinite future of possible interfaces for representing and interacting with a body of shared knowledge.</p>

<p>What we’ve described is a nonutopian, fully realizable path to making a scientific system that is fully negotiable through the entire theoretical-empirical loop with minor development of existing tools and minimal adjustment of scientific practices. No clouds, no journals, a little rough around the edges but collectively owned by all scientists.</p>

<div class="draft-text">Final system summary</div>

<h3 id="credit-assignment">Credit Assignment</h3>

<blockquote>
  <p>The reason we are (once again) having a fight about whether the producers of publicly available/published data should be authors on any work using said data is that we have a completely dysfunctional system for crediting the generation of useful data. <a class="citation" href="#eisenReasonWeAre2021">[296]</a></p>

  <p>The same is true for people who generate useful reagents, resources and software. <a class="citation" href="#eisenSameTruePeople2021">[297]</a></p>

  <p>And like everything, the real answer lies on how we assess candidates for jobs, grants, etc… So long as people treat authorship as the most/only valuable currency, this debate will fester. But it’s in our power to change it. - Michael Eisen, EIC eLife <a class="citation" href="#eisenEverythingRealAnswer2021">[298]</a></p>
</blockquote>

<div class="draft-text">
	quote from harold varmus of PubMed: 

	I also think one of the big obstacles to freeing up scientific information remains the way in which we continue to pay allegiance to the idea that the most important work is published in so-called ‘high-impact’ journals that continue to restrict access by imposing highly lucrative subscription fees. These journals continue to thrive, despite a kind of anti-social policy, because so many academic scientists evaluate each other’s work and measure abilities and accomplishments based on where people have published.

	The only way by which we’ll eventually get out of the current situation is by changing the formula dramatically. That means that we’ll probably have to move to a world where the authors have full control – their work will be presented online together with expert reviews and perhaps accompanied by a new evaluation system in which members of the scientific community will provide qualitative and perhaps quantitative measures of the value of the paper. The current world of high- and low-impact journals will eventually dissolve, it's just taking a lot longer than I thought

	<a class="citation" href="#varmusOncogenesOpenScience2019">[299]</a>
</div>

<p>The critical anchor for changes to the scientific infrastructural systems is the system of professional incentives that structure it. As long as the only thing that has professional value is authorship in journal papers, the system stays: Blog posts, analysis pathways, wikis, and forums are nice and all, but they don’t count as <em>science.</em></p>

<p>Imagining different systems of credit assignment is easy: just make a new DOI-like identifier for my datasets that I can put on my CV. Integrating systems of credit assignment into commonly-held beliefs about what is valuable is harder. One way to frame solutions to the credit assignment problem is as a collective action problem: everyone/funding agencies/hiring committees just need to <em>decide</em> that publishing data, reviewing, criticism et al. is valuable without any serious changes to broader scientific infrastructure. Another is to <em>displace</em> the system of credit assignment by aligning the interests of the broad array of researchers, technicians, and students that it directly impacts to build an alternative.</p>

<p>The sheer quantity of work that is currently uncredited in science is a structural advantage to any more expansive system of credit assignment. The strategic question is how to design a system that aligns the self-interest of everyone with uncredited work to build it.</p>

<p>That’s what I’ve tried to do here. Everything that exists in this system is attributable to one or many equal peers. Rather than attempting to be an abstract body of knowledge, clean and tidy, that conceals its social underpinnings, we embrace its messy and pluralistic personality. We have <em>not</em> been focused on some techno-utopian dream of automatically computing over a system of universally linked data, but on representing and negotiating over a globally discontinuous body of work and ideas linked to people and groups. We have <em>not</em> been imagining new platforms and services to suit a limited set of needs, but on a set of tools and frameworks to let people work together to cumulatively build what they need.</p>

<p>Credit is woven throughout this system: the means of using someone else’s work are tied to crediting it. While credit is currently meted out by proprietary journal aggregators like google scholar, citeseer, or web of science; downloading a dataset, using an analysis tool, and so on should be directly attributed to a digital identity that you control.</p>

<p>The first-order effects for the usual suspects in need of credit are straightforward: counting the number of analyses and papers our datasets are cited in, seeing the type of experiments our software was used to perform. Control over the means of credit assignment also opens the possibility of surfacing the work that happens invisibly but is nonetheless essential for the normal operation of research. Why shouldn’t the animal care technician receive credit for caring for the animals that were involved with a study, its results, and its impact on science more broadly?</p>

<p>Contextual technical knowledge is an example that warrants special consideration. Why would anyone spend the time to describe the fine technical details of how to use a type of motor, or which solenoids last the longest, or how to solder this particular type of circuit board?</p>

<p>First (and hopefully familiarly), by making it practically useful for the researchers involved: say in this example we’re using a lab wiki to coordinate work locally, using tools that can use the wiki information to automatically configure <a href="https://wiki.auto-pi-lot.com/index.php/Lee_LHDA0531115H">solenoid</a> or <a href="https://wiki.auto-pi-lot.com/index.php/TT_Electronics_OPB903L55">sensor</a> polarity, or the <a href="https://wiki.auto-pi-lot.com/index.php/HiFiBerry_Amp2">dependencies for a sound card</a>.</p>

<p>Second, by making sure the researchers are credited for their work. A name prominently displayed on a wiki page and a permalink for a CV is ok, but clearly not enough. Foundational technical and documentation work like this is useful in itself, but its impact is mostly felt <em>downstream</em> in the work it enables. Beyond first-order credit, a linked credit assignment system lets us evaluate <em>higher-order</em> effects of work that <em>more closely resemble</em> the actual impact of the work. Say we find someone else’s <a href="https://wiki.auto-pi-lot.com/index.php/3D_CAD">3D Model</a>, modify it for our use, and then use it to collect a dataset and publish a paper. Someone else sees it and links a colleague to it, and they too use it in their work. Over time someone else updates the design and puts it in some derivative component. Most of the linking is automatic, built into the interfaces of the relevant tools, and soon the network of links is dense and deep.</p>

<p>The incentives here are all aligned towards creating links and assigning credit: For us, instead of just getting professional credit for our paper, we also get credit for extending someone else’s work, for documenting it, and for the potentially large number of nth-order derivative uses. Our credit extends multimodally, including papers that cite papers that use our tool, and the “amount” of credit can be contextualized because the type of link between them is explicit – as opposed to the non-semantic links of citation. Our colleague that recommended our part gets credit as well, as they should since helpful communication is presumably something we want to reward. I <em>want</em> to use the extended graph of credit rather than just listing my paper because it’s a lot more impressive! Since I want to be credited, I’m also invested in expanding the space of linked tools. Rather than the scarcity mindset of authorship, a link-based system can push us towards abundance: “good” work is work that engages with and extends a broad array of techniques, technologies, and expertise.</p>

<p>It’s easy to imagine extended credit scenarios for a broad array of workers: since my work happens at <code class="language-plaintext highlighter-rouge">@institution</code>, and the <code class="language-plaintext highlighter-rouge">@institution:mice</code> are cared for by the members of the <code class="language-plaintext highlighter-rouge">@institution:animal_care</code> team, we can measure the impact of their work on the downstream work it supports. A grad student rotating in a lab might not get enough data to make a paper, but they might make some tangible improvement to lab infrastructure, which they can document and receive credit for. Open source software developers might get some credit from a code paper, but will be systematically undervalued from failure to cite it and undercounted in derivative packages. The many groups of workers whose work is formally excluded from scientific valuation are those with the most to gain by reimagining credit systems, and an infrastructural plan that actively involves them and elevates their work has a much broader base of labor, expertise, and potential for buy-in.</p>

<p>Some of my more communitarian colleagues might share my distaste for metricizing knowledge work — but hiring committees and granting agencies are going to use <em>some</em> metric, the question is whether it’s a good reflection of our work and who controls it. Our problems with the h-index (eg. <a class="citation" href="#teixeiradasilvaMultipleVersionsHindex2018">[300, 301]</a>) are problems with paper citations being a bad basis for evaluating scientific “value”, and their primacy is in turn a consequence of the monopoly over scientific communication and organization by publishers and aggregators like Scopus and Google Scholar. Their successors, black box algorithmic tools like SciVal with valuation criteria that are bad for science (but good for administrators) like ‘trendiness’ are here whether we like it or not. A transparent graph of scientific credit at least gives the <em>possibility</em> for reimagining the more fundamental questions of scientific valuation: assigning credit for communication, maintenance, mentorship, and so on. So some misguided reductions of the complexity of scientific labor to a single number are inevitable, but at least we’ll be able to <em>see what they’re based on</em> and <em>propose alternatives.</em></p>

<p>It’s true that some of these extended metrics are already possible to compute. One could crawl package dependencies for code, or download the <a href="https://academictorrents.com/details/e4287cb7619999709f6e9db5c359dda17e93d515">100GB Crossref database</a> <a class="citation" href="#crossrefJanuary2021Public2021">[302]</a> and manually crunch our statistics, but being <em>able</em> to compute some means of credit is very different than making it a <em>normal part</em> of doing and evaluating research. The multimodality of credit assignment that’s possible with a linked data system is part of its power: our work <em>actually does</em> have impacts across modalities, and we should be able to represent that as part of our contribution to science.</p>

<p>Reaching a critical mass of linked tools and peers is not altogether necessary for them to be useful, but critical mass may trigger a positive feedback loop for the development of the system itself. Even in isolation, a semantic wiki is a better means of assigning credit than a handful of google docs, experimental tools that automatically annotate data are better than a pile of <code class="language-plaintext highlighter-rouge">.csv</code> files, etc. Bridging two tools to share credit is better than one tool in isolation, and more people using them are better than fewer for any given user of the system. Lessons learned from STS, Computer-Supported Cooperative Work (CSCW), pirates, wikis, forums, et al. make it clear that <em>the labor of maintaining and building the system can’t be invisible.</em></p>

<div class="draft-text">Find citations and quote for ^^. Fortunately building a system for credit assignment should allow you to be credited! Conclude by talking about building more just system of valuing science, and that's one of the critical means by which any infrastructure might displace hegemonic systems! 
</div>

<div class="draft-text">Need to zoom out a bit here -- credit assignment is really the problem here, so what have we done? we've imagined a new way of assigning credit. Instead of starting from publication and working backwards to identity through citations, we start from identity and build a system for creating things and defining the things you do in a public way, integrated with the practice of research. The idea of namespaces is a foundational part of the web (cite tim BL quote about how scientists actually started the notion of linking that inspired the web), permalinks and URIs, but it wasn't formulated as a means of identity. DNS and the system of domains as it is now, where you need to ask permission to create a new one and there is a single representation of a name. it's one particular system for assigning identity. We can extend a lot of the ideas of the web re: URIs to identites that have multiple parallel namespaces by constructing it around a combined p2p and computational system. 
</div>

<h1 id="conclusion">Conclusion</h1>

<h2 id="tactics--strategy">Tactics &amp; Strategy</h2>

<blockquote>
  <p>Don’t scab for the bosses / don’t listen to their lies / us poor folks haven’t got a chance / unless we organize</p>

  <p>Which side are you on?</p>

  <p>Florence Reece (1931) <em>Which Side Are You On?</em></p>
</blockquote>

<blockquote>
  <p><strong>Oh but they will mock us and they will mistreat us til they can replace us all with an app or a kiosk,</strong> And apathy is the natural byproduct of being exhausted and sick all the time. So don’t tell me that working like this is the meaning of life […]</p>

  <p>Cuz it’s just a means to an ending, and all of the energy that I end up expending, I will get it back in spades when the systems that necessitate all of this work fall apart… <strong>And we can work for ourselves for a change!</strong></p>

  <p>So we gotta work! Cuz none of our visions of a better tomorrow will come to fruition without a whole lot of work! And that’s all there is to it, so, God Dammit <strong>I guess that we gotta do it!</strong></p>

  <p>RENT STRIKE (2021) <a href="https://rentstrike.bandcamp.com/track/work-future-perfect-2">Work! (Future Perfect)</a> <a class="citation" href="#rentstrikeWorkFuturePerfect2021">[303]</a></p>
</blockquote>

<div class="draft-text">
	Emphasize that we need a plan and a strategy, but we also need the will. We need to organize against tech companies and the rapidly corporatizing university. We are part of the labor struggle even if we don't acknowledge knowledge work as labor.
</div>

<h3 id="gestural-roadmap">(Gestural) Roadmap</h3>

<p>Some of the tactical vision for this piece is embedded in its structure and has been discussed throughout, but to again reaffirm the strictly <em>non-utopian</em> nature of this argument it’s worth revisiting the practical means by which we might build it. I have tried to take care to hew as close to existing technologies and practices as possible, and so the amount of new development that is needed is relatively light. As is true in the rest of the piece, the recommendations here are just for the purpose of illustration, and here more than anywhere else every step of this is subject to negotiation and the contingency of future work.</p>

<p>For the purposes of brevity, I’m going to refer to the family of RDF-based tools like JSON-LD, turtle, OWL, and so on as “RDF-like.”</p>

<p>These, I think, are the most minimal development steps that would get a system like this off the ground and offer some basic use.</p>

<ol>
  <li>Build framework for <strong>bridging RDF-like schema to p2p client</strong> - The implementation of a given schema needs to be made abstract so that data can be subset from a given dataset using the notation of the schema namespace. This can happen gradually – at first mapping from metadata to the entire dataset, but need to be able to read/write individual entities. <a href="https://docs.dat.foundation/docs/faq">Dat</a> can share subsets of data using Hypercore’s <a href="https://hypercore-protocol.org/guides/walkthroughs/creating-and-sharing-hypercores/">sparse mode</a>. Work could also happen in the other direction, extending DANDI/Datalad with a peer-to-peer backend.</li>
  <li>Build <strong>p2p/ActivityPub client</strong> - currently activitypub accounts are associated with a homeserver like <a href="https://mastodon.social/">https://mastodon.social/</a>, but the authors of the ActivityPub protocol and JSON-LD have described how a system of decentralized identity (eg. DID <a class="citation" href="#spornyDecentralizedIdentifiersDIDs2021">[185]</a>) could make it fully peer-to-peer <a class="citation" href="#webberActivityPubDecentralizedDistributed2017">[276]</a>. Joint ActivityPub/p2p systems that use AP to index data and p2p to share it already exist (eg. <a href="https://joinpeertube.org/#what-is-peertube">PeerTube</a>). Much of the work left undefined here would be interface and UX design to make hosting an instance as easy as possible.</li>
  <li>Experiment with <strong>minimal schema for federation.</strong> We want to get code running first in smaller communities to experiment with the basic ontologies for communication, data sharing, and framework linking. There are a <a href="https://lov.linkeddata.es/dataset/lov/vocabs">large number</a> of existing vocabularies and ontologies to draw from, so we don’t need to start from scratch. More important than <em>which</em> ontology is chosen is making it <em>simple</em> to browse and manipulate them. Eg.:
    <ul>
      <li>Social interaction: <a href="https://www.w3.org/TR/activitystreams-vocabulary/">ActivityStreams</a> <a class="citation" href="#snellActivityStreams2017">[190]</a>, Semantically Interlinked Online Communities (<a href="http://sioc-project.org/">SIOC</a>) <a class="citation" href="#harthLinkingSemanticallyEnabled2004">[304]</a></li>
      <li>Permissions: Open Digital Rights Language (<a href="https://www.w3.org/ns/odrl/2/">ODRL</a>)</li>
      <li>Scientific communication - <a href="http://linkedscience.org/lsc/ns/">Linked Science Core</a> <a class="citation" href="#kauppinenLinkedOpenScienceCommunicating2011">[305]</a>, Modern Science Ontology (<a href="https://saidfathalla.github.io/Science-knowledge-graph-ontologies/doc/ModSci_doc/index-en.html#dataproperties-headline">ModSci</a>)</li>
      <li>Workflows/Analysis Pipelines - Open Provenance Model for Workflows (<a href="https://www.opmw.org/model/OPMW/">OPMW</a>)</li>
      <li>and many, many more.</li>
    </ul>
  </li>
  <li>A basic <strong>tracker-like web framework</strong> for caching and serving metadata, as well as hosting some initial forums or other semidurable communication system for organizing federations. It’s an open question to me how much of <a href="https://github.com/WhatCD/Gazelle">Gazelle</a>/<a href="https://github.com/WhatCD/Ocelot">Ocelot</a> is worth resurrecting, or whether it would be better to build from an ActivityPub client or a torrent tracker (or some other existing code I’m not familiar with).</li>
</ol>

<p>From that basic means of communication, the rest of the development path needs less specification, the examples I have given are just one way of realizing the <a href="#design-principles">design principles</a>. Beyond that, doing accounting for the other functionality described above:</p>

<ul>
  <li>Start building <strong>bridges to existing repositories</strong> like <a href="https://gui.dandiarchive.org/#/">Dandihub</a>, Wikidata, and so on.</li>
  <li>Refine vocabularies! Refine schema negotiation, etc.</li>
  <li>Build tools for data translation - <em>theoretically</em> building I/O tools from RDF-like schema to individual data formats should allow for interconversion, but it will of course be more difficult than that.</li>
  <li>Extend analysis and experimental tools to incorporate and produce linked data within our p2p/AP system.</li>
  <li>Integrate dependency management for linked data that specifies code to run, eg. see <a href="https://spack.readthedocs.io/en/latest/#">spack</a> (see spack)</li>
  <li>The wide world of communication tools awaits…</li>
</ul>

<p>The numerical order of these lists is just a byproduct of the linearity of text, and many of these development projects can happen simultaneously with minimal mutual coordination. The chronology given is mostly strategic: finding an incremental development path where every step is useful and allows for the next to be taken.</p>

<p>A p2p data system is, to me, is the most likely and proximal anchor from which the rest of the technologies might flow — this is part of the idea animating the Solid project as well <a class="citation" href="#berners-leeSociallyAwareCloud2009">[23, 306, 307, 64]</a>.</p>

<p>There is no reason, in principle, that data must first be converted into some standardized format — ideally, it would be possible to fluidly link data in-place, incorporating whatever means by which metadata is stored already. Passing through a point of standardization serves a few purposes: first, lowering the work associated with linking by only needing to declare links between a few hundred formats instead of the infinity of arbitrarily structured data. Second, to minimize frustration and maximize delight of early adopters: people are more likely to stick around if they can run a client, plug their data in, and see it hosted with the links prepopulated from the format schema. Third, to integrate with existing tools and databases to avoid the perception of potential sunk cost spending time formatting data in some new idiosyncratic way.</p>

<p>Shared data is a concrete, widely understood goal shared by many scientists already, but there are relatively slim incentives for spending the time to do it. The first major hurdle is to make those incentives. Propping up a p2p system will eventually need new development, but existing p2p systems can still make a strong case for themselves with small, local examples: using them to share data with local collaborators, or to share data during a workshop or conference, or even to start rehosting already-public centrally hosted data. Small communities of practice can start their own “retreat from the cloud” by documenting their process and setting up their own local hosting and servers. They also make natural allies with the p2p tool developers. Being a test case for their software and cultivating social ties across domains is one way to start aligning our goals and movement building. Tools like Dat and Solid are good fits, though they currently need some UX and docs work to be accessible to a broad audience.</p>

<p>Cultivating new relationships with knowledge and technical communities outside our usual academic circles is a critical part of any infrastructural development project. Though these days we hear about disaffected people abandoning academia for industry, there is plenty of disaffectation to go around on the other side — particularly in software world. There are a growing number of extremely talented programmers that grow disillusioned with working for companies they view as unethical, which unfortunately happen to be some of the largest employers in the field. Ethical, important, and dare I say fun software often has no business model, but data, computation, and communications infrastructural infrastructure in science, especially with a coherent frame could give many of them a job they don’t have to feel conflicted about.</p>

<div class="draft-text">
  <ul>
    <li>once some people are sharing their data on p2p, need some means of organizing it. adapting frontend tools and start using distributed communication tools. People are clearly eager to use stuff like DandiHub and get their data into an index, but they are all so lonely and vacant! Only instead of discord and slack stuff like Matrix is scalable!</li>
    <li>general pattern of integrating with new communities, for that we need some means of communication. Where are the technologists we should ally with? They’re on fediverse and Matrix!</li>
    <li>Lots of proofs of concept, but need communities to actually start testing them and using them! https://scenaristeur.github.io/agora/ https://openengiadina.net/</li>
    <li>Starting communication and knowledge organization on wikis and etc. is itself a step towards realizing the system! Tools like Matrix make it so people don’t have to commit to <em>one community or platform in particular</em> but instead can explore and sort themselves. The UX has come a long way in the last few years, and Element and Gitter are both ready for general use.</li>
    <li>Knowledge organizing systems like openbehavior and stuff are cool, we should start transcluding them (with credit!) into wikis and other social tools in order to organize a broader scope of the software and social scene.</li>
    <li>Flanking technologies of data analysis and experimental tooling serve as stronger incentive systems.</li>
    <li>Not needing to build a single new platform, or new journal, or even a new organization, but focusing on means of communication.</li>
    <li>adversarial interoperability: always focus on being a superset of existing technologies.</li>
    <li>make metadata declaration easier than translating with the scattered array of data ingestion tools.</li>
  </ul>
</div>

<h3 id="to-whom-it-may-concern">To Whom It May Concern…</h3>

<p>Who is supposed to do what, and why would different interested groups want to pursue decentralized infrastructure? A few love letters addressed to different groups:</p>

<h4 id="rank-and-file-researchers">Rank and File Researchers</h4>

<ul>
  <li>PIs vs grad students/post-docs: PIs need to realize that the true cost is doing nothing, the ROI on infrastructure is massive given the extremely high costs of labor for doing all this shit. Grad students and Post docs should start seeing the total isolation of their local tooling as problematic and engage with their neighboring labs to share technologies and start building locally integrated tools!</li>
</ul>

<div class="draft-text">Description of ONICE!</div>

<ul>
  <li>We need to start making alliances we’re not necessarily used to, but this is the fun part! !! ally with the many disaffected tech workers that don’t want to work with google and facebook – we all talk about ppl fleeing for industry, but what academia can offer in return is jobs building tools that aren’t soul sucking click maximizers.</li>
  <li>We should also start working closer with our Librarians, they are also facing the squeeze and have had their profession degraded to being subscription custodians.</li>
  <li>We need to recognize our place outside of the highest echelons as fundamentally in danger by advancing infrastructural polarization. A “not my job” mentality might work for now, but for how long?</li>
  <li>Less concretely, we need to start expanding what we think is possible! We need to be realistic and demand the impossible! Let’s let the work of escaping ownership by platform capitalism be joyful, a rennaisance of working cooperatively and rejuvinating the sense of purpose as scientists invested in the health of society.</li>
</ul>

<h4 id="open-source-developers">Open Source Developers</h4>

<ul>
  <li>UX and community systems first! Start a project by reaching out to other devs and seeing who’s doing overlapping work.</li>
  <li>Integrate with existing tools rather than bulding new ones is holy: you can still get credit in existing systems for writing the paper, and your tool is more likely to be used, and it’s likely to benefit from some of the structuring elements of the framework.</li>
  <li>Stop building cloud shit! Or if you have to make something for the cloud bc compute infrastructure isn’t there, make sure it’s also deployable on local computers. We don’t need any more single-use platforms!</li>
</ul>

<h4 id="funding-agencies">Funding Agencies</h4>

<ul>
  <li>If you pay us we will build it!</li>
  <li>fund integrating existing tools in addition to maintaining them. Target funding for new tools that fill specific gaps — it’s almost impossible to get a really well maintained library off the ground b/c catch-22 of development!</li>
  <li>You’re being swindled! Sort of a conflict of interest because to some degree centralization is politically useful: eg. see data sharing agreement with ICE, but this probably isn’t shared by the people actually at the granting agencies and I don’t want to speculate on some conspiracy theory.</li>
</ul>

<h4 id="university-administrators">University Administrators</h4>

<ul>
  <li>You’re also being swindled!</li>
  <li>Local infrastructure is good for you too — many universities are plagued by SaaS that is expedient but ultimately makes the entire operation of the university very fragile.</li>
  <li>Having good local data infrastructure is a really good thing to be able to tell applicants, and makes use of intranet for collaboration instead of external bandwidth. You get to say “we have a sick new storage and compute server” instead of “we’re a huge subscriber to AWS”</li>
  <li>Y’all are the ones who have to pay the journal costs and deal with your university being uncompetetive with other institutions that can afford more, and so you should be leading the charge to nonprofit journals and a move beyond them, rather than mandating Open Access which is a regressive move.</li>
</ul>

<h4 id="librarians">Librarians!</h4>

<p>We love ya</p>

<blockquote>
  <p>In other words—and the Loon is grinning a beaky grin just now, because her BAE said exactly this to a roomful of library linked-data people about a decade ago—make it easy to rely on linked data, easier than it is to rely on MARC, and the library world will shift, from the smallest and poorest libraries upward… and David will at last stone Goliath to death with his linked-data slingshot. <a class="citation" href="#librariaStoningGoliath2022">[174]</a></p>
</blockquote>

<h2 id="limitations">Limitations</h2>

<p>We can start with a few of the big and obvious limitations: people could ignore this piece entirely and it is dead on arrival. This project would be a direct threat to some of the most powerful entities in science, and they will likely actively work against it. I could be completely misinformed and missing something fundamental about the underlying technologies. The social tensions between the relevant development communities could be too great to overcome.</p>

<p>Two outstanding problems on Mastodon hint at a few open challenges to development: feed organization and the fluidity of federation formation, dissolution, and interaction.</p>

<p>By default, and affirmed by maybe an understandable reaction against algorithmic feed organization, Mastodon is a mostly chronological list of posts from people that you follow and that are in your host server’s federated networks. While this transparency is reassuring that we aren’t being microtargeted for advertising, it does make the system overwhelming to navigate, and splitting accounts multiple times to accomodate is common. A system of semantic organization is a distinct third way between algorithmic and chronological organization. Building a system that goes beyond moderator-specified category systems familiar in forums towards a sensible interface for navigating tangled concept hierarchies is an open challenge, as far as I’m aware.</p>

<p>An intermediate goal might be to give finer control over groups, but groups are currently a complicated question between fediverse implementations <a class="citation" href="#StandardizingActivityPubGroups2021">[308]</a>.</p>

<div class="draft-text">
- identity!
- interaction of p2p and linked data system -- lightweight linked metadata can be reproduced more easily than massive raw data, but it needs to be possible to apply permissions and access regulation with more verifiability than just being able to access a unique tracker ID or being pointed to a UUID.
- some might say we will have a hard time indexing across a bunch of namespaces that people hold individually -- this is actually a good thing. We *want* the system to be difficult to make full scrapes to capture and repackage. We *want* connections to be purposeful and transparent, rather than having arbitrary crawlers sucking up all scientific data.
- ppl might lie! ppl already lie! and we handle ambiguity all the time. the real dangerous thing is a system that *presents* itself as infallible/neutral/true.
</div>

<h2 id="in-closing">In Closing</h2>

<h2 id="contrasting-visions-of-science">Contrasting Visions of Science</h2>

<p>Through this text I have tried to sketch in parallel a potentially liberatory infrastructural future with the many offramps and alternatives that could lead us astray, but haven’t given a picture of what <em>actually doing research</em> might be like were this project to come anywhere close to succeeding. Through the hints at what could be our current and future information capitalism dystopia the alternative is too a little foggy: what happens if we do nothing? Let me make the point with a bit of speculative fiction.</p>

<h3 id="what-if-we-do-nothing">What if we do nothing?</h3>

<p>You’re a researcher with dead-center median funding at an institute with dead-center median prestige, and you have a new idea.</p>

<p>The publishing industry has built its surveillance systems into much of the practice of science: their SeamlessAccess login system and browser fingerprinting harvest your reading patterns across the web<a class="citation" href="#sariGuestPostTechnology2018">[309, 310, 311, 312, 313]</a>, Mendeley watches what you highlight and how you organize papers, and with a data sharing agreement with Google crossreference and deanonymize your drafts in progress <a class="citation" href="#pooleySurveillancePublishing2021">[6]</a>. Managing constant surveillance is a normal part of doing science now, so when reading papers you are careful to always use a VPN, stay off the WiFi whenever possible, randomly scroll around the page to appear productive while the PDF is printing to read offline. The publishers have finally managed to kill sci-hub with a combination of litigation and lobbying universities to implement mandatory multifactor authentication, cutting off their ability to scrape new papers. The few papers you’re able to find, and fewer that you’re able to access, after several weeks of carefully covering your tracks while hopping citation trees make you think your hunch might be right — you’re on to something.</p>

<p>This is a perfect project for a collaboration with an old colleague from back in grad school. Their SciVal Ranking is a little low, so you’re taking a risk by working with them, but friendship has to be worth something right? “Don’t tell me I never did nothing for you.” You haven’t spoken in many years though, so you have to be careful on your approach. The repackaged products of all their surveillance are sold back to the few top-tier labs able to afford the hype-prediction products that steer all of their research programs <a class="citation" href="#lifesciencesprofessionalservicesEmergingTrendsPancreatitis2021">[314, 315]</a>. The publishers sell tips on what’s hot, and since they sell the same products to granting agencies and control the publishing process, every prediction can be self-fulfilling — the product is plainly prestige, and the product is good. If you approach your colleague carelessly, they could turn around and plug the idea into the algorithm to check its score, tipping off the larger labs that can turn their armies of postdocs on a dime to pounce. There is no keeping up with the elites anymore.</p>

<p>Even if you do manage to keep it a secret, it’ll be a hard road to pull off the experiment at all. There are a few scattered open source tools left, but the rest have been salami sliced into a few dozen mutually incompatible platforms (compatibility only available with the HyperGold Editions). The larger labs are able to afford all the engineers they need to build tools, but have little reason to share any of the technical knowledge with the rest of us — why should they spoil the chance to spin it off into a startup? There aren’t any jobs left in academia anyway.</p>

<p>Industry capture has crept into ever more of the little grant funding you have, all the subscriptions and fees add up, so you can only afford to mentor one grad student at a time while keeping plausibly up to date with new instrument technology. You can’t choose who they are anymore really. The candidate ranking algorithms have thoroughly baked the exclusionary biases of the history of science into the pool of applicants<a class="citation" href="#pooleySurveillancePublishing2021">[6, 14]</a>, so the only ones left are those who have been playing to the algorithm since they were in middle school. Advocates for any sort of diversity in academia are long gone. We’ve never been able to confirm it, but everyone knows that the publishers tip the scales of the algorithm to downrank anyone who starts organizing against them.</p>

<p>Your colleague and you manage to coordinate. they’re the same as they’ve always been, trustworthy. You really need someone from a different field at least in consultation, but there isn’t really a good way to find who would be a good fit. Somehow Twitter is still the best way to communicate at large, but you’ve never really gotten how it works and the discourse has gotten <em>dark</em> so you don’t have enough followers to reach outside your small bubble of friends. You decide to go it your own, and find the best papers you can from what you think is the right literature base, but there’s no good way of knowing you’re following the right track. Maybe that part of the paper is for the supplement.</p>

<p>Data is expensive, if you can find it. Who can pay the egress costs for several TB anymore? You forego some modeling that would help with designing the experiment because you don’t have the right subscription to access the data you need. You’ll have to wait until there is a promotional event to to get some from a Science Influencer.</p>

<p>You experiment in public silence until you’ve collected your data. Phew, probably safe from getting scooped. You start the long slog of batch analysis with the scraps of Cloud Compute time you can afford.</p>

<p>Papers are largely unchanged, still the same old PDFs. They’re a source of grim nostalgia, at least we’ll always have PDF. What has changed is citation: since it’s the major component of the ranking algorithm, nobody cites to reference ideas anymore, just to try and keep their colleagues afloat. The researchers who still care about the state of science publish a parallel list of citations for those who still care to read them, but most just ignore them — the past is irrelevant anyway, the only way to stay afloat is hunting hype. You know this is distorting the literature base, feeding the algorithm junk data that will steer the research recommendations off course, but you don’t want to see your colleague down the hall fired <a class="citation" href="#brembsAlgorithmicEmploymentDecisions2021">[14]</a>. Their rankings have been sinking lately.</p>

<p>Uploading preprints is expensive now too, and they charge by the version, so you make sure you’ve checked every letter before sending it off. It’s a really compelling bit of science, some of that old style science, fundamental mechanisms, basic research kind of stuff. You check your social media metrics to perfectly time your posts about it, click send, and wait. Your friends reply with their congratulations, glad you managed to pull it off, but there’s not really a lot that can be made a meme of, and it’s not inflammatory enough to bait a sea of hot takes. You watch your Altmetric idle and sigh. You won’t get a rankings boost, but at least it looks like you’re safe from sinking for awhile.</p>

<p>You’re going to take a few weeks off before starting the multi-year process of publication. Few researchers are willing to review for free anymore, everyone is sick of publisher profiteering, but we didn’t manage to build an alternative in time, and now it’s too dangerous to try. Triage at the top of the journal prestige hierarchy is ruthless. Most submissions not pre-coordinated with the editor are pre-desk rejected after failing any one of the dozen or so benchmarks for “quality” and trendiness crunched by their black box algorithms. Instead we ping-pong papers down the hierarchy, paying submission fees all along the way. Don’t worry, there’s always some journal that will take any work — they want the publication fees in any case. If you’re cynically playing the metrics game, you can rely on the class of blatantly sacrificial junk journals that can be hastily folded up when some unpaid PubPeer blogger manages to summon enough outrage on social media. We haven’t managed to fix the problems with peer review that favor in-crowd, clickbait-friendly, though not necessarily reproducible, research. It turned out to have been a feature, not a bug for their profit model all along.</p>

<p>You’re not sure if you’ve made a contribution to the field, there isn’t any sense of cumulative consensus on basic problems. People study things that are similar to you, lots of them, and you talk. You forget what they’ve been doing sometimes, though, and you catch what you can. You like your work, and even find value in it. You can forget about the rest when you do it. And you like your lab. The system isn’t perfect but everyone knows that. Some good science still gets done, you see it all the time from the people you respect. It’s a lot of work to keep track of, at least without the subscription. But you managed to make it through another round. That feels ok for now. And it’s not your job, your job is to do science.</p>

<p>The attention span of your discipline has gotten shorter and shorter, twisting in concentric hype cycles, the new <em>rota fortuna.</em> It’s good business, keeping research programs moving helps the other end of the recommendation system. It started with advertising that looked like research <a class="citation" href="#elsevier360AdvertisingSolutions">[161]</a>, but the ability to sell influence over the course of basic science turned out to be particularly lucrative. Just little nudges here and there, you know, just supply responding to demand. They turn a blind eye to the botnets hired to manipulate trending research topics by simulating waves of clicks and scrolls. More clicks, more ads, the market speaks, everybody wins.</p>

<p>The publishers are just one piece of the interlocking swarm of the information economy. The publishers sell their data to all the others, and buy whatever they need to complete their profiles. They move in lockstep: profit together, lobby together. The US Supreme Court is expected to legalize copyrighting facts soon, opening up new markets for renting licenses to research by topic area. No one really notices intellectual property expansions anymore. There are more papers than ever, but the science is all “fake news.” Nobody reads it anyway.</p>

<h3 id="what-we-could-build">What we could build</h3>

<p>You’re a researcher with dead-center median funding at an institute with dead-center median prestige, and you have a new idea.</p>

<p>You are federated with a few organizations in your subdiscipline that have agreed to share their full namespaces, as well as a broader, public multidisciplinary indexing federation that organizes metadata more coarsely. You navigate to a few nodes in the public index that track work from some related research questions. You’re able to find a number of forum conversations, blog posts, and notebooks in the intersection between the question nodes, but none that are exactly what you’re thinking about. There’s no such thing as paywalls anymore, but some of the researchers have requested to be credited on view, so you accept the prompts that make a <code class="language-plaintext highlighter-rouge">read</code> link between you and their work. You can tell relatively quickly that there is affirmatively a gap in understanding here, rather than needing to spend weeks reading to rule it out by process of elimination — you’re on to something.</p>

<p>You request access to some of the private sections of federations that claim to have data related to the question nodes. They have some writing, data, and code public, but the data you’re after is very raw and was never written up — just left with a reference to a topic in case someone else wanted to use it later. Most accept you since they can see your affiliation in good standing with people and federations they know and trust. Others are a little more cagey, asking that you request again when you have a more developed project rather than just looking around so they can direct your permissions more finely, or else not responding at all. The price of privacy, autonomy, and consent: we might grumble about it sometimes, but all things considered are glad to pay it.</p>

<p>Your home federations have a few different names for things than those you’ve joined, so you spend a few hours making some new mappings between your communities, and send them along with some terms they don’t have but you think might be useful for them and post them to their link proposals inbox. They each have their own governance process to approve the links and associate them with their namespace, but in the meantime they exist on yours so you use them to start gathering and linking data from a few different disciplines to answer some preliminary questions you have. In the course of feeling out a project, you’ve made some new connections between communities, concepts, and formats, and made incremental improvements on knowledge organization in multiple fields. You’re rehosting some of their data as a gesture of good faith, because you’re using it and it’s become part of your project, (and because a few of the federations have ratio requirements).</p>

<p>You do some preliminary analysis to refine your hypotheses and direct the experimental design. You are able to find some analysis code from your new colleagues in a notebook linked to the data of theirs that you’re using. It doesn’t do <em>exactly</em> what you want, but you’re able to extend it to do a variation on the analysis and link it from their code in case anyone else wants to do something similar.</p>

<p>You post a notebook of some preliminary results from your secondary analysis and a brief description of your idea and experimental plan in a thread that is transcluded between the forums of the now several federations involved in your project. There’s little reason to fear being scooped: since you’re in public conversation with a lot of the people in the relevant research areas, and have been linking your work to the concepts and groups that any competitor also would have to, it doesn’t really make sense to try and rush out a result faster than you to take credit for your ideas. All the provenance of your conversations and analyses is already public, and so if someone did try and take credit for your idea, you would be able to link to their work with some “uncredited derivation” link.</p>

<p>In the thread, several people from another discipline point out that they have already done some of what you planned to do, so you link to their post to give them credit for pointing you in the right direction and transclude the relevant work in your project. Others spitball some ideas for refinements to the experiment, and try out alternate analysis strategies on your preliminary results. It’s interesting and useful, you hadn’t thought about it that way. They give you access to some of their nonpublic datasets that they never had a chance to write up. It’ll be useful in combination with your experimental results, and in the process you’ll be helping them analyze and interpret their unused data.</p>

<p>You’re ready to start your experiment. They say an hour in the library is worth a thousand at the bench, and your preliminary work has let you skip about a third of what you had initially planned to do. The project gives credit and attribution to the many people whose work you are building on and who have helped you so far, and has been made richer from the discussion and half dozen alternative analyses proposed and linked from your thread.</p>

<p>Some of the techniques and instruments are new to you, but you’re able to piece together how they work by surfing between the quasi-continuous wikis shared between federations. Hardware still costs money, but since most people able to make do with less specialized scientific instruments because of the wealth of DIY instrument documentation, and scientists are able to maintain grant funded nonprofit instrument fabrication organizations because their work is appropriately credited by the work that uses them, it’s a lot less expensive. You try out some parameter sets and experiment scripts in your experimental software linked by some technical developers in the other fields. You get to skip a lot of the fine tuning by making use of the contextual knowledge: less dead ends on the wrong equipment, not having to rediscover the subtleties of how the parameters interact, knowing that the animals do the experiment better if the second phase is delayed by a second or two more than you’d usually think. Your experimental software lets you automatically return the favor, linking your new parameters and experimental scripts as extensions of the prior work.</p>

<p>While you were planning and discussing your experiment you had been contributing your lab’s computing hardware to a computational co-op so other people could deploy analyses on it while it was idle. Now you have some credit stored up and distribute the chunks of your analysis across the network. It takes a little bit of tweaking to get some of the more resource-intensive analysis steps to work on the available machines. You don’t have time to organize a full pull request to the main analysis code, but if someone wants to do something similar they’ll be able to find your version since it’s linked to the main library as well as the rest of your project.</p>

<p>You combine the various intermediary results you have posted and been discussing in the forums into a more formal piece of writing. You need to engage with the legacy scientific literature for context, so you highlight the segments you need and make direct reference to and transclude the arguments that they are making in your piece. While you’re writing you annotate inline how your work <code class="language-plaintext highlighter-rouge">[[extends::@oldWork]]</code> because it <code class="language-plaintext highlighter-rouge">[[hasPerspective::@newDiscipline]]</code>. Some of your results <code class="language-plaintext highlighter-rouge">[[contradict::@oldWork:a-claim]]</code> and so the people who have published work affirming it are notified and invited to comment.</p>

<p>There isn’t any need for explicit peer review to confirm your work as “real science” or not. The social production of science is very visible already, and the smaller pieces you have been discussing publicly are densely contextualized by affirmative and skeptical voices from the several disciplines you were engaging with. You have <code class="language-plaintext highlighter-rouge">@public</code> annotations enabled on my writing, so anyone reading my work is able to see the inbound links from others highlighting and commenting on it. Submitting in smaller pieces with continual feedback has let you steer your work in more useful directions than your initial experimental plan, so you’ve already been in contact with many of the people who would otherwise have been your biggest skeptics and partially addressed their concerns. People are used to assessing the social context of a work: the interfaces make it visually obvious that work that has few annotations, a narrow link tree, or has a really restricted circle of people able to annotate it has relatively less support. When a previously well-supported set of ideas is called into question by new methods or measurements, it’s straightforward to explore how its contextual understanding has changed over time.</p>

<p>It’s rare for people to submit massive singular works with little public engagement beforehand. There isn’t a lot of reward for minimal authorship because the notion of “authorship” has been dissolved in favor of fluid and continuous credit assignment — engaging with little prior work and making few contributions to the data and tooling where it would have been obvious to do so is generally seen as antisocial. They are in the unenviable position of having sunk several years of work into a flawed experimental design that many people in the community could have warned about and helped with, but now since the criticisms are annotated on their work they likely will have to do yet more work if they can’t be adequately addressed or dismissed. We don’t miss the old system of peer review.</p>

<p>It’s clear that you have made a contribution to not only your field, but several that you collaborated with. Your project is a lot more than a single PDF: you can see (and be credited for) the links between data formats, communities, forum posts, notebooks, analytical tools, theories, etc. that you created. It’s clear how your work relates to and extends prior work because you were engaging with the structure of scientific research throughout. Your work implies further open questions in the open spaces in the concept graphs of several different research communities, and can organize future experiments without the need for explicit coordination.</p>

<p>There are a dozen or so metrics that are used to evaluate research and researchers. None of them are exactly neutral, and there is ongoing debate about the meaning and use of each since there are so many modalities of credit in a given person’s graph. There isn’t such a thing as a <em>proprietary</em> metric though, because no company has a monopoly on proprietary information that they could say makes it unique, and why would you trust a random number given by a company when there are plenty of ways to measure the public credit graphs? It’s relatively hard to game the system, there aren’t any proprietary algorithms to fool, and trust is a social process based on mutual affiliation instead of a filter bubble.</p>

<p>The public inspectability of scientific results, the lowered barriers to scientific communication, and ability to find research and researchers without specialized training has dramatically changed between science and the public at large. It’s straightforward to find a community of scientists for a given topic and ask questions in the public community forums. Scientific communication resembles the modes of communication most people are familiar with, and have shed some of the stilted formality that made it impenetrable. There isn’t such a firm boundary between ‘scientist’ and ‘nonscientist’ because anyone can make use of public data and community clusters to make arguments on the same forums and feeds that the scientists do with the same mechanism of credit assignment.</p>

<p>Scientists, building new systems of communication and tooling and then seeding them with their communities has provided alternatives to some of the platforms that dominated the earlier web. The scientists were able to use some of their labor and funding to overcome the development problems of prior alternatives, so they are just as easy to use as (and much more fun than) platforms like Twitter and Facebook. Their well-documented and easily deployed experimental hardware and software has empowered a new generation of DIY enthusiasts, making it possible for many people to build low-cost networked electronics to avoid the surveillance of the ad-based “Internet of Things,” air quality sensors, medical devices, wireless meshnets, and so on. The scientists helped make controlling and using personal data much more accessible and fluid. We now control our own medical data and selectively share it as-needed with healthcare providers. Mass genetics databases collected by companies like 23andme and abused by law enforcement slowly fall out of date because we can do anything the geneticists can do.</p>

<p>By taking seriously the obligation conferred by their stewardship of the human knowledge project, the scientists rebuilt their infrastructure to serve the public good instead of the companies that parasitize it. In the process they did their part ending some of the worst harms of the era of global information oligopoly.</p>

<p>Most things aren’t completely automatic or infinite, but you don’t want them to be. It’s nice to negotiate with your federations and communities, it makes you feel like a person instead of a product. Being in a silent room where algorithms shimmer data as a dark wind friction-free through the clouds sounds lonely. Now we are the winds and clouds and the birds that gossip between them, and all the chatter reminds us that we forgot what we were taught to want. You take the hiccups and errors and dead links as the work of the world we built together.</p>

<p>Everything is a little rough, a little gestural, and all very human.</p>

<h1 id="references">References</h1>

<ol class="bibliography"><li><span id="bowkerInformationInfrastructureStudies2010">1. Bowker GC, Baker K, Millerand F, Ribes D (2010) Toward Information Infrastructure Studies: Ways of Knowing in a Networked Environment. <i>International Handbook of Internet Research</i>, :97–117. https://doi.org/10.1007/978-1-4020-9789-8_5</span></li>
<li><span id="tilsonDigitalInfrastructuresMissing2010">2. Tilson D, Lyytinen K, Sørensen C (2010) Digital Infrastructures: The Missing IS Research Agenda. <i>Information Systems Research</i>, 21(4):748–759. https://doi.org/10.1287/isre.1100.0318</span></li>
<li><span id="starStepsEcologyInfrastructure1996">3. Star SL, Ruhleder K (1996) Steps Toward an Ecology of Infrastructure: Design and Access for Large Information Spaces. <i>Information Systems Research</i>, 7(1):111–134. https://doi.org/10.1287/isre.7.1.111</span></li>
<li><span id="michicancivilrightscommissionFlintWaterCrisis2017">4. Commission MCR (2017) The Flint Water Crisis: Systemic Racism Through the Lens of Flint. https://web.archive.org/web/20210518020755/https://www.michigan.gov/documents/mdcr/VFlintCrisisRep-F-Edited3-13-17_554317_7.pdf</span></li>
<li><span id="mirowskiFutureOpenScience2018">5. Mirowski P (2018) The Future(s) of Open Science. <i>Social Studies of Science</i>, 48(2):171–203. https://doi.org/10.1177/0306312718772086</span></li>
<li><span id="pooleySurveillancePublishing2021">6. Pooley J (2021) Surveillance Publishing. https://doi.org/10.31235/osf.io/j6ung</span></li>
<li><span id="zuboffBigOtherSurveillance2015">7. Zuboff S (2015) Big Other: Surveillance Capitalism and the Prospects of an Information Civilization. <i>Journal of Information Technology</i>, 30(1):75–89. https://doi.org/10.1057/jit.2015.5</span></li>
<li><span id="warkCapitalDeadThis2021">8. Wark MK (2021) Capital Is Dead: Is This Something Worse? :208. </span></li>
<li><span id="robertsBuildingGenBankPublished2001">9. Roberts RJ, Varmus HE, Ashburner M, Brown PO, Eisen MB, Khosla C, Kirschner M, Nusse R, Scott M, Wold B (2001) Building A "GenBank" of the Published Literature. <i>Science</i>, 291(5512):2318–2319. https://doi.org/10.1126/science.1060273</span></li>
<li><span id="varmusArtPoliticsScience2009">10. Varmus H (2009) The Art and Politics of Science. http://www.ncbi.nlm.nih.gov/books/NBK190622/</span></li>
<li><span id="klingRealStakesVirtual2004">11. Kling R, Spector LB, Fortuna J (2004) The Real Stakes of Virtual Publishing: The Transformation of E-Biomed into PubMed Central. <i>Journal of the American Society for Information Science and Technology</i>, 55(2):127–148. https://doi.org/10.1002/asi.10352</span></li>
<li><span id="markovitzBiomedicineElectronicPublishing2000">12. Markovitz BP (2000) Biomedicine’s Electronic Publishing Paradigm Shift. <i>Journal of the American Medical Informatics Association : JAMIA</i>, 7(3):222–229. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC61424/</span></li>
<li><span id="franceschi-bicchieraiAcademicJournalClaims2022">13. Franceschi-Bicchierai L (2022) Academic Journal Claims It Fingerprints PDFs for ‘Ransomware,’ Not Surveillance. <i>Vice</i>, https://www.vice.com/en/article/4aw48g/academic-journal-claims-it-fingerprints-pdfs-for-ransomware-not-surveillance</span></li>
<li><span id="brembsAlgorithmicEmploymentDecisions2021">14. Brembs B (2021) Algorithmic Employment Decisions in Academia? <i>bjoern.brembs.blog</i>, http://bjoern.brembs.net/2021/09/algorithmic-employment-decisions-in-academia/</span></li>
<li><span id="appleWatchOSDeliversNew2022">15. Apple (2022) watchOS 9 Delivers New Ways to Stay Connected, Active, and Healthy. <i>Apple Newsroom</i>, https://www.apple.com/newsroom/2022/06/watchos-9-delivers-new-ways-to-stay-connected-active-and-healthy/</span></li>
<li><span id="douressProfessionalMatchingService2007">16. Douress J, Previte P, Butchko J, Kennedy B, Stagg C, Grippo F, Lee E (2007) Professional Matching Service. (EP1825404A2)https://patents.google.com/patent/EP1825404A2/en?q=pharmaceutical&amp;assignee=Elsevier&amp;page=1</span></li>
<li><span id="biddleICESearchedLexisNexis2022">17. Biddle S (2022) ICE Searched LexisNexis Database Over 1 Million Times in Just Seven Months. <i>The Intercept</i>, https://theintercept.com/2022/06/09/ice-lexisnexis-mass-surveillances/</span></li>
<li><span id="biddleLexisNexisProvideGiant2021">18. Biddle S (2021) LexisNexis to Provide Giant Database of Personal Information to ICE. <i>The Intercept</i>, https://theintercept.com/2021/04/02/ice-database-surveillance-lexisnexis/</span></li>
<li><span id="lamdanDefundPoliceDefund2020">19. Lamdan S (2020) Defund the Police, and Defund Big Data Policing, Too. <i>Jurist</i>, https://www.jurist.org/commentary/2020/06/sarah-lamdan-data-policing/</span></li>
<li><span id="lamdanLibrarianshipCrossroadsICE2019">20. Lamdan S (2019) Librarianship at the Crossroads of ICE Surveillance. <i>In the Library with the Lead Pipe</i>, https://www.inthelibrarywiththeleadpipe.org/2019/ice-surveillance/</span></li>
<li><span id="westDataCapitalismRedefining2019">21. West SM (2019) Data Capitalism: Redefining the Logics of Surveillance and Privacy. <i>Business &amp; Society</i>, 58(1):20–41. https://doi.org/10.1177/0007650317718185</span></li>
<li><span id="warkHackerManifesto2004">22. Wark MK (2004) A Hacker Manifesto. https://www.hup.harvard.edu/catalog.php?isbn=9780674015432</span></li>
<li><span id="berners-leeSociallyAwareCloud2009">23. Berners-Lee T (2009) Socially Aware Cloud Storage - Design Issues. <i>Socially Aware Cloud Storage</i>, https://www.w3.org/DesignIssues/CloudStorage.html</span></li>
<li><span id="berners-leeWebServicesOverview2009">24. Berners-Lee T (2009) Web Services Overview - Design Issues. https://www.w3.org/DesignIssues/WebServices.html</span></li>
<li><span id="brembsReplacingAcademicJournals2021">25. Brembs B, Huneman P, Schönbrodt F, Nilsonne G, Susi T, Siems R, Perakakis P, Trachana V, Ma L, Rodriguez-Cuadrado S (2021) Replacing Academic Journals. https://doi.org/10.5281/zenodo.5526635</span></li>
<li><span id="ponziSciencePyramidScheme2020">26. Ponzi C (2020) Is Science a Pyramid Scheme? The Correlation between an Author’s Position in the Academic Hierarchy and Her Scientific Output per Year. https://doi.org/10.31235/osf.io/c3xg5</span></li>
<li><span id="bietzSustainingDevelopmentCyberinfrastructure2012">27. Bietz MJ, Ferro T, Lee CP (2012) Sustaining the Development of Cyberinfrastructure: An Organization Adapting to Change. <i>Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work</i>, :901–910. https://doi.org/10.1145/2145204.2145339</span></li>
<li><span id="woolKnowledgeNetworksHow2020">28. Wool LE, Laboratory TIB (2020) Knowledge across Networks: How to Build a Global Neuroscience Collaboration. https://doi.org/10.1016/j.conb.2020.10.020</span></li>
<li><span id="barleyBackroomsScienceWork1994">29. BARLEY STEPHENR, BECHKY BETHA (1994) In the Backrooms of Science: The Work of Technicians in Science Labs. <i>Work and Occupations</i>, 21(1):85–126. https://doi.org/10.1177/0730888494021001004</span></li>
<li><span id="howisonIncentivesIntegrationScientific2013">30. Howison J, Herbsleb JD (2013) Incentives and Integration in Scientific Software Production. <i>Proceedings of the 2013 Conference on Computer Supported Cooperative Work</i>, :459–470. https://doi.org/10.1145/2441776.2441828</span></li>
<li><span id="mathisDeepLabCutMarkerlessPose2018">31. Mathis A, Mamidanna P, Cury KM, Abe T, Murthy VN, Mathis MW, Bethge M (2018) DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning. <i>Nature Neuroscience</i>, 21(9):1281–1289. https://doi.org/10.1038/s41593-018-0209-y</span></li>
<li><span id="carverSurveyStatePractice2022">32. Carver JC, Weber N, Ram K, Gesing S, Katz DS (2022) A Survey of the State of the Practice for Research Software in the United States. <i>PeerJ Computer Science</i>, 8:e963. https://doi.org/10.7717/peerj-cs.963</span></li>
<li><span id="mangulImprovingUsabilityArchival2019">33. Mangul S, Martin LS, Eskin E, Blekhman R (2019) Improving the Usability and Archival Stability of Bioinformatics Software. <i>Genome Biology</i>, 20(1):47. https://doi.org/10.1186/s13059-019-1649-8</span></li>
<li><span id="kumarBioinformaticsSoftwareBiologists2007">34. Kumar S, Dudley J (2007) Bioinformatics Software for Biologists in the Genomics Era. <i>Bioinformatics</i>, 23(14):1713–1717. https://doi.org/10.1093/bioinformatics/btm239</span></li>
<li><span id="howisonSoftwareScientificLiterature2016">35. Howison J, Bullard J (2016) Software in the Scientific Literature: Problems with Seeing, Finding, and Using Software Mentioned in the Biology Literature. <i>Journal of the Association for Information Science and Technology</i>, 67(9):2137–2155. https://doi.org/10.1002/asi.23538</span></li>
<li><span id="NIHStrategicPlan2018">36. (2018) NIH Strategic Plan for Data Science. https://web.archive.org/web/20210907014444/https://datascience.nih.gov/sites/default/files/NIH_Strategic_Plan_for_Data_Science_Final_508.pdf</span></li>
<li><span id="ribesLongNowTechnology2009">37. Ribes D, Finholt T (2009) The Long Now of Technology Infrastructure: Articulating Tensions in Development. <i>Journal of the Association for Information Systems</i>, 10(5):375–398. https://doi.org/10.17705/1jais.00199</span></li>
<li><span id="altschulAnatomySuccessfulComputational2013">38. Altschul S, Demchak B, Durbin R, Gentleman R, Krzywinski M, Li H, Nekrutenko A, Robinson J, Rasband W, Taylor J, Trapnell C (2013) The Anatomy of Successful Computational Biology Software. <i>Nature Biotechnology</i>, 31(10):894–897. https://doi.org/10.1038/nbt.2721</span></li>
<li><span id="palmerDitchingSemanticWeb2008">39. Palmer SB (2008) Ditching the Semantic Web? http://inamidst.com/whits/2008/ditching</span></li>
<li><span id="poirierTurnScruffyEthnographic2017">40. Poirier L (2017) A Turn for the Scruffy: An Ethnographic Study of Semantic Web Architecture. <i>Proceedings of the 2017 ACM on Web Science Conference</i>, :359–367. https://doi.org/10.1145/3091478.3091505</span></li>
<li><span id="hiltzikTamingWildWild2001">41. Hiltzik MA (2001) Taming the Wild, Wild Web. <i>Los Angeles Times</i>, https://web.archive.org/web/20010801142640/https://www.latimes.com/business/la-072601netarch.story</span></li>
<li><span id="larsenPoliticalNatureTCP2012">42. Larsen R (2012) The Political Nature of TCP/IP. :56. </span></li>
<li><span id="clarkCloudyCrystalBall1992">43. Clark D (1992) A Cloudy Crystal Ball - Visions of the Future. <i>Proceedings of the Twenty-Fourth Internet Engineering Task Force</i>, :539–543. https://www.ietf.org/proceedings/24.pdf</span></li>
<li><span id="swartzAaronSwartzProgrammable2013">44. Swartz A (2013) Aaron Swartz’s A Programmable Web: An Unfinished Work. <i>Synthesis Lectures on the Semantic Web: Theory and Technology</i>, 3(2):1–64. https://doi.org/10.2200/S00481ED1V01Y201302WBE005</span></li>
<li><span id="mcilroyUNIXTimeSharingSystem1978">45. McIlroy MD, Pinson EN, Tague BA (1978) UNIX Time-Sharing System: Forward. <i>The Bell System Technical Journal</i>, 57(6)http://archive.org/details/bstj57-6-1899</span></li>
<li><span id="ElsevierSevenBridges2017">46. (2017) Elsevier and Seven Bridges Receive NIH Data Commons Grant for Biomedical Data Analysis. https://www.elsevier.com/about/press-releases/archive/science-and-technology/elsevier-and-seven-bridges-receive-nih-data-commons-grant-for-biomedical-data-analysis</span></li>
<li><span id="macinnesCompatibilityStandardsMonopoly2005">47. MacInnes I (2005) Compatibility Standards and Monopoly Incentives: The Impact of Service-Based Software Licensing. <i>International Journal of Services and Standards</i>, 1(3):255–270. https://doi.org/10.1504/IJSS.2005.005799</span></li>
<li><span id="RELXAnnualReport2020">48. (2020) RELX Annual Report 2020. :196. https://www.relx.com/ /media/Files/R/RELX-Group/documents/reports/annual-reports/2020-annual-report.pdf</span></li>
<li><span id="CriticismAmazon2021">49. (2021) Criticism of Amazon. <i>Wikipedia</i>, https://en.wikipedia.org/w/index.php?title=Criticism_of_Amazon&amp;oldid=1043543093</span></li>
<li><span id="elsevierTopicProminenceSciencea">50. Elsevier Topic Prominence in Science - Scival | Elsevier Solutions. <i>Elsevier.com</i>, https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science</span></li>
<li><span id="buranyiStaggeringlyProfitableBusiness2017">51. Buranyi S (2017) Is the Staggeringly Profitable Business of Scientific Publishing Bad for Science? <i>The Guardian</i>, https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science</span></li>
<li><span id="reillyNIHSTRIDESInitiative2021">52. Reilly RT (2021) NIH STRIDES Initiative. https://web.archive.org/web/20211006011408/https://ncihub.org/resources/2422/download/21.01.08_NIH_STRIDES_Presentation.pptx</span></li>
<li><span id="STRIDESInitiativeSuccess2020">53. (2020) STRIDES Initiative Success Story: University of Michigan TOPMed | Data Science at NIH. https://web.archive.org/web/20210324024612/https://datascience.nih.gov/strides-initiative-success-story-university-michigan-topmed</span></li>
<li><span id="leinGenomewideAtlasGene2007">54. Lein ES, Hawrylycz MJ, Ao N, Ayres M, Bensinger A, Bernard A, Boe AF, Boguski MS, Brockway KS, Byrnes EJ, Chen L, Chen L, Chen T-M, Chi Chin M, Chong J, Crook BE, Czaplinska A, Dang CN, Datta S, Dee NR, Desaki AL, Desta T, Diep E, Dolbeare TA, Donelan MJ, Dong H-W, Dougherty JG, Duncan BJ, Ebbert AJ, Eichele G, Estin LK, Faber C, Facer BA, Fields R, Fischer SR, Fliss TP, Frensley C, Gates SN, Glattfelder KJ, Halverson KR, Hart MR, Hohmann JG, Howell MP, Jeung DP, Johnson RA, Karr PT, Kawal R, Kidney JM, Knapik RH, Kuan CL, Lake JH, Laramee AR, Larsen KD, Lau C, Lemon TA, Liang AJ, Liu Y, Luong LT, Michaels J, Morgan JJ, Morgan RJ, Mortrud MT, Mosqueda NF, Ng LL, Ng R, Orta GJ, Overly CC, Pak TH, Parry SE, Pathak SD, Pearson OC, Puchalski RB, Riley ZL, Rockett HR, Rowland SA, Royall JJ, Ruiz MJ, Sarno NR, Schaffnit K, Shapovalova NV, Sivisay T, Slaughterbeck CR, Smith SC, Smith KA, Smith BI, Sodt AJ, Stewart NN, Stumpf K-R, Sunkin SM, Sutram M, Tam A, Teemer CD, Thaller C, Thompson CL, Varnam LR, Visel A, Whitlock RM, Wohnoutka PE, Wolkey CK, Wong VY, Wood M, Yaylaoglu MB, Young RC, Youngstrom BL, Feng Yuan X, Zhang B, Zwingman TA, Jones AR (2007) Genome-Wide Atlas of Gene Expression in the Adult Mouse Brain. <i>Nature</i>, 445(7124):168–176. https://doi.org/10.1038/nature05453</span></li>
<li><span id="grillnerWorldwideInitiativesAdvance2016">55. Grillner S, Ip N, Koch C, Koroshetz W, Okano H, Polachek M, Poo M-ming, Sejnowski TJ (2016) Worldwide Initiatives to Advance Brain Research. <i>Nature Neuroscience</i>, 19(9):1118–1122. https://doi.org/10.1038/nn.4371</span></li>
<li><span id="kochBigScienceTeam2016">56. Koch C, Jones A (2016) Big Science, Team Science, and Open Science for Neuroscience. <i>Neuron</i>, 92(3):612–616. https://doi.org/10.1016/j.neuron.2016.10.019</span></li>
<li><span id="mainenBetterWayCrack2016">57. Mainen ZF, Häusser M, Pouget A (2016) A Better Way to Crack the Brain. <i>Nature News</i>, 539(7628):159. https://doi.org/10.1038/539159a</span></li>
<li><span id="abbottInternationalLaboratorySystems2017">58. Abbott LF, Angelaki DE, Carandini M, Churchland AK, Dan Y, Dayan P, Deneve S, Fiete I, Ganguli S, Harris KD, Häusser M, Hofer S, Latham PE, Mainen ZF, Mrsic-Flogel T, Paninski L, Pillow JW, Pouget A, Svoboda K, Witten IB, Zador AM (2017) An International Laboratory for Systems and Computational Neuroscience. <i>Neuron</i>, 96(6):1213–1218. https://doi.org/10.1016/j.neuron.2017.12.013</span></li>
<li><span id="laboratoryStandardizedReproducibleMeasurement2020">59. Laboratory TIB, Aguillon-Rodriguez V, Angelaki DE, Bayer HM, Bonacchi N, Carandini M, Cazettes F, Chapuis GA, Churchland AK, Dan Y, Dewitt EEJ, Faulkner M, Forrest H, Haetzel LM, Hausser M, Hofer SB, Hu F, Khanal A, Krasniak CS, Laranjeira I, Mainen ZF, Meijer GT, Miska NJ, Mrsic-Flogel TD, Murakami M, Noel J-P, Pan-Vazquez A, Rossant C, Sanders JI, Socha KZ, Terry R, Urai AE, Vergara HM, Wells MJ, Wilson CJ, Witten IB, Wool LE, Zador A (2020) Standardized and Reproducible Measurement of Decision-Making in Mice. <i>bioRxiv</i>, :2020.01.17.909838. https://doi.org/10.1101/2020.01.17.909838</span></li>
<li><span id="laboratoryDataArchitectureLargescale2020">60. Laboratory TIB, Bonacchi N, Chapuis G, Churchland A, Harris KD, Hunter M, Rossant C, Sasaki M, Shen S, Steinmetz NA, Walker EY, Winter O, Wells M (2020) Data Architecture for a Large-Scale Neuroscience Collaboration. <i>bioRxiv</i>, :827873. https://doi.org/10.1101/827873</span></li>
<li><span id="lopesBonsaiEventbasedFramework2015">61. Lopes G, Bonacchi N, Frazão J, Neto JP, Atallah BV, Soares S, Moreira L, Matias S, Itskov PM, Correia PA, Medina RE, Calcaterra L, Dreosti E, Paton JJ, Kampff AR (2015) Bonsai: An Event-Based Framework for Processing and Controlling Data Streams. <i>Frontiers in Neuroinformatics</i>, 9https://doi.org/10.3389/fninf.2015.00007</span></li>
<li><span id="balkanSmallTechnologyFoundation">62. Balkan A, Kalbag L Small Technology Foundation - About. <i>Small Technology Foundation</i>, https://small-tech.org/about/</span></li>
<li><span id="kaplanPartAntisoftwareAction2020">63. Kaplan J, Bayer C (2020) Part 1: Antisoftware Action. <i>anti software software club</i>, https://antisoftware.club/manifesto/2020/03/16/part-1.html</span></li>
<li><span id="sambraSolidPlatformDecentralized2016">64. Sambra AV, Mansour E, Hawke S, Zereba M, Greco N, Ghanem A, Zagidulin D, Aboulnaga A, Berners-Lee T (2016) Solid: A Platform for Decentralized Social Applications Based on Linked Data. <i>MIT CSAIL &amp; Qatar Computing Research Institute, Tech. Rep.</i>, :16. </span></li>
<li><span id="Rfc5321SimpleMail">65. Rfc5321 - Simple Mail Transfer Protocol. https://datatracker.ietf.org/doc/html/rfc5321</span></li>
<li><span id="lemmer-webberStandardsDivisionsCollaboration2018">66. Lemmer-Webber C (2018) On Standards Divisions and Collaboration (or: Why Can’t the Decentralized Social Web People Just Get Along?). <i>Dustycloud Brainstorms</i>, http://dustycloud.org/blog/on-standards-divisions-collaboration/</span></li>
<li><span id="rosenblattDigitalObjectIdentifier1997">67. Rosenblatt B (1997) The Digital Object Identifier: Solving the Dilemma of Copyright Protection Online. <i>Journal of Electronic Publishing</i>, 3(2)https://doi.org/10.3998/3336451.0003.204</span></li>
<li><span id="crossrefFormationCrossRefShort2009">68. CrossRef (2009) The Formation of CrossRef: A Short History. https://www.crossref.org/pdfs/CrossRef10Years.pdf</span></li>
<li><span id="clarkDesignPhilosophyDARPA1988">69. Clark D (1988) The Design Philosophy of the DARPA Internet Protocols. <i>Symposium Proceedings on Communications Architectures and Protocols</i>, :106–114. https://doi.org/10.1145/52324.52336</span></li>
<li><span id="berners-leePrinciplesDesign1998">70. Berners-Lee T (1998) Principles of Design. https://www.w3.org/DesignIssues/Principles.html#Decentrali</span></li>
<li><span id="schubertActivityPubFinalThoughts2019">71. Schubert D (2019) ActivityPub - Final Thoughts, One Year Later. https://overengineer.dev/blog/2019/01/13/activitypub-final-thoughts-one-year-later.html</span></li>
<li><span id="yooProtocolLayeringInternet2013">72. Yoo CS (2013) Protocol Layering and Internet Policy. <i>University of Pennsylvania Law Review</i>, 161:66. https://scholarship.law.upenn.edu/faculty_scholarship/454</span></li>
<li><span id="carpenterRFC1958Architectural1996">73. Carpenter BE (1996) RFC 1958 - Architectural Principles of the Internet. https://tools.ietf.org/html/rfc1958</span></li>
<li><span id="grudinGroupwareSocialDynamics1994">74. Grudin J (1994) Groupware and Social Dynamics: Eight Challenges for Developers. <i>Communications of the ACM</i>, 37(1):92–105. https://doi.org/10.1145/175222.175230</span></li>
<li><span id="randallDistributedOntologyBuilding2011">75. Randall D, Procter R, Lin Y, Poschen M, Sharrock W, Stevens R (2011) Distributed Ontology Building as Practical Work. <i>International Journal of Human-Computer Studies</i>, 69(4):220–233. https://doi.org/10.1016/j.ijhcs.2010.12.011</span></li>
<li><span id="markoffTomorrowWorldWide1996">76. Markoff J (1996) Tomorrow, the World Wide Web!;Microsoft, the PC King, Wants to Reign Over the Internet. <i>The New York Times</i>, https://www.nytimes.com/1996/07/16/business/tomorrow-world-wide-web-microsoft-pc-king-wants-reign-over-internet.html</span></li>
<li><span id="teamScientificDataFormats">77. Team A Scientific Data Formats - Just Solve the File Format Problem. http://justsolve.archiveteam.org/wiki/Scientific_Data_formats</span></li>
<li><span id="rubelNWBAccessibleData2019a">78. Rübel O, Tritt A, Dichter B, Braun T, Cain N, Clack N, Davidson TJ, Dougherty M, Fillion-Robin J-C, Graddis N, Grauer M, Kiggins JT, Niu L, Ozturk D, Schroeder W, Soltesz I, Sommer FT, Svoboda K, Lydia N, Frank LM, Bouchard K (2019) NWB:N 2.0: An Accessible Data Standard for Neurophysiology. <i>bioRxiv</i>, :523035. https://doi.org/10.1101/523035</span></li>
<li><span id="rubelNeurodataBordersEcosystem2021">79. Rübel O, Tritt A, Ly R, Dichter BK, Ghosh S, Niu L, Soltesz I, Svoboda K, Frank L, Bouchard KE (2021) The Neurodata Without Borders Ecosystem for Neurophysiological Data Science. :2021.03.13.435173. https://doi.org/10.1101/2021.03.13.435173</span></li>
<li><span id="cohenAcademicTorrentsCommunityMaintained2014">80. Cohen JP, Lo HZ (2014) Academic Torrents: A Community-Maintained Distributed Repository. <i>Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment</i>, :1–2. https://doi.org/10.1145/2616498.2616528</span></li>
<li><span id="loAcademicTorrentsScalable2016">81. Lo HZ, Cohen JP (2016) Academic Torrents: Scalable Data Distribution. https://doi.org/10.48550/arXiv.1603.04395</span></li>
<li><span id="langilleBioTorrentsFileSharing2010">82. Langille MGI, Eisen JA (2010) BioTorrents: A File Sharing Service for Scientific Data. <i>PLoS ONE</i>, 5(4)https://doi.org/10.1371/journal.pone.0010071</span></li>
<li><span id="himmelsteinSciHubProvidesAccess2018">83. Himmelstein DS, Romero AR, Levernier JG, Munro TA, McLaughlin SR, Greshake Tzovaras B, Greene CS (2018) Sci-Hub Provides Access to Nearly All Scholarly Literature. <i>eLife</i>, 7https://doi.org/10.7554/eLife.32822</span></li>
<li><span id="sci-hubTorrentHealthTracker2022">84. Sci-Hub (2022) Torrent Health Tracker. https://web.archive.org/web/20220609161833/https://phillm.net/torrent-health-frontend/stats-filtered-table.php?propname%5B%5D=seeders&amp;comp%5B%5D=%3C&amp;value%5B%5D=12&amp;propname%5B%5D=type&amp;comp%5B%5D===&amp;value%5B%5D=scimag</span></li>
<li><span id="bookwarriorLibraryGenesisDecentralized2021">85. bookwarrior (2021) Library Genesis: Decentralized Library. https://web.archive.org/web/20220601201202/https://libgen.fun/dweb.html</span></li>
<li><span id="shenHandbookPeertoPeerNetworking2010">86. Shen X, Yu H, Buford J, Akon M (2010) Handbook of Peer-to-Peer Networking. </span></li>
<li><span id="cohenBitTorrentProtocolSpecification2017">87. Cohen B (2017) The BitTorrent Protocol Specification. https://www.bittorrent.org/beps/bep_0003.html</span></li>
<li><span id="vandersarPirateBayFive2011">88. Van der Sar E (2011) The Pirate Bay: Five Years After The Raid. https://torrentfreak.com/the-pirate-bay-five-years-after-the-raid-110531/</span></li>
<li><span id="roettgersPirateBayDistributing2009">89. Roettgers J (2009) The Pirate Bay: Distributing the World’s Entertainment for $3,000 a Month. <i>Gigaom</i>, https://gigaom.com/2009/07/19/the-pirate-bay-distributing-the-worlds-entertainment-for-3000-a-month/</span></li>
<li><span id="PirateBayArchiveteam2020">90. (2020) The Pirate Bay - Archiveteam. <i>Archive Team - The Pirate Bay</i>, https://wiki.archiveteam.org/index.php?title=The_Pirate_Bay&amp;oldid=45467</span></li>
<li><span id="spiesDataIntegrityLibrarians2017">91. Spies J (2017) Data Integrity for Librarians, Archivists, and Criminals: What We Can Steal from Bitcoin, BitTorrent, and Usenet. <i>CNI: Coalition for Networked Information</i>, https://www.cni.org/topics/digital-curation/data-integrity-for-librarians-archivists-and-criminals-what-we-can-steal-from-bitcoin-bittorrent-and-usenet</span></li>
<li><span id="kim15YearsPirate2019">92. Kim E (2019) After 15 Years, the Pirate Bay Still Can’t Be Killed. <i>MEL Magazine</i>, https://melmagazine.com/en-us/story/after-15-years-the-pirate-bay-still-cant-be-killed</span></li>
<li><span id="vandersarOpenBayNow2014">93. Van der Sar E (2014) The Open Bay: Now Anyone Can Run A Pirate Bay ’Copy.’ <i>TorrentFreak</i>, https://torrentfreak.com/open-bay-now-everyone-can-run-pirate-bay-copy-141219/</span></li>
<li><span id="vandersarWhatCdDead2016">94. Van der Sar E (2016) What.Cd Is Dead, But The Torrent Hydra Lives On. <i>TorrentFreak</i>, https://torrentfreak.com/what-cd-is-dead-but-the-torrent-hydra-lives-on-161202/</span></li>
<li><span id="scottGeocitiesTorrentUpdate2010">95. Scott J (2010) Geocities Torrent Update. <i>ASCII by Jason Scott</i>, http://ascii.textfiles.com/archives/2894</span></li>
<li><span id="rossiPeekingBitTorrentSeedbox2014">96. Rossi D, Pujol G, Wang X, Mathieu F (2014) Peeking through the BitTorrent Seedbox Hosting Ecosystem. <i>Traffic Monitoring and Analysis</i>, :115–126. https://doi.org/10.1007/978-3-642-54999-1_10</span></li>
<li><span id="hoffmanHTTPBasedSeedingSpecification">97. Hoffman J, DeHackEd HTTP-Based Seeding Specification. http://www.bittornado.com/docs/webseed-spec.txt</span></li>
<li><span id="kahle000000Torrents2012">98. Kahle B (2012) Over 1,000,000 Torrents of Downloadable Books, Music, and Movies. <i>Internet Archive Blogs</i>, http://blog.archive.org/2012/08/07/over-1000000-torrents-of-downloadable-books-music-and-movies/</span></li>
<li><span id="kreitzSpotifyLargeScale2010b">99. Kreitz G, Niemela F (2010) Spotify – Large Scale, Low Latency, P2P Music-on-Demand Streaming. <i>2010 IEEE Tenth International Conference on Peer-to-Peer Computing (P2P)</i>, :1–10. https://doi.org/10.1109/P2P.2010.5569963</span></li>
<li><span id="andreevBiologistsNeedModern2021">100. Andreev A, Morrell T, Briney K, Gesing S, Manor U (2021) Biologists Need Modern Data Infrastructure on Campus. <i>arXiv:2108.07631 [q-bio]</i>, http://arxiv.org/abs/2108.07631</span></li>
<li><span id="charlesCommunityDrivenBigOpen2020">101. Charles AS, Falk B, Turner N, Pereira TD, Tward D, Pedigo BD, Chung J, Burns R, Ghosh SS, Kebschull JM, Silversmith W, Vogelstein JT (2020) Toward Community-Driven Big Open Brain Science: Open Big Data and Tools for Structure, Function, and Genetics. <i>Annual Review of Neuroscience</i>, 43:441–464. https://doi.org/10.1146/annurev-neuro-100119-110036</span></li>
<li><span id="benetIPFSContentAddressed2014">102. Benet J (2014) IPFS - Content Addressed, Versioned, P2P File System. <i>arXiv:1407.3561 [cs]</i>, http://arxiv.org/abs/1407.3561</span></li>
<li><span id="ogdenDatDistributedDataset2017">103. Ogden M (2017) Dat - Distributed Dataset Synchronization And Versioning. https://doi.org/10.31219/osf.io/nsv2c</span></li>
<li><span id="patsakisHydrasIPFSDecentralised2019">104. Patsakis C, Casino F (2019) Hydras and IPFS: A Decentralised Playground for Malware. <i>International Journal of Information Security</i>, 18(6):787–799. https://doi.org/10.1007/s10207-019-00443-0</span></li>
<li><span id="cohenBEP52BitTorrent2017">105. Cohen B (2017) BEP 52: The BitTorrent Protocol Specification V2. https://www.bittorrent.org/beps/bep_0052.html</span></li>
<li><span id="zhangUnravelingBitTorrentEcosystem2011">106. Zhang C, Dhungel P, Wu D, Ross KW (2011) Unraveling the BitTorrent Ecosystem. <i>IEEE Transactions on Parallel and Distributed Systems</i>, 22(7):1164–1177. https://doi.org/10.1109/TPDS.2010.123</span></li>
<li><span id="clarkeFreenetDistributedAnonymous2001">107. Clarke I, Sandberg O, Wiley B, Hong TW (2001) Freenet: A Distributed Anonymous Information Storage and Retrieval System. <i>Designing Privacy Enhancing Technologies: International Workshop on Design Issues in Anonymity and Unobservability Berkeley, CA, USA, July 25–26, 2000 Proceedings</i>, :46–66. https://doi.org/10.1007/3-540-44702-4_4</span></li>
<li><span id="capadisliSolidProtocol2020">108. Capadisli S, Berners-Lee T, Verborgh R, Kjernsmo K, Bingham J, Zagidulin D (2020) Solid Protocol. https://solidproject.org/TR/protocol</span></li>
<li><span id="SolidP2PFoundation">109. Solid - P2P Foundation. https://wiki.p2pfoundation.net/Solid</span></li>
<li><span id="basamanowiczReleaseGroupsDigital2011">110. Basamanowicz JR (2011) Release Groups and Digital Copyright Piracy. https://doi.org/10/etd6644_JBasamanowicz.pdf</span></li>
<li><span id="hindujaDeindividuationInternetSoftware2008">111. Hinduja S (2008) Deindividuation and Internet Software Piracy. <i>CyberPsychology &amp; Behavior</i>, 11(4):391–398. https://doi.org/10.1089/cpb.2007.0048</span></li>
<li><span id="eveWarezInfrastructureAesthetics2021">112. Eve MP (2021) Warez: The Infrastructure and Aesthetics of Piracy. https://doi.org/10.53288/0339.1.00</span></li>
<li><span id="dunhamWhatCDLegacy2018">113. Dunham I (2018) What.CD: A Legacy of Sharing. https://doi.org/10.7282/T3V128F3</span></li>
<li><span id="rosenDayMusicBurned2019">114. Rosen J (2019) The Day the Music Burned. <i>The New York Times</i>, https://www.nytimes.com/2019/06/11/magazine/universal-fire-master-recordings.html</span></li>
<li><span id="sonnadEulogyWhatCd2016">115. Sonnad N (2016) A Eulogy for What.Cd, the Greatest Music Collection in the History of the World—until It Vanished. <i>Quartz</i>, https://qz.com/840661/what-cd-is-gone-a-eulogy-for-the-greatest-music-collection-in-the-world/</span></li>
<li><span id="meulpolderPublicPrivateBitTorrent">116. Meulpolder M, D’Acunto L, Capota M, Wojciechowski M, Pouwelse JA, Epema DHJ, Sips HJ Public and Private BitTorrent Communities: A Measurement Study. :5. </span></li>
<li><span id="jiaHowSurviveThrive2013">117. Jia AL, Chen X, Chu X, Pouwelse JA, Epema DHJ (2013) How to Survive and Thrive in a Private BitTorrent Community. <i>Distributed Computing and Networking</i>, :270–284. https://doi.org/10.1007/978-3-642-35668-1_19</span></li>
<li><span id="liuUnderstandingImprovingRatio2010">118. Liu Z, Dhungel P, Wu D, Zhang C, Ross KW (2010) Understanding and Improving Ratio Incentives in Private Communities. <i>2010 IEEE 30th International Conference on Distributed Computing Systems</i>, :610–621. https://doi.org/10.1109/ICDCS.2010.90</span></li>
<li><span id="kashEconomicsBitTorrentCommunities2012">119. Kash IA, Lai JK, Zhang H, Zohar A (2012) Economics of BitTorrent Communities. <i>Proceedings of the 21st International Conference on World Wide Web</i>, :221–230. https://doi.org/10.1145/2187836.2187867</span></li>
<li><span id="chenImprovingSustainabilityPrivate2011a">120. Chen X, Chu X, Li Z (2011) Improving Sustainability of Private P2P Communities. <i>2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)</i>, :1–6. https://doi.org/10.1109/ICCCN.2011.6005944</span></li>
<li><span id="fecherReputationEconomyHow2017">121. Fecher B, Friesike S, Hebing M, Linek S (2017) A Reputation Economy: How Individual Reward Considerations Trump Systemic Arguments for Open Access to Data. <i>Palgrave Communications</i>, 3(1):1–10. https://doi.org/10.1057/palcomms.2017.51</span></li>
<li><span id="brossCommunityCollaborationContribution2013">122. Bross J (2013) Community, Collaboration and Contribution: Evaluating a BitTorrent Tracker as a Digital Library. https://doi.org/10.17615/g1cw-kw06</span></li>
<li><span id="foucaultOrderThings2001">123. Foucault M (2001) The Order of Things. https://doi.org/10.4324/9780203996645</span></li>
<li><span id="berners-leeLinkedData2006">124. Berners-Lee T (2006) Linked Data. https://www.w3.org/DesignIssues/LinkedData.html</span></li>
<li><span id="joLessonsArchivesStrategies2020">125. Jo ES, Gebru T (2020) Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning. <i>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</i>, :306–316. https://doi.org/10.1145/3351095.3372829</span></li>
<li><span id="selbstFairnessAbstractionSociotechnical2019">126. Selbst AD, Boyd D, Friedler SA, Venkatasubramanian S, Vertesi J (2019) Fairness and Abstraction in Sociotechnical Systems. <i>Proceedings of the Conference on Fairness, Accountability, and Transparency</i>, :59–68. https://doi.org/10.1145/3287560.3287598</span></li>
<li><span id="gebruDatasheetsDatasets2021">127. Gebru T, Morgenstern J, Vecchione B, Vaughan JW, Wallach H, Iii HD, Crawford K (2021) Datasheets for Datasets. <i>Communications of the ACM</i>, 64(12):86–92. https://doi.org/10.1145/3458723</span></li>
<li><span id="bowkerSortingThingsOut1999">128. Bowker GC, Star SL (1999) Sorting Things Out: Classification and Its Consequences. </span></li>
<li><span id="bietzCollaborationMetagenomicsSequence2009">129. Bietz MJ, Lee CP (2009) Collaboration in Metagenomics: Sequence Databases and the Organization of Scientific Work. <i>ECSCW 2009</i>, :243–262. https://doi.org/10.1007/978-1-84882-854-4_15</span></li>
<li><span id="ceustersFoundationsRealistOntology2010">130. Ceusters W, Smith B (2010) Foundations for a Realist Ontology of Mental Disease. <i>Journal of Biomedical Semantics</i>, 1(1):10. https://doi.org/10.1186/2041-1480-1-10</span></li>
<li><span id="consortiumUniversalBiomedicalData2019">131. Consortium TBDT (2019) Toward A Universal Biomedical Data Translator. <i>Clinical and Translational Science</i>, 12(2):86–90. https://doi.org/10.1111/cts.12591</span></li>
<li><span id="bruskiewichBiolinkBiolinkmodel2021">132. Bruskiewich R, Deepak, Moxon S, Mungall C, Solbrig H, cbizon, Brush M, Shefchek K, Hannestad L, YaphetKG, Harris N, bbopjenkins, diatomsRcool, Wang P, Balhoff J, Schaper K, XIN JIWEN, Owen P, Stupp G, JervenBolleman, Badger TG, Emonet V, vdancik (2021) Biolink/Biolink-Model: 2.2.5. https://zenodo.org/record/5520104</span></li>
<li><span id="unniBiolinkModelUniversal2022">133. Unni DR, Moxon SAT, Bada M, Brush M, Bruskiewich R, Caufield JH, Clemons PA, Dancik V, Dumontier M, Fecho K, Glusman G, Hadlock JJ, Harris NL, Joshi A, Putman T, Qin G, Ramsey SA, Shefchek KA, Solbrig H, Soman K, Thessen AE, Haendel MA, Bizon C, Mungall CJ, Consortium TBDT (2022) Biolink Model: A Universal Schema for Knowledge Graphs in Clinical, Biomedical, and Translational Science. <i>Clinical and Translational Science</i>, n/a(n/a)https://doi.org/10.1111/cts.13302</span></li>
<li><span id="fechoProgressUniversalBiomedical2022">134. Fecho K, Thessen AE, Baranzini SE, Bizon C, Hadlock JJ, Huang S, Roper RT, Southall N, Ta C, Watkins PB, Williams MD, Xu H, Byrd W, Dančík V, Duby MP, Dumontier M, Glusman G, Harris NL, Hinderer EW, Hyde G, Johs A, Su AI, Qin G, Zhu Q, Consortium TBDT (2022) Progress toward a Universal Biomedical Data Translator. <i>Clinical and Translational Science</i>, n/a(n/a)https://doi.org/10.1111/cts.13301</span></li>
<li><span id="renaissancecomputinginstituterenciBiomedicalDataTranslator2022">135. Renaissance Computing Institute (RENCI) (2022) Biomedical Data Translator Platform Moves to the next Phase. https://renci.org/blog/biomedical-data-translator-platform-moves-to-the-next-phase/</span></li>
<li><span id="hailuNIHfundedProjectAims2019">136. Hailu R (2019) NIH-funded Project Aims to Build a ’Google’ for Biomedical Data. <i>STAT</i>, https://www.statnews.com/2019/07/31/nih-funded-project-aims-to-build-a-google-for-biomedical-data/</span></li>
<li><span id="renaissancecomputinginstituterenciUseCasesShow2022">137. Renaissance Computing Institute (RENCI) (2022) Use Cases Show Translator’s Potential to Expedite Clinical Research. https://renci.org/blog/use-cases-show-translators-potential-to-expedite-clinical-research/</span></li>
<li><span id="goelExplanationContainerCaseBased2021">138. Goel P, Johs AJ, Shrestha M, Weber RO (2021) Explanation Container in Case-Based Biomedical Question-Answering. :10. https://web.archive.org/web/*/https://gaia.fdi.ucm.es/events/xcbr/papers/ICCBR_2021_paper_100.pdf</span></li>
<li><span id="groteEthicsAlgorithmicDecisionmaking2020">139. Grote T, Berens P (2020) On the Ethics of Algorithmic Decision-Making in Healthcare. <i>Journal of Medical Ethics</i>, 46(3):205–211. https://doi.org/10.1136/medethics-2019-105586</span></li>
<li><span id="obermeyerDissectingRacialBias2019">140. Obermeyer Z, Powers B, Vogeli C, Mullainathan S (2019) Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations. <i>Science</i>, 366(6464):447–453. https://doi.org/10.1126/science.aax2342</span></li>
<li><span id="panchArtificialIntelligenceAlgorithmic2019">141. Panch T, Mattie H, Atun R (2019) Artificial Intelligence and Algorithmic Bias: Implications for Health Systems. <i>Journal of Global Health</i>, 9(2):020318. https://doi.org/10.7189/jogh.09.020318</span></li>
<li><span id="panchInconvenientTruthAI2019">142. Panch T, Mattie H, Celi LA (2019) The “Inconvenient Truth” about AI in Healthcare. <i>npj Digital Medicine</i>, 2(1):1–3. https://doi.org/10.1038/s41746-019-0155-4</span></li>
<li><span id="mortonROBOKOPAbstractionLayer2019">143. Morton K, Wang P, Bizon C, Cox S, Balhoff J, Kebede Y, Fecho K, Tropsha A (2019) ROBOKOP: An Abstraction Layer and User Interface for Knowledge Graphs to Support Question Answering. <i>Bioinformatics</i>, 35(24):5382–5384. https://doi.org/10.1093/bioinformatics/btz604</span></li>
<li><span id="ordishAlgorithmsMedicalDevices2019">144. Ordish J, Murfet H, Hall A (2019) Algorithms as Medical Devices. https://www.phgfoundation.org/media/74/download/algorithms-as-medical-devices.pdf?v=1</span></li>
<li><span id="el-sayedMedicalAlgorithmsNeed2021">145. El-Sayed SS Abdul (2021) Medical Algorithms Need Better Regulation. <i>Scientific American</i>, https://www.scientificamerican.com/article/the-fda-should-better-regulate-medical-algorithms/</span></li>
<li><span id="ramTransphobiaEncodedExamination2021">146. Ram A, Kronk CA, Eleazer JR, Goulet JL, Brandt CA, Wang KH (2021) Transphobia, Encoded: An Examination of Trans-Specific Terminology in SNOMED CT and ICD-10-CM. <i>Journal of the American Medical Informatics Association</i>, (ocab200)https://doi.org/10.1093/jamia/ocab200</span></li>
<li><span id="udomsinprasertLeukocyteTelomereLength2020">147. Udomsinprasert W, Chanhom N, Suvichapanich S, Wattanapokayakit S, Mahasirimongkol S, Chantratita W, Jittikoon J (2020) Leukocyte Telomere Length as a Diagnostic Biomarker for Anti-Tuberculosis Drug-Induced Liver Injury. <i>Scientific Reports</i>, 10(1):5628. https://doi.org/10.1038/s41598-020-62635-2</span></li>
<li><span id="RePORTRePORTERBiomedical2021">148. (2021) RePORT ⟩ RePORTER "Biomedical Data Translator". https://reporter.nih.gov/search/kDJ97zGUFEaIBIltUmyd_Q/projects?sort_field=FiscalYear&amp;sort_order=desc</span></li>
<li><span id="subramanianGoogleBuysFreebase2010">149. Subramanian K (2010) Google Buys Freebase - This Is Huge. <i>CloudAve</i>, https://www.cloudave.com/140/google-buys-freebase-this-is-huge/</span></li>
<li><span id="IntroducingKnowledgeGraph2012">150. (2012) Introducing the Knowledge Graph: Things, Not Strings. <i>Google</i>, https://blog.google/products/search/introducing-knowledge-graph-things-not/</span></li>
<li><span id="sullivanFAQAllNew2013">151. Sullivan D (2013) FAQ: All About The New Google "Hummingbird" Algorithm. <i>Search Engine Land</i>, https://searchengineland.com/google-hummingbird-172816</span></li>
<li><span id="bharatGeneratingUserInformation2005">152. Bharat K, Lawrence S, Sahami M (2005) Generating User Information for Use in Targeted Advertising. (US20050131762A1)https://patents.google.com/patent/US20050131762A1/en</span></li>
<li><span id="SmithFacebookInc2018">153. (2018) Smith v. Facebook, Inc., No. 17-16206 (9th Cir. Dec. 6, 2018). https://casetext.com/case/smith-v-facebook-inc-2</span></li>
<li><span id="krashinskyGoogleBrokeCanada2014">154. Krashinsky S (2014) Google Broke Canada’s Privacy Laws with Targeted Health Ads, Watchdog Says. <i>The Globe and Mail</i>, https://www.theglobeandmail.com/technology/tech-news/google-broke-canadas-privacy-laws-with-targeted-ads-regulator-says/article16343346/</span></li>
<li><span id="bourreauGoogleFitbitWill2020">155. Bourreau M, Caffarra C, Chen Z, Choe C, Crawford GS, Duso T, Genakos C, Heidhues P, Peitz M, Rønde T, Schnitzer M, Schutz N, Sovinsky M, Spagnolo G, Toivanen O, Valletti T, Vergé T (2020) Google/Fitbit Will Monetise Health Data and Harm Consumers. (107):13. </span></li>
<li><span id="translatorconsortiumClinicalDataServices2020">156. Translator Consortium (2020) Clinical Data Services Provider. https://github.com/NCATSTranslator/Translator-All/blob/1c44f9a2515d239730a070201ccc7d1083c27fed/presentations/Translator_2020_Kick-Off_Presentation-Clinical_Data_Services.pdf</span></li>
<li><span id="bridgesAmazonRingLargest2021">157. Bridges L (2021) Amazon’s Ring Is the Largest Civilian Surveillance Network the US Has Ever Seen. <i>The Guardian</i>, https://www.theguardian.com/commentisfree/2021/may/18/amazon-ring-largest-civilian-surveillance-network-us</span></li>
<li><span id="AWSAnnouncesAWS2021">158. (2021) AWS Announces AWS Healthcare Accelerator for Startups in the Public Sector. <i>Amazon Web Services</i>, https://aws.amazon.com/blogs/publicsector/aws-announces-healthcare-accelerator-program-startups-public-sector/</span></li>
<li><span id="lermanAmazonBuiltIts2021">159. Lerman R (2021) Amazon Built Its Own Health-Care Service for Employees. Now It’s Selling It to Other Companies. <i>Washington Post</i>, https://www.washingtonpost.com/technology/2021/03/17/amazon-healthcare-service-care-expansion/</span></li>
<li><span id="quinnYouCanTrust2021">160. Quinn C (2021) You Can’t Trust Amazon When It Feels Threatened. <i>Last Week in AWS</i>, https://www.lastweekinaws.com/blog/you-cant-trust-amazon-when-it-feels-threatened/</span></li>
<li><span id="elsevier360AdvertisingSolutions">161. Elsevier 360\textdegree Advertising Solutions | Advertising | Advertisers. <i>Elsevier.com</i>, https://www.elsevier.com/advertising-reprints-supplements/advertising</span></li>
<li><span id="relx2021AnnualReport2021">162. RELX (2021) 2021 Annual Report. https://www.relx.com/ /media/Files/R/RELX-Group/documents/reports/annual-reports/relx-2021-annual-report.pdf</span></li>
<li><span id="consortiumBiomedicalDataTranslator2019">163. Consortium TBDT (2019) The Biomedical Data Translator Program: Conception, Culture, and Community. <i>Clinical and Translational Science</i>, 12(2):91–94. https://doi.org/10.1111/cts.12592</span></li>
<li><span id="shammaTranslationColonialism2018">164. Shamma T (2018) Translation and Colonialism. <i>The Routledge Handbook of Translation and Culture</i>, :279–295. </span></li>
<li><span id="berners-leeSemanticWeb2001">165. Berners-Lee T, HENDLER J, Lassila O (2001) The Semantic Web. <i>Scientific American</i>, 284(5):34–43. https://www.jstor.org/stable/26059207</span></li>
<li><span id="klyneRDFConceptsAbstract2014">166. Klyne G, Wood D, Lanthaler M (2014) RDF 1.1 Concepts and Abstract Syntax. http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</span></li>
<li><span id="beckettRDFTurtle2014">167. Beckett D, Berners-Lee T, Prud’hommeaux E, Carothers G (2014) RDF 1.1 Turtle. http://www.w3.org/TR/2014/REC-turtle-20140225/</span></li>
<li><span id="w3cOntologiesW3C">168. w3c Ontologies - W3C. <i>Semantic Web - W3C</i>, https://www.w3.org/standards/semanticweb/ontology</span></li>
<li><span id="brickleyRDFSchema2014">169. Brickley D, Guha RV (2014) RDF Schema 1.1. http://www.w3.org/TR/2014/REC-rdf-schema-20140225/</span></li>
<li><span id="berners-leeRelationalDatabasesSemantic2009">170. Berners-Lee T (2009) Relational Databases and the Semantic Web (in Design Issues). https://www.w3.org/DesignIssues/RDB-RDF.html</span></li>
<li><span id="berners-leeWhatSemanticWeb1998">171. Berners-Lee T (1998) What the Semantic Web Can Represent. https://www.w3.org/DesignIssues/RDFnot.html</span></li>
<li><span id="brickleySKOSCoreGuide2005">172. Brickley D, Miles A (2005) SKOS Core Guide. https://www.w3.org/TR/swbp-skos-core-guide/</span></li>
<li><span id="milesQuickGuidePublishing2005">173. Miles A (2005) Quick Guide to Publishing a Thesaurus on the Semantic Web. https://www.w3.org/TR/swbp-thesaurus-pubguide/</span></li>
<li><span id="librariaStoningGoliath2022">174. Libraria G (2022) Stoning Goliath. <i>Gavia Library - The Library Loon</i>, https://gavialib.com/2022/06/stoning-goliath/</span></li>
<li><span id="speicherLinkedDataPlatform2015">175. Speicher S, Arwe J, Malhotra (2015) Linked Data Platform 1.0. http://www.w3.org/TR/2015/REC-ldp-20150226/</span></li>
<li><span id="heimbignerFederatedArchitectureInformation1985">176. Heimbigner D, McLeod D (1985) A Federated Architecture for Information Management. <i>ACM Transactions on Information Systems</i>, 3(3):253–278. https://doi.org/10.1145/4229.4233</span></li>
<li><span id="litwinInteroperabilityMultipleAutonomous1990">177. Litwin W, Mark L, Roussopoulos N (1990) Interoperability of Multiple Autonomous Databases. <i>ACM Computing Surveys</i>, 22(3):267–293. https://doi.org/10.1145/96602.96608</span></li>
<li><span id="kashyapSemanticSchematicSimilarities1996">178. Kashyap V, Sheth A (1996) Semantic and Schematic Similarities between Database Objects:A Context-Based Approach. <i>The VLDB Journal</i>, 5(4):276–304. https://doi.org/10.1007/s007780050029</span></li>
<li><span id="hullManagingSemanticHeterogeneity1997">179. Hull R (1997) Managing Semantic Heterogeneity in Databases: A Theoretical Prospective. <i>Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</i>, :51–61. https://doi.org/10.1145/263661.263668</span></li>
<li><span id="busseFederatedInformationSystems1999">180. Busse S, Kutsche R-D, Leser U, Weber H (1999) Federated Information Systems: Concepts, Terminology and Architectures. :40. </span></li>
<li><span id="djokic-petrovicPIBASFedSPARQLWebbased2017">181. Djokic-Petrovic M, Cvjetkovic V, Yang J, Zivanovic M, Wild DJ (2017) PIBAS FedSPARQL: A Web-Based Platform for Integration and Exploration of Bioinformatics Datasets. <i>Journal of Biomedical Semantics</i>, 8(1):42. https://doi.org/10.1186/s13326-017-0151-z</span></li>
<li><span id="hasnainBioFedFederatedQuery2017">182. Hasnain A, Mehmood Q, Sana e Zainab S, Saleem M, Warren C, Zehra D, Decker S, Rebholz-Schuhmann D (2017) BioFed: Federated Query Processing over Life Sciences Linked Open Data. <i>Journal of Biomedical Semantics</i>, 8(1):13. https://doi.org/10.1186/s13326-017-0118-0</span></li>
<li><span id="shethFederatedDatabaseSystems1990">183. Sheth AP, Larson JA (1990) Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases. <i>ACM Computing Surveys</i>, 22(3):183–236. https://doi.org/10.1145/96602.96604</span></li>
<li><span id="bonifatiDistributedDatabasesPeertopeer2008">184. Bonifati A, Chrysanthis PK, Ouksel AM, Sattler K-U (2008) Distributed Databases and Peer-to-Peer Databases: Past and Present. <i>ACM SIGMOD Record</i>, 37(1):5–11. https://doi.org/10.1145/1374780.1374781</span></li>
<li><span id="spornyDecentralizedIdentifiersDIDs2021">185. Sporny M, Longley D, Sabadello M, Reed D, Steele O, Allen C (2021) Decentralized Identifiers (DIDs) v1.0. https://w3c.github.io/did-core/</span></li>
<li><span id="MeatballWikiPersonalCategories">186. Meatball Wiki: PersonalCategories. http://meatballwiki.org/wiki/PersonalCategories</span></li>
<li><span id="pirroDHTbasedSemanticOverlay2012">187. Pirrò G, Talia D, Trunfio P (2012) A DHT-based Semantic Overlay Network for Service Discovery. <i>Future Generation Computer Systems</i>, 28(4):689–707. https://doi.org/10.1016/j.future.2011.11.007</span></li>
<li><span id="Webber:18:A">188. Webber C, Tallon J, Shepherd E, Guy A, Prodromou E (2018) ActivityPub. https://www.w3.org/TR/2018/REC-activitypub-20180123/</span></li>
<li><span id="spornyJSONLDJSONbasedSerialization2020">189. Sporny M, Longley D, Kellogg G, Lanthaler M, Champin P-A, Lindström N (2020) JSON-LD 1.1 - A JSON-based Serialization for Linked Data. https://www.w3.org/TR/json-ld/</span></li>
<li><span id="snellActivityStreams2017">190. Snell JM, Prodromou E (2017) Activity Streams 2.0. https://www.w3.org/TR/activitystreams-core/</span></li>
<li><span id="SPARQLFederatedQuery2013">191. (2013) SPARQL 1.1 Federated Query. https://www.w3.org/TR/sparql11-federated-query/</span></li>
<li><span id="simaEnablingSemanticQueries2019">192. Sima AC, Mendes de Farias T, Zbinden E, Anisimova M, Gil M, Stockinger H, Stockinger K, Robinson-Rechavi M, Dessimoz C (2019) Enabling Semantic Queries across Federated Bioinformatics Databases. <i>Database</i>, 2019(baz106)https://doi.org/10.1093/database/baz106</span></li>
<li><span id="hankeDefenseDecentralizedResearch2021">193. Hanke M, Pestilli F, Wagner AS, Markiewicz CJ, Poline J-B, Halchenko YO (2021) In Defense of Decentralized Research Data Management. <i>Neuroforum</i>, 27(1):17–25. https://doi.org/10.1515/nf-2020-0037</span></li>
<li><span id="saundersAutopilotAutomatingBehavioral2019">194. Saunders JL, Wehr M (2019) Autopilot: Automating Behavioral Experiments with Lots of Raspberry Pis. <i>bioRxiv</i>, :807693. https://doi.org/10.1101/807693</span></li>
<li><span id="spiesWorkflowCentricApproachIncreasing2017">195. Spies J (2017) A Workflow-Centric Approach to Increasing Reproducibility and Data Integrity. https://scholarworks.iu.edu/dspace/handle/2022/21729</span></li>
<li><span id="harrisArrayProgrammingNumPy2020">196. Harris CR, Millman KJ, van der Walt SJ, Gommers R, Virtanen P, Cournapeau D, Wieser E, Taylor J, Berg S, Smith NJ, Kern R, Picus M, Hoyer S, van Kerkwijk MH, Brett M, Haldane A, del Río JF, Wiebe M, Peterson P, Gérard-Marchant P, Sheppard K, Reddy T, Weckesser W, Abbasi H, Gohlke C, Oliphant TE (2020) Array Programming with NumPy. <i>Nature</i>, 585(7825):357–362. https://doi.org/10.1038/s41586-020-2649-2</span></li>
<li><span id="virtanenSciPyFundamentalAlgorithms2020">197. Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, Burovski E, Peterson P, Weckesser W, Bright J, van der Walt SJ, Brett M, Wilson J, Millman KJ, Mayorov N, Nelson ARJ, Jones E, Kern R, Larson E, Carey CJ, Polat İ, Feng Y, Moore EW, VanderPlas J, Laxalde D, Perktold J, Cimrman R, Henriksen I, Quintero EA, Harris CR, Archibald AM, Ribeiro AH, Pedregosa F, van Mulbregt P (2020) SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. <i>Nature Methods</i>, 17(3):261–272. https://doi.org/10.1038/s41592-019-0686-2</span></li>
<li><span id="pedregosaScikitlearnMachineLearning2011">198. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay É (2011) Scikit-Learn: Machine Learning in Python. <i>The Journal of Machine Learning Research</i>, 12(null):2825–2830. </span></li>
<li><span id="wiltschkoRevealingStructurePharmacobehavioral2020">199. Wiltschko AB, Tsukahara T, Zeine A, Anyoha R, Gillis WF, Markowitz JE, Peterson RE, Katon J, Johnson MJ, Datta SR (2020) Revealing the Structure of Pharmacobehavioral Space through Motion Sequencing. <i>Nature Neuroscience</i>, 23(11):1433–1443. https://doi.org/10.1038/s41593-020-00706-3</span></li>
<li><span id="coffeyDeepSqueakDeepLearningbased2019">200. Coffey KR, Marx RG, Neumaier JF (2019) DeepSqueak: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations. <i>Neuropsychopharmacology</i>, 44(5):859–868. https://doi.org/10.1038/s41386-018-0303-6</span></li>
<li><span id="yatsenkoDataJointSimplerRelational2018">201. Yatsenko D, Walker EY, Tolias AS (2018) DataJoint: A Simpler Relational Data Model. <i>arXiv:1807.11104 [cs]</i>, http://arxiv.org/abs/1807.11104</span></li>
<li><span id="yatsenkoDataJointElementsData2021">202. Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS (2021) DataJoint Elements: Data Workflows for Neurophysiology. <i>bioRxiv</i>, :2021.03.30.437358. https://doi.org/10.1101/2021.03.30.437358</span></li>
<li><span id="pachitariuKilosortRealtimeSpikesorting2016">203. Pachitariu M, Steinmetz N, Kadir S, Carandini M, D HK (2016) Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels. :061481. https://doi.org/10.1101/061481</span></li>
<li><span id="gamblinSpackPackageManager2015">204. Gamblin T, LeGendre M, Collette MR, Lee GL, Moody A, de Supinski BR, Futral S (2015) The Spack Package Manager: Bringing Order to HPC Software Chaos. <i>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</i>, https://doi.org/10.1145/2807591.2807623</span></li>
<li><span id="dolstraNixSafePolicyFree2004">205. Dolstra E, de Jonge M, Visser E (2004) Nix: A Safe and Policy-Free System for Software Deployment. <i>Proceedings of the 18th USENIX Conference on System Administration</i>, :79–92. </span></li>
<li><span id="vanderaalstYAWLAnotherWorkflow2005">206. van der Aalst WMP, ter Hofstede AHM (2005) YAWL: Yet Another Workflow Language. <i>Information Systems</i>, 30(4):245–275. https://doi.org/10.1016/j.is.2004.02.002</span></li>
<li><span id="sharmaPythonPackagesUpload2022">207. Sharma A (2022) Python Packages Upload Your AWS Keys, Env Vars, Secrets to the Web. https://blog.sonatype.com/python-packages-upload-your-aws-keys-env-vars-secrets-to-web</span></li>
<li><span id="steegenIncreasingTransparencyMultiverse2016">208. Steegen S, Tuerlinckx F, Gelman A, Vanpaemel W (2016) Increasing Transparency Through a Multiverse Analysis. <i>Perspectives on Psychological Science</i>, 11(5):702–712. https://doi.org/10.1177/1745691616658637</span></li>
<li><span id="millerScientistNightmareSoftware2006">209. Miller G (2006) A Scientist’s Nightmare: Software Problem Leads to Five Retractions. <i>Science</i>, 314(5807):1856–1857. https://doi.org/10.1126/science.314.5807.1856</span></li>
<li><span id="soergelRampantSoftwareErrors2015">210. Soergel DAW (2015) Rampant Software Errors May Undermine Scientific Results. <i>F1000Research</i>, 3https://doi.org/10.12688/f1000research.5930.2</span></li>
<li><span id="eklundClusterFailureWhy2016a">211. Eklund A, Nichols TE, Knutsson H (2016) Cluster Failure: Why fMRI Inferences for Spatial Extent Have Inflated False-Positive Rates. <i>Proceedings of the National Academy of Sciences</i>, 113(28):7900–7905. https://doi.org/10.1073/pnas.1602413113</span></li>
<li><span id="bhandarineupaneCharacterizationLeptazolinesPolar2019">212. Bhandari Neupane J, Neupane RP, Luo Y, Yoshida WY, Sun R, Williams PG (2019) Characterization of Leptazolines A–D, Polar Oxazolines from the Cyanobacterium Leptolyngbya Sp., Reveals a Glitch with the “Willoughby–Hoye” Scripts for Calculating NMR Chemical Shifts. <i>Organic Letters</i>, 21(20):8449–8453. https://doi.org/10.1021/acs.orglett.9b03216</span></li>
<li><span id="larsonFoldingHomeGenome2009">213. Larson SM, Snow CD, Shirts M, Pande VS (2009) Folding@Home and Genome@Home: Using Distributed Computing to Tackle Previously Intractable Problems in Computational Biology. (arXiv:0901.0866)https://doi.org/10.48550/arXiv.0901.0866</span></li>
<li><span id="bebergFoldingHomeLessons2009">214. Beberg AL, Ensign DL, Jayachandran G, Khaliq S, Pande VS (2009) Folding@home: Lessons from Eight Years of Volunteer Distributed Computing. <i>2009 IEEE International Symposium on Parallel &amp; Distributed Processing</i>, :1–8. https://doi.org/10.1109/IPDPS.2009.5160922</span></li>
<li><span id="smarrPacificResearchPlatform2018a">215. Smarr L, Crittenden C, DeFanti T, Graham J, Mishin D, Moore R, Papadopoulos P, Würthwein F (2018) The Pacific Research Platform: Making High-Speed Networking a Reality for the Scientist. <i>Proceedings of the practice and experience on advanced research computing</i>, :1–8. https://doi.org/10.1145/3219104.3219108</span></li>
<li><span id="wulfBEADLXMLDocumentation2020">216. Wulf M (2020) BEADL XML Documentation V 0.1. http://archive.org/details/beadl-xml-documentation-v-0.1</span></li>
<li><span id="nwbbehavioraltaskwgNWBBehavioralTask2020">217. WG NWBBT (2020) NWB Behavioral Task WG. http://archive.org/details/nwb-behavioral-task-wg</span></li>
<li><span id="saundersAUTOPILOTAutomatingExperiments2022">218. Saunders JL, Ott LA, Wehr M (2022) AUTOPILOT: Automating Experiments with Lots of Raspberry Pis. <i>bioRxiv : the preprint server for biology</i>, https://doi.org/10.1101/807693v2</span></li>
<li><span id="lopesBonsaiEventbasedFramework2015a">219. Lopes G, Bonacchi N, Frazão J, Neto JP, Atallah BV, Soares S, Moreira L, Matias S, Itskov PM, Correia PA, Medina RE, Calcaterra L, Dreosti E, Paton JJ, Kampff AR (2015) Bonsai: An Event-Based Framework for Processing and Controlling Data Streams. <i>Frontiers in Neuroinformatics</i>, 9:7. https://doi.org/10.3389/fninf.2015.00007</span></li>
<li><span id="lopesNewOpenSourceTools2021">220. Lopes G, Monteiro P (2021) New Open-Source Tools: Using Bonsai for Behavioral Tracking and Closed-Loop Experiments. <i>Frontiers in Behavioral Neuroscience</i>, 15:53. https://doi.org/10.3389/fnbeh.2021.647640</span></li>
<li><span id="sterlingPublicationDecisionsTheir1959">221. Sterling TD (1959) Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significance–Or Vice Versa. <i>Journal of the American Statistical Association</i>, 54(285):30–34. https://doi.org/10.2307/2282137</span></li>
<li><span id="francoPublicationBiasSocial2014">222. Franco A, Malhotra N, Simonovits G (2014) Publication Bias in the Social Sciences: Unlocking the File Drawer. <i>Science</i>, 345(6203):1502–1505. https://doi.org/10.1126/science.1255484</span></li>
<li><span id="marderMaintainingJoyDiscovery2022">223. Marder E (2022) Maintaining the Joy of Discovery. <i>eLife</i>, 11:e80711. https://doi.org/10.7554/eLife.80711</span></li>
<li><span id="relx2020ResultsPresentation2021">224. RELX (2021) 2020 Results Presentation to Investors - Transcript. https://www.relx.com/ /media/Files/R/RELX-Group/documents/investors/transcripts/results-2020-transcript.pdf</span></li>
<li><span id="RELXAnnualReport2019">225. (2019) RELX Annual Report 2019. https://www.relx.com/ /media/Files/R/RELX-Group/documents/reports/annual-reports/2019-annual-report.pdf</span></li>
<li><span id="brembsPrestigiousScienceJournals2018">226. Brembs B (2018) Prestigious Science Journals Struggle to Reach Even Average Reliability. <i>Frontiers in Human Neuroscience</i>, 12https://www.frontiersin.org/articles/10.3389/fnhum.2018.00037</span></li>
<li><span id="springernatureBrandedContent">227. Springer Nature Branded Content. <i>Nature Research Partnerships</i>, https://partnerships.nature.com/product/branded-content-native-advertising/</span></li>
<li><span id="dekeyzerProcessingNativeAdvertising2021">228. De Keyzer F, Dens N, De Pelsmacker P (2021) The Processing of Native Advertising Compared to Banner Advertising: An Eye-Tracking Experiment. <i>Electronic Commerce Research</i>, https://doi.org/10.1007/s10660-021-09523-7</span></li>
<li><span id="koutsopoulosNativeAdvertisementSelection2016">229. Koutsopoulos I, Spentzouris P (2016) Native Advertisement Selection and Allocation in Social Media Post Feeds. <i>Machine Learning and Knowledge Discovery in Databases</i>, :588–603. https://doi.org/10.1007/978-3-319-46128-1_37</span></li>
<li><span id="elsevierDrugDesignOptimization">230. Elsevier Drug Design Optimization. <i>Elsevier.com</i>, https://www.elsevier.com/solutions/professional-services/drug-design-optimization</span></li>
<li><span id="elsevierTopicProminenceScienceb">231. Elsevier Topic Prominence in Science - Scival | Elsevier Solutions. <i>Elsevier.com</i>, https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science</span></li>
<li><span id="hansonUserTrackingAcademic2019">232. Hanson C (2019) User Tracking on Academic Publisher Platforms. <i>The Coalition for Networked Information</i>, </span></li>
<li><span id="researchfoundationofkoreaSouthKoreaTechnological2020">233. Research Foundation of Korea, Elsevier (2020) South Korea: A Technological Powerhouse Strengthening Its Research and Innovation Footprint. https://www.elsevier.com/research-intelligence/resource-library/south-korea-research-and-innovation-power-house</span></li>
<li><span id="seongBrainKorea212008">234. Seong S, Popper SW, Goldman CA, Evans DK, Grammich CA (2008) Brain Korea 21 Phase II: A New Evaluation Model. https://www.rand.org/pubs/monographs/MG711.html</span></li>
<li><span id="elsevierCaseStudyNational2019">235. Elsevier (2019) Case Study: National Research Foundation of Korea. https://www.elsevier.com/__data/assets/pdf_file/0007/927583/ACAD_RL_SV_CS_NRFK_Web.pdf</span></li>
<li><span id="elsevierkoreaSciValHwalyongeulWihan2021">236. Elsevier Korea (2021) SciVal 활용을 위한 Workflow 제안. https://www.elsevier.com/__data/assets/pdf_file/0010/1179244/01.-SciVal-Workflow202112thSciVal-User-Group-Meeting.pdf</span></li>
<li><span id="heathersRealScandalIvermectin2021">237. Heathers J (2021) The Real Scandal About Ivermectin. <i>The Atlantic</i>, https://www.theatlantic.com/science/archive/2021/10/ivermectin-research-problems/620473/</span></li>
<li><span id="shenMeetThisSuperspotter2020">238. Shen H (2020) Meet This Super-Spotter of Duplicated Images in Science Papers. <i>Nature</i>, 581(7807):132–136. https://doi.org/10.1038/d41586-020-01363-z</span></li>
<li><span id="bikPrevalenceInappropriateImage2016">239. Bik EM, Casadevall A, Fang FC (2016) The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications. <i>mBio</i>, 7(3):e00809–16. https://doi.org/10.1128/mBio.00809-16</span></li>
<li><span id="westMisinformationScience2021">240. West JD, Bergstrom CT (2021) Misinformation in and about Science. <i>Proceedings of the National Academy of Sciences</i>, 118(15):e1912444117. https://doi.org/10.1073/pnas.1912444117</span></li>
<li><span id="kennedyAmericansTrustScientists2022">241. Kennedy B, Tyson A, Funk C (2022) Americans’ Trust in Scientists, Other Groups Declines. https://policycommons.net/artifacts/2256823/americans-trust-in-scientists-other-groups-declines/3015477/</span></li>
<li><span id="krauseTrustFallacyScientists2021">242. Krause NM, Scheufele DA, Freiling I, Brossard D (2021) The Trust Fallacy: Scientists’ Search for Public Pathologies Is Unhealthy, Unhelpful, and Ultimately Unscientific. <i>American Scientist</i>, 109(4):226–232. https://go.gale.com/ps/i.do?p=AONE&amp;sw=w&amp;issn=00030996&amp;v=2.1&amp;it=r&amp;id=GALE%7CA669437356&amp;sid=googleScholar&amp;linkaccess=abs</span></li>
<li><span id="chuSlowedCanonicalProgress2021">243. Chu JSG, Evans JA (2021) Slowed Canonical Progress in Large Fields of Science. <i>Proceedings of the National Academy of Sciences</i>, 118(41)https://doi.org/10.1073/pnas.2021636118</span></li>
<li><span id="whiteFutureOpenOpenSource2019">244. White SR, Amarante LM, Kravitz AV, Laubach M (2019) The Future Is Open: Open-Source Tools for Behavioral Neuroscience Research. <i>eNeuro</i>, 6(4):ENEURO.0223–19.2019. https://doi.org/10.1523/ENEURO.0223-19.2019</span></li>
<li><span id="baldwinScientificAutonomyPublic2018">245. Baldwin M (2018) Scientific Autonomy, Public Accountability, and the Rise of “Peer Review” in the Cold War United States. <i>Isis</i>, 109(3):538–558. https://doi.org/10.1086/700070</span></li>
<li><span id="swartzMakingMoreWikipedias2006">246. Swartz (2006) Making More Wikipedias (Aaron Swartz’s Raw Thought). http://www.aaronsw.com/weblog/morewikipedias</span></li>
<li><span id="leufWikiWayQuick2001a">247. Leuf B, Cunningham W (2001) The Wiki Way : Quick Collaboration on the Web. http://archive.org/details/isbn_9780201714999</span></li>
<li><span id="hillWikipediaEndOpen2019">248. Hill BM, Shaw A (2019) Wikipedia and the End of Open Collaboration? <i>Wikipedia @ 20</i>, :12. https://wikipedia20.pubpub.org/pub/lifecycles</span></li>
<li><span id="swartzWhoWritesWikipedia2006">249. Swartz A (2006) Who Writes Wikipedia? (Aaron Swartz’s Raw Thought). http://www.aaronsw.com/weblog/whowriteswikipedia</span></li>
<li><span id="halfakerRiseDeclineOpen2013">250. Halfaker A, Geiger RS, Morgan JT, Riedl J (2013) The Rise and Decline of an Open Collaboration System: How Wikipedia’s Reaction to Popularity Is Causing Its Decline. <i>American Behavioral Scientist</i>, 57(5):664–688. https://doi.org/10.1177/0002764212469365</span></li>
<li><span id="C2wikiWikiHistory">251. C2wiki - Wiki History. http://wiki.c2.com/?WikiHistory</span></li>
<li><span id="valentineC2wikiExerciseDialogical2021">252. valentine beka (2021) C2wiki Is an Exercise in Dialogical Methods. of Laying Bare the Fact That Knowledge and Ideas Are Not Some Truth Delivered from On High, but Rather a Social Process, a Conversation, a Dialectic, between Various Views and Interests. <i>@beka_valentine</i>, https://twitter.com/beka_valentine/status/1454522998594043906</span></li>
<li><span id="WikipediaFlow2021">253. (2021) Wikipedia:Flow. <i>Wikipedia</i>, https://en.wikipedia.org/w/index.php?title=Wikipedia:Flow&amp;oldid=1027052364</span></li>
<li><span id="schneiderUnderstandingImprovingWikipedia2011">254. Schneider J, Passant A, Breslin JG (2011) Understanding and Improving Wikipedia Article Discussion Spaces: 2011 ACM Symposium. :808–813. https://doi.org/10.1145/1982185.1982358</span></li>
<li><span id="viegasTalkYouType2007">255. Viegas FB, Wattenberg M, Kriss J, van Ham F (2007) Talk Before You Type: Coordination in Wikipedia. <i>2007 40th Annual Hawaii International Conference on System Sciences (HICSS’07)</i>, :78–78. https://doi.org/10.1109/HICSS.2007.511</span></li>
<li><span id="MeatballWikiRightToLeave">256. Meatball Wiki: RightToLeave. http://meatballwiki.org/wiki/RightToLeave</span></li>
<li><span id="hancockOpenOfficeOrgDead2010">257. Hancock T (2010) OpenOffice.Org Is Dead, Long Live LibreOffice – or, The Freedom to Fork. http://freesoftwaremagazine.com/articles/openoffice_org_dead_long_live_libreoffice/</span></li>
<li><span id="MeatballWikiEnlargeSpace">258. Meatball Wiki: EnlargeSpace. http://meatballwiki.org/wiki/EnlargeSpace</span></li>
<li><span id="MeatballWikiForkingOfOnlineCommunitiesa">259. Meatball Wiki: ForkingOfOnlineCommunities. http://meatballwiki.org/wiki/ForkingOfOnlineCommunities</span></li>
<li><span id="tkaczSpanishForkWikipedia2011">260. Tkacz N (2011) The Spanish Fork: Wikipedia’s Ad-Fuelled Mutiny. <i>Wired UK</i>, https://www.wired.co.uk/article/wikipedia-spanish-fork</span></li>
<li><span id="tkaczWikipediaPoliticsOpenness2014">261. Tkacz N (2014) Wikipedia and the Politics of Openness. <i>Wikipedia and the Politics of Openness; https://web.archive.org/web/20211116211648/https://www.degruyter.com/document/doi/10.7208/9780226192444/html</i>, https://doi.org/10.7208/9780226192444</span></li>
<li><span id="kleinWereDCSeattle2001">262. Klein N (2001) Were the DC and Seattle Protests Unfocused? https://naomiklein.org/were-dc-and-seattle-protests-unfocused/</span></li>
<li><span id="bookchinNoteAffinityGroups1969">263. Bookchin M (1969) A Note on Affinity Groups. :2. </span></li>
<li><span id="kluyverJupyterNotebooksPublishing2016">264. Kluyver T, Ragan-Kelley B, Pérez F, Granger B, Bussonnier M, Frederic J, Kelley K, Hamrick J, Grout J, Corlay S, Ivanov P, Avila D, Abdalla S, Willing C, Team JD (2016) Jupyter Notebooks – a Publishing Format for Reproducible Computational Workflows. <i>Positioning and Power in Academic Publishing: Players, Agents and Agendas</i>, :87–90. https://doi.org/10.3233/978-1-61499-649-1-87</span></li>
<li><span id="hunterMatplotlib2DGraphics2007">265. Hunter JD (2007) Matplotlib: A 2D Graphics Environment. <i>Computing in Science &amp; Engineering</i>, 9(03):90–95. https://doi.org/10.1109/MCSE.2007.55</span></li>
<li><span id="peroniFaBiOCiTOOntologies2012">266. Peroni S, Shotton D (2012) FaBiO and CiTO: Ontologies for Describing Bibliographic Resources and Citations. <i>Journal of Web Semantics</i>, 17:33–43. https://doi.org/10.1016/j.websem.2012.08.001</span></li>
<li><span id="sorgaardUseParagraphStyles1996">267. Sørgaard P, Sandahl TI, Ljungberg F (1996) Use of Paragraph Styles in Word Processing: A Stepping Stone for CSCW? :10. https://doi.org/10.1.1.55.7928</span></li>
<li><span id="knuthTeXbook1986">268. Knuth DE (1986) The TeXbook. (A)</span></li>
<li><span id="forresterInventingWeGo2012">269. Forrester J (2012) Inventing as We Go: Building a Visual Editor for MediaWiki. <i>Diff</i>, https://diff.wikimedia.org/2012/12/07/inventing-as-we-go-building-a-visual-editor-for-mediawiki/</span></li>
<li><span id="capadisliDecentralisedAuthoringAnnotations2017">270. Capadisli S, Guy A, Verborgh R, Lange C, Sören A, Berners-Lee T (2017) Decentralised Authoring, Annotations and Notifications for a Read-Write Web with Dokieli. https://csarven.ca/dokieli-rww</span></li>
<li><span id="capadisliLinkedResearchDecentralised2019">271. Capadisli S (2019) Linked Research on the Decentralised Web. https://csarven.ca/linked-research-decentralised-web</span></li>
<li><span id="chattopadhyayWhatWrongComputational2020">272. Chattopadhyay S, Prasad I, Henley AZ, Sarma A, Barik T (2020) What’s Wrong with Computational Notebooks? Pain Points, Needs, and Design Opportunities. <i>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</i>, :1–12. https://doi.org/10.1145/3313831.3376729</span></li>
<li><span id="ruleTenSimpleRules2019">273. Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, Moshiri N, Nguyen MH, Rosenthal SB, Pérez F, Rose PW (2019) Ten Simple Rules for Writing and Sharing Computational Analyses in Jupyter Notebooks. <i>PLOS Computational Biology</i>, 15(7):e1007007. https://doi.org/10.1371/journal.pcbi.1007007</span></li>
<li><span id="woffordJupyterNotebooksDiscovery2020">274. Wofford MF, Boscoe BM, Borgman CL, Pasquetto IV, Golshan MS (2020) Jupyter Notebooks as Discovery Mechanisms for Open Science: Citation Practices in the Astronomy Community. <i>Computing in Science &amp; Engineering</i>, 22(1):5–15. https://doi.org/10.1109/MCSE.2019.2932067</span></li>
<li><span id="kaniiniActivityPubPresentState2019">275. kaniini (2019) ActivityPub: The Present State, or Why Saving the ’worse Is Better’ Virus Is Both Possible and Important. <i>kaniini’s blog!</i>, https://blog.dereferenced.org/activitypub-the-present-state-or-why-saving-the-worse-is-better-virus-is</span></li>
<li><span id="webberActivityPubDecentralizedDistributed2017">276. Webber C, Sporny M (2017) ActivityPub: From Decentralized to Distributed Social Networks. :15. </span></li>
<li><span id="jimenezBorderlandingAcademicResearchers2020">277. Jiménez LM, Hermann-Wilmarth JM (2020) Borderlanding Academic Researchers: A Range of Whisper Networks. <i>Journal of Lesbian Studies</i>, 24(4):327–331. https://doi.org/10.1080/10894160.2019.1678331</span></li>
<li><span id="doctorowAdversarialInteroperabilityReviving2019">278. Doctorow C (2019) Adversarial Interoperability: Reviving an Elegant Weapon From a More Civilized Age to Slay Today’s Monopolies. <i>Electronic Frontier Foundation</i>, https://www.eff.org/deeplinks/2019/06/adversarial-interoperability-reviving-elegant-weapon-more-civilized-age-slay</span></li>
<li><span id="doctorowAdversarialInteroperability2019">279. Doctorow C (2019) Adversarial Interoperability. <i>Electronic Frontier Foundation</i>, https://www.eff.org/deeplinks/2019/10/adversarial-interoperability</span></li>
<li><span id="jacksonMarginaliaReadersWriting2001">280. Jackson HJ (2001) Marginalia: Readers Writing in Books. </span></li>
<li><span id="csillagFuzzyAnchoring2013">281. csillag (2013) Fuzzy Anchoring. <i>Hypothesis</i>, https://web.hypothes.is/blog/fuzzy-anchoring/</span></li>
<li><span id="harnadSkyWriting1987">282. Harnad S (1987) Sky-Writing. https://web.archive.org/web/20220315173536/https://www.southampton.ac.uk/ harnad/skywriting.html</span></li>
<li><span id="harnadScholarlySkywritingPrepublication1990">283. Harnad S (1990) Scholarly Skywriting and the Prepublication Continuum of Scientific Inquiry. <i>Psychological Science</i>, 1(6):342–344. https://doi.org/10.1111/j.1467-9280.1990.tb00234.x</span></li>
<li><span id="ellisBooksTranslationWanted1986">284. Ellis M (1986) Books on Translation Wanted. https://groups.google.com/g/sci.lang/c/RY0h8td6D48/m/qKqCQeW1M7oJ</span></li>
<li><span id="ELifePartnersHypothes2016">285. (2016) eLife Partners with Hypothes.Is to Advance Open Scholarly Annotation. <i>eLife</i>, https://elifesciences.org/for-the-press/7e7220f6/elife-partners-with-hypothes-is-to-advance-open-scholarly-annotation</span></li>
<li><span id="dwhlyBioRxivSelectsHypothesis2017">286. dwhly (2017) bioRxiv Selects Hypothesis to Enable Annotation on Preprints. <i>Hypothesis</i>, https://web.hypothes.is/blog/biorxiv-selects-hypothesis/</span></li>
<li><span id="heatherstainesPreprintServicesGather2018">287. heatherstaines (2018) Preprint Services Gather to Explore an Annotated Future. <i>Hypothesis</i>, https://web.hypothes.is/blog/preprint-services-gather-to-explore-an-annotated-future/</span></li>
<li><span id="nateangellAnnouncingTRiPTransparent2019">288. nateangell (2019) Announcing TRiP: Transparent Review in Preprints, Powered by Hypothesis Annotation. <i>Hypothesis</i>, https://web.hypothes.is/blog/announcing-trip-transparent-review-in-preprints-powered-by-hypothesis-annotation/</span></li>
<li><span id="ivanecFutureNoteTaking2021">289. Ivanec E, Whaley D, Doyon D, Cunningham W, Guerry B, Sauter O, Sullivan CW, Zhan J (2021) The Future of Note Taking (FoNT). <i>I Annotate 2021</i>, https://iannotate.org/2021/program/panel_font.html</span></li>
<li><span id="velitchkovPersonalKnowledgeGraphs">290. Personal Knowledge Graphs. https://personalknowledgegraphs.com/</span></li>
<li><span id="bostonNeedKnowInformationSeeking2022">291. Boston A (2022) Need to Know: The Information-Seeking Behavior of Doja Cat. <i>twitter.com</i>, https://hcommons.org/deposits/item/hc:46629/</span></li>
<li><span id="butlerDonLookNow2008">292. Butler B, Joyce E, Pike J (2008) Don’t Look Now, but We’ve Created a Bureaucracy: The Nature and Roles of Policies and Rules in Wikipedia. <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, :1101–1110. https://doi.org/10.1145/1357054.1357227</span></li>
<li><span id="meehlTheoreticalRisksTabular1978">293. Meehl PE (1978) Theoretical Risks and Tabular Asterisks: Sir Karl, Sir Ronald, and the Slow Progress of Soft Psychology. <i>Journal of Consulting and Clinical Psychology</i>, 46(4):806–834. https://doi.org/10.1037/0022-006X.46.4.806</span></li>
<li><span id="vanrooijTheoryTestHow2021">294. van Rooij I, Baggio G (2021) Theory Before the Test: How to Build High-Verisimilitude Explanatory Theories in Psychological Science. <i>Perspectives on Psychological Science</i>, :1745691620970604. https://doi.org/10.1177/1745691620970604</span></li>
<li><span id="guestHowComputationalModeling2021">295. Guest O, Martin AE (2021) How Computational Modeling Can Force Theory Building in Psychological Science. <i>Perspectives on Psychological Science</i>, :1745691620970585. https://doi.org/10.1177/1745691620970585</span></li>
<li><span id="eisenReasonWeAre2021">296. Eisen M (2021) The Reason We Are (Once Again) Having a Fight about Whether the Producers of Publicly Available/Published Data Should Be Authors on Any Work Using Said Data Is That We Have a Completely Dysfunctional System for Crediting the Generation of Useful Data. <i>@mbeisen</i>, https://twitter.com/mbeisen/status/1459911286242701313</span></li>
<li><span id="eisenSameTruePeople2021">297. Eisen M (2021) The Same Is True for People Who Generate Useful Reagents, Resources and Software. <i>@mbeisen</i>, https://twitter.com/mbeisen/status/1459911287773691906</span></li>
<li><span id="eisenEverythingRealAnswer2021">298. Eisen M (2021) And like Everything, the Real Answer Lies on How We Assess Candidates for Jobs, Grants, Etc\ldots So Long as People Treat Authorship as the Most/Only Valuable Currency, This Debate Will Fester. But It’s in Our Power to Change It. <i>@mbeisen</i>, https://twitter.com/mbeisen/status/1459911289380093956</span></li>
<li><span id="varmusOncogenesOpenScience2019">299. Varmus H (2019) Of Oncogenes and Open Science: An Interview with Harold Varmus. <i>Disease Models &amp; Mechanisms</i>, 12(3):dmm038919. https://doi.org/10.1242/dmm.038919</span></li>
<li><span id="teixeiradasilvaMultipleVersionsHindex2018">300. Teixeira da Silva JA, Dobránszki J (2018) Multiple Versions of the H-Index: Cautionary Use for Formal Academic Purposes. <i>Scientometrics</i>, 115(2):1107–1113. https://doi.org/10.1007/s11192-018-2680-3</span></li>
<li><span id="costasReflectionsCautionaryUse2018">301. Costas R, Franssen T (2018) Reflections around ‘the Cautionary Use’ of the h-Index: Response to Teixeira Da Silva and Dobránszki. <i>Scientometrics</i>, 115(2):1125–1130. https://doi.org/10.1007/s11192-018-2683-0</span></li>
<li><span id="crossrefJanuary2021Public2021">302. Crossref (2021) January 2021 Public Data File from Crossref. <i>Academic Torrents</i>, https://doi.org/10.13003/GU3DQMJVG4</span></li>
<li><span id="rentstrikeWorkFuturePerfect2021">303. RENT STRIKE (2021) Work! (Future Perfect). https://rentstrike.bandcamp.com/track/work-future-perfect-2</span></li>
<li><span id="harthLinkingSemanticallyEnabled2004">304. Harth A, Breslin JG, O’Murchu I, Decker S (2004) Linking Semantically Enabled Online Community Sites. <i>Proceedings of the 1st Workshop on Friend of a Friend</i>, :11. </span></li>
<li><span id="kauppinenLinkedOpenScienceCommunicating2011">305. Kauppinen T, de Espindola GM (2011) Linked Open Science-Communicating, Sharing and Evaluating Data, Methods and Results for Executable Papers. <i>Procedia Computer Science</i>, 4:726–731. https://doi.org/10.1016/j.procs.2011.04.076</span></li>
<li><span id="finleyTimBernersLeeInventor2017">306. Finley K (2017) Tim Berners-Lee, Inventor of the Web, Plots a Radical Overhaul of His Creation. <i>Wired</i>, https://www.wired.com/2017/04/tim-berners-lee-inventor-web-plots-radical-overhaul-creation/</span></li>
<li><span id="barabasDefendingInternetFreedom2017">307. Barabas C, Narula N, Zuckerman E (2017) Defending\hspace0pt \hspace0ptInternet\hspace0pt \hspace0ptFreedom\hspace0pt \hspace0ptthrough\hspace0pt \hspace0ptDecentralization: Back\hspace0pt \hspace0ptto\hspace0pt \hspace0ptthe\hspace0pt \hspace0ptFuture? https://www.bitsoffreedom.nl/wp-content/uploads/2017/08/defending-internet-freedom-through-decentralization.pdf</span></li>
<li><span id="StandardizingActivityPubGroups2021">308. (2021) Standardizing on ActivityPub Groups - ActivityPub. <i>SocialHub</i>, https://socialhub.activitypub.rocks/t/standardizing-on-activitypub-groups/1984</span></li>
<li><span id="sariGuestPostTechnology2018">309. Sari F (2018) Guest Post: Technology, Law, and Education: A Three-Pronged Approach to Fight Digital Piracy. <i>The Scholarly Kitchen</i>, https://scholarlykitchen.sspnet.org/2018/04/24/guest-post-technology-law-education-three-pronged-approach-fight-digital-piracy/</span></li>
<li><span id="brembsSNSINewPRISM2020">310. Brembs B (2020) Is the SNSI the New PRISM? <i>bjoern.brembs.blog</i>, http://bjoern.brembs.net/2020/10/is-the-snsi-the-new-prism/</span></li>
<li><span id="nisoNISORP272019Recommended2019">311. NISO (2019) NISO RP-27-2019, Recommended Practices for Improved Access to Institutionally-Provided Information Resources: Results from the Resource Access in the 21st Century (RA21) Project | NISO Website. https://www.niso.org/publications/rp-27-2019-ra21</span></li>
<li><span id="snsiCybersecurityLandscapeProtecting2020">312. SNSI (2020) Cybersecurity Landscape - Protecting the Scholarly Infrastructure. https://www.snsi.info/news-and-events/cybersecurity-landscape/</span></li>
<li><span id="SeamlessAccessActionSeamlessAccess">313. SeamlessAccess in Action - SeamlessAccess. https://seamlessaccess.org/work/</span></li>
<li><span id="lifesciencesprofessionalservicesEmergingTrendsPancreatitis2021">314. Life Sciences Professional Services (2021) Emerging Trends for Pancreatitis in the Scientific Literature. https://www.elsevier.com/__data/assets/pdf_file/0003/1209216/Elsevier-Emerging-trends-pancreatitis-scientific-literature.pdf</span></li>
<li><span id="elsevierTopicProminenceScience">315. Elsevier Topic Prominence in Science - Scival | Elsevier Solutions. <i>Elsevier.com</i>, https://www.elsevier.com/solutions/scival/features/topic-prominence-in-science</span></li></ol>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:socialmediaflirting" role="doc-endnote">
      <p>(save some <a href="#forums-are-just-one-point-in-a-continuous-feature-space-of-commu">complicated half-in flirtation</a> with social media). <a href="#fnref:socialmediaflirting" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:disciplinecaveat" role="doc-endnote">
      <p>At least in systems neuroscience, appropriate caveats below. <a href="#fnref:disciplinecaveat" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:nocrypto" role="doc-endnote">
      <p>Recently the notion of decentralized digital infrastructure has been co-opted by a variety of swindlers and other motivated parties to refer to blockchain-based technologies like cryptocurrencies, decentralized autonomous organizations, and the like. This work will not discuss them, as they have not been demonstrated to do anything that peer-to-peer technology with adjoining social systems can’t do except use a collosal quantity of fossil fuels and drain a lot of credulous people’s bank accounts. <a href="#fnref:nocrypto" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:usuallyjusttheirs" role="doc-endnote">
      <p>(and usually their lab or institute only) <a href="#fnref:usuallyjusttheirs" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whichvoid" role="doc-endnote">
      <p>(Twitter) <a href="#fnref:whichvoid" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:solaris" role="doc-endnote">
      <blockquote>
        <p>…  the recording instruments registered a profusion of signals - fragmentary indications of some outlandish activity, which in fact defeated all attempts at analysis. Did these data point to a momentary condition of stimulation, or to regular impulses correlated with the gigantic structures which the ocean was in the process of creating elsewhere, at the antipodes of the region under investigation? Had the electronic apparatus recorded the cryptic manifestation of the ocean’s ancient secrets? Had it revealed its innermost workings to us? Who could tell? No two reactions to the stimuli were the same. Sometimes the instruments almost exploded under the violence of the impulses, sometimes there was total silence; it was impossible to obtain a repetition of any previously observed phenomenon. Constantly, it seemed, the experts were on the brink of deciphering the ever-growing mass of information. Was it not, after all, with this object in mind that computers had been built of virtually limitless capacity, such as no previous problem had ever demanded?</p>

        <p>And, indeed, some results were obtained. The ocean as a source of electric and magnetic impulses and of gravitation expressed itself in a more or less mathematical language. Also, by calling on the most abstruse branches of statistical analysis, it was possible to classify certain frequencies in the discharges of current. Structural homologues were discovered, not unlike those already observed by physicists in that sector of science which deals with the reciprocal interaction of energy and matter, elements and compounds, the finite and the infinite. This correspondence convinced the scientists that they were confronted with a monstrous entity endowed with reason, a protoplasmic ocean-brain enveloping the entire planet and idling its time away in extravagant theoretical cognitation about the nature of the universe. Our instruments had intercepted minute random fragments of a prodigious and everlasting monologue unfolding in the depths of this colossal brain, which was inevitably beyond our understanding.</p>
      </blockquote>

      <blockquote>
        <p>So much for the mathematicians. These hypotheses, according to some people, underestimated the resources of the human mind; they bowed to the unknown, proclaiming the ancient doctrine, arrogantly resurrected, of ignoramus et ignorabimus. Others regarded the mathematicians’ hypotheses as sterile and dangerous nonsense, contributing towards the creation of a modern mythology based on the notion of this giant brain - whether plasmic or electronic was immaterial - as the ultimate objective of existence, the very synthesis of life.</p>
      </blockquote>

      <blockquote>
        <p>Yet others… but the would-be experts were legion and each had his own theory. A comparison of the ‘contact’ school of thought with other branches of Solarist studies, in which specialization had rapidly developed, especially during the last quarter of a century, made it clear that a Solarist-cybernetician had difficulty in making himself understood to a Solarist-symmetriadologist. Veubeke, director of the Institute when I was studying there, had asked jokingly one day: <strong>“How do you expect to communicate with the ocean, when you can’t even understand one another?”</strong> The jest contained more than a grain of truth. […]</p>
      </blockquote>

      <blockquote>
        <p>Lifting the heavy volume with both hands, I replaced it on the shelf, and thought to myself that our scholarship, all the information accumulated in the libraries, amounted to a useless jumble of words, a sludge of statements and suppositions, and that we had not progressed an inch in the 78 years since researches had begun. The situation seemed much worse now than in the time of the pioneers, since the assiduous efforts of so many years had not resulted in a single indisputable conclusion. “</p>
      </blockquote>

      <p>Stanisław Lem, <em>Solaris</em>, essential reading for all neuroscientists <a href="#fnref:solaris" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whatisblast" role="doc-endnote">
      <p>“Basic Local Alignment Search Tool” - a tool to compare genetic or protein sequences to find potential matches or analogues. <a href="#fnref:whatisblast" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:andgoogle" role="doc-endnote">
      <p>This isn’t a story of “good people” and “bad people,” as a lot of the linked data technology also serves as the backbone for abusive technology monopolies like google’s acquisition of Freebase <a class="citation" href="#iainFreebaseDeadLong2019">[317]</a> and the profusion of knowledge graph-based medical platforms. <a href="#fnref:andgoogle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:xkcd" role="doc-endnote">
      <p>There is, of course, an XKCD for that to which we make obligatory reference: <a href="https://xkcd.com/927/">https://xkcd.com/927/</a> <a href="#fnref:xkcd" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:poetryexample" role="doc-endnote">
      <p>For one example of many, see <a href="https://github.com/python-poetry/poetry/issues/3855">Issue #3855</a>, where several users try to make sense of the way poetry resolves packages from multiple sources — a conversation that has been happening for more than a year at the time of writing across <a href="https://github.com/python-poetry/poetry/discussions/4137#discussioncomment-2320644">multiple related issues</a>. <a href="#fnref:poetryexample" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:uxloop" role="doc-endnote">
      <p>Incentivized to develop new packages -&gt; need to reinvent interfaces -&gt; hard to develop and extend -&gt; incentivized to develop new packages <a href="#fnref:uxloop" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:googleantitrust" role="doc-endnote">
      <p>eg. see the complaint in State of Texas et al. v. Google that alleges Google rigs ad markets designed to lessen its dominance and uses its control over Chrome and Android to create a single, always-on tracking ecosystem owned only by them <a class="citation" href="#ReGoogleDigital2021">[318]</a> <a href="#fnref:googleantitrust" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:STRIDESsuccess" role="doc-endnote">
      <p>Their success stories tell the story of platform non-integration where scientists have to handbuild new tools to manage their data across multiple cloud environments: “We have been storing data in both cloud environments because we wanted the ecosystem we are creating to work on both clouds” <a class="citation" href="#STRIDESInitiativeSuccess2020">[53]</a> <a href="#fnref:STRIDESsuccess" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:awsdown" role="doc-endnote">
      <p>Though the system of engineered helpless that convinces us that we’re incapable of managing our own web infrastructure is not actually as reliable and seamless as it claims, as the long history of dramatic outages at AWS can show us <a class="citation" href="#lawlerAmazonServerOutage2021">[319, 320]</a> <a href="#fnref:awsdown" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:actuallywork" role="doc-endnote">
      <p>aka doing hard development work in sometimes adverse conditions. <a href="#fnref:actuallywork" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tymae" role="doc-endnote">
      <p>Thanks a lot to the one-and-only stunning and brilliant Dr. Eartha Mae Guthman for suggesting looking at the BRAIN initiative grants as a way of getting insight on core facilities. <a href="#fnref:tymae" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:pnidatascience" role="doc-endnote">
      <blockquote>
        <p>Project Summary: Core 2, Data Science […] In addition, the Core will build a data science platform that stores behavior, neural activity, and neural connectivity in a relational database that is queried by the DataJoint language. […] This data-science platform will facilitate collaborative analysis of datasets by multiple researchers within the project, and make the analyses reproducible and extensible by other researchers. […] https://projectreporter.nih.gov/project_info_description.cfm?aid=9444126&amp;icde=0</p>
      </blockquote>
      <p><a href="#fnref:pnidatascience" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:pnicaveat" role="doc-endnote">
      <p>Though again, this project is examplary, built by friends, and would be an excellent place to start extending towards global infrastructure. <a href="#fnref:pnicaveat" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dataportal" role="doc-endnote">
      <p>Seriously, don’t miss their extremely impressive <a href="https://data.internationalbrainlab.org/">data portal</a>. <a href="#fnref:dataportal" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:permauri" role="doc-endnote">
      <p>As one example, while I will write about <a href="https://www.w3.org/DesignIssues/LinkedData.html">linked data</a>, I don’t necessarily mean it in precisely the original instantiation as an irrevocable URI/RDF/SPARQL-only web, but do draw on its triplet link structure. <a href="#fnref:permauri" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:stm" role="doc-endnote">
      <p>The global trade association of publishers that serves as its lobbying and propaganda arm. <a href="#fnref:stm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hastydoi" role="doc-endnote">
      <p>The description provided by the “official” CrossRef 10 year retrospective paints a picture of panicked executives making an announcement for something they weren’t quite sure what it would be, but it would be <em>something</em> to compete with pubmed:</p>

      <blockquote>
        <p>We decided to issue an announcement of a broad STM reference linking initiative. It was, of course, a strategic move only, since we had neither plan nor prototype.”</p>

        <p>A small group led by Arnoud de Kemp of Springer-Verlag met in an adjacent room immediately following the Board meeting to draft the announcement, which was distributed to all attendees of the STM annual meeting the following day and published in an STM membership publication.</p>

        <p>Campbell recalled running into Bolman and Swanson (neither of whom was then on the STM Board) in the hotel lobby immediately after the drafting of the announcement. Their astonishment at hearing what had just transpired was matched by Campbell’s own on learning what they had been working on. […]</p>

        <p>Bolman and Swanson chose to seize the moment, and called an ad hoc meeting the following evening, Tuesday, October 12, to announce their venture and assemble a coalition of publishers to launch it. […]</p>

        <p>The potential benefit of the service that would become CrossRef was immediately apparent. Organizations such as AIP and IOP (Institute of Physics) had begun to link to each other’s publications, and the impossibility of replicating such one-off arrangements across the industry was obvious. As Tim Ingoldsby later put it, “All those linking agreements were going to kill us.” <a class="citation" href="#crossrefFormationCrossRefShort2009">[68]</a></p>
      </blockquote>
      <p><a href="#fnref:hastydoi" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:urisnew" role="doc-endnote">
      <p>It is hard to appreciate in retrospect how radical URLs/URIs were at the time — it might seem trivial to us now to be able to arbitrarily link to different locations on the internet, but before the internet linking was a carefully controlled process within publishing, looking more like ISBN and ISSNs than hyperlinks. <a href="#fnref:urisnew" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:isocostsmoney" role="doc-endnote">
      <p>Reading the standard costs 88 Swiss Francs. <a href="#fnref:isocostsmoney" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:selfdescribing" role="doc-endnote">
      <p>AKA you shouldn’t need to resort to some external source to understand it. Data should come packaged with clear metadata, software should have its own docs, etc. <a href="#fnref:selfdescribing" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:rfc1958" role="doc-endnote">
      <p>A “request for comment” from the Network Working Group of the Internet Engineering Task Force on the architecture of the internet. The IETF designs many of the protocols that serve as the backbone of the internet. <a href="#fnref:rfc1958" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:moneyofc" role="doc-endnote">
      <p>(in addition to a demonstration of the raw power of concentrated capital, of course) <a href="#fnref:moneyofc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:slackirc" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Internet_Relay_Chat">IRC</a>, internet relay chat, was a messaging system that served many of the same functions as the group messaging program <a href="https://slack.com/">Slack</a> serves now. Also see its more active cousin <a href="https://en.wikipedia.org/wiki/XMPP">XMPP</a> <a href="#fnref:slackirc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:cdns" role="doc-endnote">
      <p>This applies to centrally managed traditional servers as well as rented space on larger CDNs like AWS, but in the case of the CDN the constraint is from their pricing model. <a href="#fnref:cdns" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:osfspeed" role="doc-endnote">
      <p>As I am writing this, I am getting a (very unscientific sample of n=1) maximum speed of 5MB/s on the <a href="https://osf.io">Open Science Framework</a> <a href="#fnref:osfspeed" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:p2pdiscipline" role="doc-endnote">
      <p>Peer to peer systems are, maybe predictably, a whole academic subdiscipline. See <a class="citation" href="#shenHandbookPeertoPeerNetworking2010">[86]</a> for reference. <a href="#fnref:p2pdiscipline" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:knockin" role="doc-endnote">
      <p>knock on wood <a href="#fnref:knockin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tm" role="doc-endnote">
      <p>™️ <a href="#fnref:tm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bittorrentv2" role="doc-endnote">
      <p>Though the Bittorrent V2 protocol specification <a class="citation" href="#cohenBEP52BitTorrent2017">[105]</a> adopts a Merkle tree data structure which could theoretically support versioned torrents, v2 torrents are still not widely supported. <a href="#fnref:bittorrentv2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whatsgit" role="doc-endnote">
      <p>Git, briefly, is a version control system that keeps a history of changes of files (blobs) as a Merkle DAG: files can be updated, and different versions can be branched and reconciled. <a href="#fnref:whatsgit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whatdiss" role="doc-endnote">
      <p>for a detailed description of the site and community, see Ian Dunham’s dissertation <a class="citation" href="#dunhamWhatCDLegacy2018">[113]</a> <a href="#fnref:whatdiss" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dbsize" role="doc-endnote">
      <p>Though spotify now boasts its library having 50 million tracks, back of the envelope calculations relating number of releases to number of tracks are fraught, given the long tail of track numbers on albums like classical music anthologies with several hundred tracks on a single “release.” <a href="#fnref:dbsize" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:subtlety" role="doc-endnote">
      <p>Though music metadata might seem like a trivial problem (just look at the fields in an MP3 header), the number of edge cases are profound. How would you categorize an early Madlib casette mixtape remastered and uploaded to his website where he is mumbling to himself while recording some live show performed by multiple artists, but on the b-side is one of his Beat Konducta collections that mix together studio recordings from a collection of other artists? Who is the artist? How would you even identify the unnamed artists in the live show? Is that a compilation or a bootleg? Is it a cassette rip, a remaster, or a web release? <a href="#fnref:subtlety" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:trackeranalogy" role="doc-endnote">
      <p>To continue the analogy to bittorrent trackers, an example domain-specific vs. domain-general dichotomy might be What.cd (with its specific formatting and aggregation tools for representing artists, albums, collections, genres, and so on) vs. ThePirateBay (with its general categories of content and otherwise search-based aggregation interface) <a href="#fnref:trackeranalogy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:yrcool" role="doc-endnote">
      <p>No shade to Figshare, which, among others, paved the way for open data and are a massively useful thing to have in society. <a href="#fnref:yrcool" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:impossibledata" role="doc-endnote">
      <blockquote>
        <p>First, we assert that a single monolithic data set that directly connects the complete set of clinical characteristics to the complete set of biomolecular features, including “-omics” data, will never exist because the number of characteristics and features is constantly shifting and exponentially growing. Second, even if such a single monolithic data set existed, all-vs.-all associations will inevitably succumb to problems with statistical power (i.e., the curse of dimensionality).9 Such problems will get worse, not better, as more and more clinical and biomolecular data are collected and become available. We also assert that there is no single language, software or natural, with which to express clinical and biomolecular observations—these observations are necessarily and appropriately linked to the measurement technologies that produce them, as well as the nuances of language. The lack of a universal language for expressing clinical and biomolecular observations presents a risk of isolation or marginalization of data that are relevant for answering a particular inquiry, but are never accessed because of a failure in translation.</p>

        <p>Based on these observations, our final assertion is that automating the ability to reason across integrated data sources and providing users who pose inquiries with a dossier of translated answers coupled with full provenance and confidence in the results is critical if we wish to accelerate clinical and translational insights, drive new discoveries, facilitate serendipity, improve clinical-trial design, and ultimately improve clinical care. This final assertion represents the driving motivation for the Translator system. <a class="citation" href="#consortiumUniversalBiomedicalData2019">[131]</a></p>
      </blockquote>
      <p><a href="#fnref:impossibledata" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dilisearch" role="doc-endnote">
      <p>Using the only API listed with a “related to” link between disease and phenotypic feature, <a href="https://smart-api.info/ui/1d288b3a3caf75d541ffaae3aab386c8">SEMMEDDB</a>, I was unable to find “Red blood cell count” with DILI (C0860207) as either the <a href="https://biothings.ncats.io/semmeddb/query?q=subject.umls%3AC0860207&amp;facet_size=10&amp;fetch_all=true&amp;_sorted=true&amp;format=json">subject</a> or <a href="https://biothings.ncats.io/semmeddb/query?q=object.umls%3AC0860207&amp;facet_size=10&amp;fetch_all=true&amp;_sorted=true&amp;format=json">object</a>, and it is unclear why one would prefer that to any number of other phenotypes like “Fever” or the ominious symptom named “Symptoms” (C1457887). <a href="#fnref:dilisearch" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:unironically" role="doc-endnote">
      <p>seemingly unironically <a href="#fnref:unironically" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fine" role="doc-endnote">
      <p>which seems totally fine and normal. <a href="#fnref:fine" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ipredit" role="doc-endnote">
      <p>I submitted a <a href="https://github.com/jannahastings/mental-functioning-ontology/pull/8">pull request</a> to remove it, but it has not been merged more than 8 months later. A teardrop in the ocean. <a href="#fnref:ipredit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:personalmedicaldata" role="doc-endnote">
      <p>A 2020 presentation in one of the Translator’s <a href="https://github.com/NCATSTranslator/Translator-All">github repositories</a> describes methods for mining individual clinical data <a class="citation" href="#translatorconsortiumClinicalDataServices2020">[156]</a> <a href="#fnref:personalmedicaldata" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:googlepatent" role="doc-endnote">
      <p>A patent from Google is telling about how they view privacy concerns: whatever we can’t get explicitly, we’ll infer to sell better ads!</p>
      <blockquote>
        <p>One possible method to improve ad targeting is for ad targeting systems to obtain and use user profiles. For example, user profiles may be determined using information voluntarily given by users (e.g., when they subscribe to a service). This user attribute information may then be matched against advertiser specified attributes of the ad (e.g., targeting criteria). Unfortunately, user profile information is not always available since many Websites (e.g., search engines) do not require subscription or user registration. Moreover, even when available, the user profile may be incomplete (e.g., because the information given at the time of subscription may be limited to what is needed for the service and hence not comprehensive, because of privacy considerations, etc.). Furthermore, advertisers may need to manually define user profile targeting information. In addition, even if user profile information is available, advertisers may not be able to use this information to target ads effectively. <a class="citation" href="#bharatGeneratingUserInformation2005">[152]</a></p>
      </blockquote>
      <p><a href="#fnref:googlepatent" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:intraconsortium" role="doc-endnote">
      <p>The descriptions of difficulty in interfacing the components of the project internally are littered throughout their public-facing documents: eg. “A lot of the work has been about defining standards, so that the components that each of the 15 teams are building can talk to each other” <a class="citation" href="#renaissancecomputinginstituterenciBiomedicalDataTranslator2022">[135]</a>, “In part due to the speed with which the program has progressed, team members also have found it challenging to coordinate milestones and deliverables across teams and align the goals of the Translator program with the goals of their own nonTranslator research projects.” <a class="citation" href="#consortiumBiomedicalDataTranslator2019">[163]</a>. These problems are, of course, completely reasonable. My comment here merely suggests that solving these problems, particularly on the self-described tight timeline of the Translator’s development, may have edged out concerns for engagemenent with the broader research community. <a href="#fnref:intraconsortium" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:federatedterminology" role="doc-endnote">
      <p>There is a lot of subtlety to the terminology surrounding “federated” and the typology of distributed systems generally, I am using it in the federated messaging sense of forming groups of people, rather than the strict term “federated databases” which do imply a standardized schema across a federation. The conception of distributed, autonomous databases described by the DataLad team <a class="citation" href="#hankeDefenseDecentralizedResearch2021">[193]</a> is a bit closer to my meaning. In the ActivityPub world, federations refer to a single homeserver under which many people can sign up. We mean something similar but distinct: people that have autonomous “homeservers” in a peer to peer system, typically multiple identities for a single person rather than many people on a single server, that can combine into federations with particular governance structures and technological systems attached. <a href="#fnref:federatedterminology" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dlg" role="doc-endnote">
      <p>Or, precisely, a “directed labeled graph” (DLG). <a href="#fnref:dlg" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notreallynwb" role="doc-endnote">
      <p>not really where it would be in the standard, but again, for the sake of example… <a href="#fnref:notreallynwb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:p2pdatalad" role="doc-endnote">
      <p><a href="https://www.datalad.org/">DataLad</a> <a class="citation" href="#halchenkoDataLadDistributedSystem2021">[321, 193]</a> and its application in Neuroscience as <a href="https://dandiarchive.org">DANDI</a> are two projects that are <em>very close</em> to what I have been describing here — developing a p2p backend for datalad might even be a promising development path towards it. <a href="#fnref:p2pdatalad" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:nwbisanexample" role="doc-endnote">
      <p>Recall that we’re using NWB for the sake of concreteness, but this argument applies to any standardized data format. <a href="#fnref:nwbisanexample" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:autopilotv050" role="doc-endnote">
      <p>Though this is a description of something we could build towards, v0.5.0 (at the time of writing released as alpha) of Autopilot has a <a href="https://docs.auto-pi-lot.com/en/latest/changelog/v0.5.0.html">data modeling</a> framework that should make this possible in future versions. <a href="#fnref:autopilotv050" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:futureversions" role="doc-endnote">
      <p>Not yet, but this is planned development for future versions. <a href="#fnref:futureversions" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:marderjoyof" role="doc-endnote">
      <p>Also see Eve Marder’s recent short and characteristically refreshing piece which in part discusses the problem of keeping up with scientific literature the context of maintaining the joy of discovery <a class="citation" href="#marderMaintainingJoyDiscovery2022">[223]</a>. <a href="#fnref:marderjoyof" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whatdivision" role="doc-endnote">
      <p>RELX is a huge information conglomerate, and scientific publication is just one division. <a href="#fnref:whatdivision" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notfree" role="doc-endnote">
      <p>“free” <a href="#fnref:notfree" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:nativeads" role="doc-endnote">
      <p>a strategy that the reprehensible digital marketing disciplines call “native advertising” <a class="citation" href="#dekeyzerProcessingNativeAdvertising2021">[228, 229]</a> <a href="#fnref:nativeads" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:rand" role="doc-endnote">
      <p>the result of another corporate collaboration with the Rand corporation. <a href="#fnref:rand" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:quittingreviewing" role="doc-endnote">
      <p>For a recent example, see the responses to Dan Goodman’s argument why he has stopped doing pre-publication peer review altogether (missing reference) <a href="#fnref:quittingreviewing" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:wikiwording" role="doc-endnote">
      <p>Interestingly, this quote is almost, but not exactly the same as that on <a href="http://wiki.c2.com/?WhyWikiWorks">Ward’s wiki</a>:</p>
      <blockquote>
        <p>So that’s it - insecure, indiscriminate, user-hostile, slow, full of difficult, nit-picking people, and frivolous. Any other online community would count each of these strengths as a terrible flaw. Perhaps wiki works because the other online communities do not.</p>
      </blockquote>

      <p>I can’t tell if Ward Cunningham wrote the original entry in the wiki, but in any case seems to have found a bit of optimism in the book. <a href="#fnref:wikiwording" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:corporatewikis" role="doc-endnote">
      <p>though their corporate manifestations would probably be unrecognizable to the project early wiki users imagined. <a href="#fnref:corporatewikis" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:wikistart" role="doc-endnote">
      <p>it’s complicated: http://wiki.c2.com/?WardsWikiTenthAnniversary <a href="#fnref:wikistart" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:npages" role="doc-endnote">
      <p><a href="http://c2.com/wiki/history/">23,244</a> unique page names according to the edit history, but the edit history was also purposely pruned from time to time. <a href="#fnref:npages" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:wardcause" role="doc-endnote">
      <p>Giving a means of organizing the writing of the Portland Pattern Repository was the reason for creating Ward’s Wiki in the first place. <a href="#fnref:wardcause" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:contemporarywikithought" role="doc-endnote">
      <p>Contemporary wikis have continued this conversation, see <a href="https://communitywiki.org/wiki/DocumentsVsMessages">DocumentsVsMessages</a> on communitywiki.org <a href="#fnref:contemporarywikithought" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:wardinvite" role="doc-endnote">
      <p>The initial motivations are actually stunningly close to the kinds of communication and knowledge organization problems we are still solving today (even in this piece)</p>
      <blockquote>
        <p>Cunningham had developed a database to collect the contributions of the listserv members. He had noticed that the content of the listserv tended to get buried, and therefore the most recent post might be under-informed about posts which came before it. The way around this problem was to collect ideas in a database, and then edit those ideas rather than begin anew with each listserv posting. Cunningham’s post states that “The plan is to have interested parties write web pages about the People, Projects and Patterns that have changed the way they program. Short stories that hint at patterns are welcome too.” As to the rhetorical expectations, Cunningham added “The writing style is casual, like email or netnews, but doesn’t have to be so repetitive since the things being discussed don’t disappear. Think of it as a moderated list where anyone can be moderator and everything is archived. It’s not quite a chat, still, conversation is possible.” - <a class="citation" href="#cummingsWhatWasWikiWhy2009">[316]</a></p>
      </blockquote>
      <p><a href="#fnref:wardinvite" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:recentchanges" role="doc-endnote">
      <p>Recent Changes was the dominant, if not controversial means of keeping track with recent wiki traffic, see <a href="http://wiki.c2.com/?RecentChangesJunkie">RecentChangesJunkie</a> <a href="#fnref:recentchanges" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:meatballthought" role="doc-endnote">
      <p>There seems to have been an overriding belief that theoretical ideas about wikis and wiki culture belong on Meatball Wiki, from <a href="http://wiki.c2.com/?WikiWikiWebFaq">WikiWikiWebFaq</a>:</p>
      <blockquote>
        <p>Q: Do two separate wikis ever merge together to create one new wiki? Has this happened before? Keep in mind that I don’t just mean two different pages within a wiki. (And for that matter, where is an appropriate page where I can post questions about the history of all wikis, not just this one?)</p>

        <p>A1: I don’t know of any such wiki merge, nor of any discussion of the history of all wikis. Such a discussion should probably reside (if created) on MeatballWiki.</p>
      </blockquote>
      <p><a href="#fnref:meatballthought" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:jimmydisagrees" role="doc-endnote">
      <p>Jimmy Wales, naturally, disputes this characterization of events. <a href="#fnref:jimmydisagrees" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AWWDMBJAWGCAWAIFDSPBATDMTAD" role="doc-endnote">
      <p>Also see <a href="https://meta.wikimedia.org/wiki/Association_of_Wikipedians_Who_Dislike_Making_Broad_Judgments_About_the_Worthiness_of_a_General_Category_of_Article,_and_Who_Are_in_Favor_of_the_Deletion_of_Some_Particularly_Bad_Articles,_but_That_Doesn%27t_Mean_They_Are_Deletionists">Association of Wikipedians Who Dislike Making Broad Judgments About the Worthiness of a General Category of Article, and Who Are in Favor of the Deletion of Some Particularly Bad Articles, but That Doesn’t Mean They Are Deletionists</a> <a href="#fnref:AWWDMBJAWGCAWAIFDSPBATDMTAD" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:timbldifferentiation" role="doc-endnote">
      <p>Tim Berners-Lee described this notion of functional differentiation in a much more general way in describing the nature of the URI:</p>
      <blockquote>
        <p>The technology should define mechanisms wherever possible without defining policy.</p>

        <p>because we recognize here that many properties of URIs are social rather than technical in origin.</p>

        <p>Therefore, you will find pointers in hypertext which point to documents which never change but you will also find pointers to documents which change with time. You will find pointers to documents which are available in more than one format. You will find pointers to documents which look different depending on who is asking for them. There are ways to describe in a machine or human readable way exactly what sort of repeatability you would expect from a URI, but the architecture of the Web is that that is for something for the owner of the URI to determine. https://www.w3.org/DesignIssues/Axioms.html</p>
      </blockquote>
      <p><a href="#fnref:timbldifferentiation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notebookcites" role="doc-endnote">
      <p>The original Jupyter Notebook paper describes the need for this near the end <a class="citation" href="#kluyverJupyterNotebooksPublishing2016">[264]</a>. <a href="#fnref:notebookcites" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:genius" role="doc-endnote">
      <p>cf. the <a href="https://genius.com">genius.com</a> overlay. <a href="#fnref:genius" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:markdowntoo" role="doc-endnote">
      <p>complete with markdown renderin! <a href="#fnref:markdowntoo" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:sparqldb" role="doc-endnote">
      <p>Tim Berners-Lee describes the distinction between traditional relationship databases and RDF databases:</p>
      <blockquote>
        <p>Relational database systems, manage RDF data, but in a specialized way. In a table, there are many records with the same set of properties. An individual cell (which corresponds to an RDF property) is not often thought of on its own. SQL queries can join tables and extract data from tables, and the result is generally a table. So, the practical use for which RDB software is used typically optimized for soing operations with a small number of tables some of which may have a large number of elements.</p>

        <p>RDB systems have datatypes at the atomic (unstructured) level, as RDF and XML will/do. Combination rules tend in RDBs to be loosely enforced, in that a query can join tables by any comlumns which match by datatype – without any check on the semantics. You could for example create a list of houses that have the same number as rooms as an employee’s shoe size, for every employee, even though the sense of that would be questionable.</p>

        <p>The Semantic Web is not designed just as a new data model - it is specifically appropriate to the linking of data of many different models. One of the great things it will allow is to add information relating different databases on the Web, to allow sophisticated operations to be performed across them. https://www.w3.org/DesignIssues/RDFnot.html</p>
      </blockquote>
      <p><a href="#fnref:sparqldb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:timblcar" role="doc-endnote">
      <p>“For example, one person may define a vehicle as having a number of wheels and a weight and a length, but not foresee a color. This will not stop another person making the assertion that a given car is red, using the color vocabular from elsewhere.” - https://www.w3.org/DesignIssues/RDB-RDF.html <a href="#fnref:timblcar" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:mutingamp2" role="doc-endnote">
      <p>for example, pin 7 mutes the board, but is still exposed in the 40-pin header. We powered an LED with pin 7 and were absolutely baffled why the sound would mute every time the light went on for a week or so. <a href="#fnref:mutingamp2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
</div>

    </main>

    <footer>
<script src="/infrastructure/assets/js/react.js"></script>
<script src="/infrastructure/assets/js/toc.js"></script>



<script defer src="https://hypothes.is/embed.js"></script>
<script src="/infrastructure/assets/js/hypothesis.js"></script>

</footer>

  </body>
</html>
