
---

I think it is important to pause and appreciate the potential for harm in the data infrastructural system describes so far, continuing to use structural transphobia as one example among many possible harms. First, a brief recap:

Through STRIDES, cloud providers like AWS, Google Cloud, and Microsoft Azure are intended to become the primary custodians of scientific data. Regardless of contracts and assurances, since their system is opaque and proprietary, there is no way to ensure that they will not crawl this data and use it to train their various algorithms-as-a-service --- and they seem all too happy to do so, as evidenced by GitHub Co-Pilot reproducing copyrighted code and code with licenses that explicitly forbade its use in that context. Given that Amazon is expanding aggressively into health technology{% cite AWSAnnouncesAWS2021 %}, including wearables and literally providing [health care](https://amazon.care/) {% cite lermanAmazonBuiltIts2021 %}, primary scientific data is a valuable prize in their mission to cement dominance in algorithmic health. 

The effort to unify data across the landscape of databases, patient data, and so on is built atop a rickety pile of SaaS so fragile that a *single person* with a *single repository* can have ripple effects across the aggregators that impact the whole knowledge graph. In the above example, an outdated set of terminology classifies a subset of human gender as a disease, which then is linked to candidate genes and other nodes in the knowledge graph. Since there is a preponderance of misguided research about about the etiology and "biological mechanisms" of transgender people, the graph neighboorhood around transness is rich with biomarkers and functional data. 

All of the above is known to be true now, but let's see how it could play out practically in an all-too-plausible thought experiment.

Though the translator system now is intended for basic research and drug discovery, there is stated desire for it to eventually become a consumer/clinical product {% cite hailuNIHfundedProjectAims2019 %}. Say a cloud provider rolls out a service for clinical recommendations for doctors informed by the full range of scientific, clinical, wearable, and other personal data they have available --- a trivial extension of [existing](https://web.archive.org/web/20211003070018/https://support.apple.com/en-us/HT208680) patient medical aggregation and [recommendation](https://web.archive.org/web/20210408221213/https://support.google.com/fit/answer/7619539?hl=en&co=GENIE.Platform%3DAndroid) services that [express](https://web.archive.org/web/20210930203834/https://press.aboutamazon.com/news-releases/news-release-details/amazon-adds-more-halo-introducing-halo-view-halo-fitness-and) their biopolitical control as a slick wristband with app. It's very "smart" and is very "private" in the sense that only the algorithm ever sees your personal data. 

Since these cloud providers as a rule depend on developing elaborate personal profiles for targeted advertising algorithmically inferred from available data[^googlepatent], that naturally includes diagnosed or inferred disease --- a practice they explicitly describe in the patents for the targeting technology{% cite bharatGeneratingUserInformation2005 %}, gone to court to defend {% cite SmithFacebookInc2018 krashinskyGoogleBrokeCanada2014 %}, formed secretive joint projects with healthcare systems to pursue {% cite bourreauGoogleFitbitWill2020 %}, and so on. Nothing too diabolical here, just a system wherein **your search results and online shopping habits influence your health care in unpredictable and frequently inaccurate {%cite rasmyMedBERTPretrainedContextualized2021 %} ways.**

Imagine, through some pattern in your personal data, **Amazon diagnoses you as trans.**Whether their assessment is true or not is unimportant. Since the Translator works as a graph-based knowledge engine, your algorithmic transness, with its links through related genes, "symptoms," and whatever other uninspectable network links the knowledge graph has, influences the medical care you receive. All part of the constellation of personalized information that constitutes "personalized medicine." 

The Translator assures us that it will give doctors understandable provenance by being able to explain how it arrived at its recommendation. Let's assume from prior experience with neural net language models that part of the process doesn't work very well, or at least doesn't give a fully exhaustive description of every single relevant graph entity. Now let's further assume based on the above DILI example that the knowledge graph is not able to reliably "understand" the complex cultural-technological context of transness, and since it is classified as a "disease" decides that you need to be "cured." Since it has access to a diverse array of biomedical data, it might even be able to concoct a very effective conversion therapy regimen *personalized just for you.* The algorithm could prescribe your conversion therapy *without you or the doctor knowing it.*

Transphobic behavior that impacts treatment is common {% cite ramTransphobiaEncodedExamination2021 strangioCanReproductiveTrans2016 %}. Since the Translator's algorithm is designed to learn from feedback and use{% cite consortiumUniversalBiomedicalData2019 %}, transphobic practices could easily reinforce and magnify the algorithm's initial guess about what transness being a disease should mean for trans people in practice. Combined with the limitations on provision of care from insurance systems {% cite strangioCanReproductiveTrans2016 %}, on a wide scale transphobic medical practices could be transmuted into a "scientifically justified" standard of care.

Scaling out further, the original intention of the tool is to guide drug discovery and pharmaceutical research, so harm could be encoded into the indefinite future of biomedical research --- imperceptibly guiding the array of candidate drugs to test based on an algorithmically biased perception of biology and medical prerogative. Even in the case that society changes and we attempt to make amends in our institution for outdated and harmful notions, the long tail of ingrained learning in a proprietary algorithm could be hard to unlearn if the proprieter is inclined to try at all. So even many years into the future when we "know better," the ghosts of algorithmically guided medical reserach and practice could still unknowingly guide our hands.

The pathologizing of transgender people is just one example among many demonstrated instances of algorithmic bias like race, disability, and effectively any other marginalized group. The critical issue is that **we might not have any idea** how the algorithm is influencing research and practice at scales large and small, immediate and indefinite. The impacts don't have to be as dramatic as this particular thought experiment to be harmful. The subtlety of having dosages, prescriptions, and candidate drugs jittered by a massive integrated machine learning system is harm in itself: our medical care becomes training data. The point is that we *can't know* the effects of letting the course of our medical research and clinical care be steered by an algorithm embedded within a platform that has *any* incentive that conflicts with our collective health.

--- 

How did we get here? How could an effort to link biomedical data become an instrument of mass surveillance and harm? 